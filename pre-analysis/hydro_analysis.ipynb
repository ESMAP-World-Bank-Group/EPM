{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35df81a60a3c6ca1",
   "metadata": {},
   "source": [
    "# GRDC Hydro data processing\n",
    "\n",
    "Processing streamflow data from the Global Runoff Data Centre (GRDC) to create a comprehensive dataset of river discharge and runoff.\n",
    "\n",
    "Rely on three kind of input data that all exists in open-source and needs to be downloaded separately:\n",
    "1. Monthly streamflow data from GRDC stations\n",
    "2. Hydropower plant data from the Global Hydropower Tracker\n",
    "3. River data from HydroRIVERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43cbf66c979286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:08:27.488876Z",
     "start_time": "2025-06-23T16:08:27.480342Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import folium\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from branca.colormap import LinearColormap\n",
    "from folium import CircleMarker\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import calendar\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "import unicodedata\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396e191b773eaf8",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97ba7cbd5074f56e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:49.075705Z",
     "start_time": "2025-06-23T15:08:49.072052Z"
    }
   },
   "outputs": [],
   "source": [
    "countries = ['Angola', 'Burundi', 'Cameroon', 'Central African Republic', 'Chad', 'Republic of the Congo', 'DR Congo', 'Equatorial Guinea', 'Gabon']\n",
    "\n",
    "# Define the base input directory\n",
    "base_dir = 'data_grdc_hydro_capp/input'\n",
    "folder_out = 'data_grdc_hydro_capp/output'\n",
    "if not os.path.exists(folder_out): os.makedirs(folder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7361de753733d",
   "metadata": {},
   "source": [
    "## 1. Load streamflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9824c38b4d3ac539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:49.293511Z",
     "start_time": "2025-06-23T15:08:49.113337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: data_grdc_hydro_capp/input/sao_tome/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/gabon/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/democroatic_republic_congo/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/central_africa/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/angola/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/chad/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/republic_congo/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/cameroon/GRDC-Monthly.nc\n",
      "Loaded: data_grdc_hydro_capp/input/burundi/GRDC-Monthly.nc\n",
      "Total merged dimensions: FrozenMappingWarningOnValuesAccess({'time': 1368, 'id': 206})\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold all xarray Datasets\n",
    "datasets = []\n",
    "\n",
    "# Walk through all directories under 'input'\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    if 'GRDC-Monthly.nc' in files:\n",
    "        file_path = os.path.join(root, 'GRDC-Monthly.nc')\n",
    "        try:\n",
    "            data_discharge = xr.open_dataset(file_path)\n",
    "            datasets.append(data_discharge)\n",
    "            print(f\"Loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path}: {e}\")\n",
    "\n",
    "# Merge all datasets\n",
    "if datasets:\n",
    "    try:\n",
    "        data_discharge = xr.concat(datasets, dim='id')  # You can use another dim if needed\n",
    "        print(f\"Total merged dimensions: {data_discharge.dims}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge datasets: {e}\")\n",
    "else:\n",
    "    print(\"No streamflow data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65647f4297482749",
   "metadata": {},
   "source": [
    "## 2. Filter  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3fc1228995adf798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:49.346605Z",
     "start_time": "2025-06-23T15:08:49.330223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations removed due to invalid area: 2\n",
      "Stations removed due to low runoff: 40\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Filter out stations with no valid area\n",
    "# -------------------------------\n",
    "\n",
    "# Identify stations with invalid or missing area\n",
    "invalid_area_mask = (data_discharge[\"area\"] <= 0) | data_discharge[\"area\"].isnull()\n",
    "removed_area_ids = data_discharge[\"id\"].values[invalid_area_mask.values]\n",
    "\n",
    "# Extract metadata of removed stations\n",
    "removed_area_meta = data_discharge.sel(id=removed_area_ids)[[\"station_name\", \"geo_x\", \"geo_y\", \"area\"]]\n",
    "stations_removed_area_df = removed_area_meta.to_dataframe().reset_index()\n",
    "\n",
    "# Remove stations with invalid area\n",
    "data_discharge = data_discharge.where(data_discharge[\"area\"] > 0, drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Filter out stations with low runoff values\n",
    "# -------------------------------\n",
    "\n",
    "threshold_runoff = 15  # mÂ³/s\n",
    "\n",
    "# Identify stations where all runoff values are below threshold or NaN\n",
    "runoff = data_discharge[\"runoff_mean\"]\n",
    "low_flow_mask = (runoff < threshold_runoff) | runoff.isnull()\n",
    "stations_to_remove = low_flow_mask.all(dim=\"time\")\n",
    "removed_ids = data_discharge[\"id\"].values[stations_to_remove.values]\n",
    "\n",
    "# Extract metadata of removed stations\n",
    "removed_meta = data_discharge.sel(id=removed_ids)[[\"station_name\", \"geo_x\", \"geo_y\"]]\n",
    "stations_removed_lowflow_df = removed_meta.to_dataframe().reset_index()\n",
    "\n",
    "# Remove those stations from the dataset\n",
    "data_discharge = data_discharge.sel(id=~stations_to_remove)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Summary (optional display or export)\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Stations removed due to invalid area: {len(stations_removed_area_df)}\")\n",
    "print(f\"Stations removed due to low runoff: {len(stations_removed_lowflow_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d83376f05f63e5",
   "metadata": {},
   "source": [
    "## 3. Format data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4284d220a3fa2ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:49.388698Z",
     "start_time": "2025-06-23T15:08:49.385348Z"
    }
   },
   "outputs": [],
   "source": [
    "def checking_duplicates_grdc(df):\n",
    "    \"\"\"\n",
    "    Check for duplicated (year, station_name, month) entries in the DataFrame.\n",
    "\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dupes = (\n",
    "        df.groupby([\"year\", \"station_name\", \"month\"])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .query(\"count > 1\")\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(dupes)} duplicated (year, station_name, month) entries\")\n",
    "\n",
    "    problem_stations = dupes['station_name'].unique()\n",
    "    print(f'Duplicated tation: {problem_stations}')\n",
    "\n",
    "    # Merge the duplicate keys back into df to see full rows\n",
    "    duplicated_rows = df.merge(dupes[[\"year\", \"station_name\", \"month\"]], on=[\"year\", \"station_name\", \"month\"])\n",
    "    #display(duplicated_rows.sort_values([\"station_name\", \"year\", \"month\"]))\n",
    "\n",
    "    return problem_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f64c637dae24cb0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:50.981332Z",
     "start_time": "2025-06-23T15:08:49.466577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2736 duplicated (year, station_name, month) entries\n",
      "Duplicated tation: ['KRIBI' 'MUYANGE']\n"
     ]
    }
   ],
   "source": [
    "var_name = 'runoff_mean'\n",
    "\n",
    "meta = data_discharge[[\"station_name\", \"geo_x\", \"geo_y\"]].to_dataframe().reset_index()\n",
    "area = data_discharge[\"area\"].to_dataframe().reset_index()\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_station = data_discharge[var_name].to_dataframe(name=\"Q\").reset_index()\n",
    "\n",
    "# Merge metadata into the main DataFrame\n",
    "data_station = data_station.merge(meta, on=\"id\")\n",
    "data_station = data_station.merge(area, on=\"id\")\n",
    "\n",
    "# Add year and month columns\n",
    "data_station[\"year\"] = data_station[\"time\"].dt.year\n",
    "data_station[\"month\"] = data_station[\"time\"].dt.month\n",
    "\n",
    "# Check if station_name has duplicates\n",
    "if data_station.duplicated(subset=[\"station_name\"]).any():\n",
    "    problem_stations = checking_duplicates_grdc(data_station)\n",
    "\n",
    "    # Step 3: Get corresponding station IDs\n",
    "    problem_ids = data_station[data_station[\"station_name\"].isin(problem_stations)][\"id\"].unique()\n",
    "\n",
    "    # Step 4: Assign unique station names for problematic IDs only\n",
    "    rename_map = {\n",
    "        id_: f\"{data_station[data_station['id'] == id_]['station_name'].iloc[0]}_{i+1}\"\n",
    "        for i, id_ in enumerate(problem_ids)\n",
    "    }\n",
    "\n",
    "    # Step 5: Apply renaming based on `id` consistently\n",
    "    data_station[\"station_name\"] = data_station.apply(\n",
    "        lambda row: rename_map[row[\"id\"]] if row[\"id\"] in rename_map else row[\"station_name\"],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Create a unique label for each station using name + coordinates (optional)\n",
    "data_station[\"station_label\"] = data_station[\"station_name\"].str.strip() + \" (\" + data_station[\"geo_y\"].round(2).astype(str) + \", \" + data_station[\"geo_x\"].round(2).astype(str) + \")\"\n",
    "\n",
    "# Pivot to wide format: year as index, MultiIndex (month, id) as columns\n",
    "data_station_pivot = data_station.pivot(index=\"year\", columns=[\"station_name\", \"month\"], values=\"Q\")\n",
    "data_station_pivot.dropna(axis=1, how='all', inplace=True)\n",
    "data_station_pivot.sort_index(ascending=True, axis=1, inplace=True)\n",
    "data_station_pivot.to_csv(os.path.join(folder_out, f'grdc_discharge_monthly-m3-s.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1c17ea795f1b1",
   "metadata": {},
   "source": [
    "## 4. Calculate runoff data\n",
    "\n",
    "Runoff data (mm/year) is calculated from monthly discharge data (mÂ³/s) using the formula:\n",
    "\n",
    "    runoff_mm = (Î£(Q_monthly_avg Ã 86400 Ã days_in_month)) / area_m2 Ã 1000\n",
    "\n",
    "Where:\n",
    "- Q is discharge in mÂ³/s\n",
    "- 86400 is seconds per day\n",
    "- days_in_month accounts for monthly totals\n",
    "- area is the catchment area in mÂ²\n",
    "- 1000 converts meters to millimeters\n",
    "\n",
    "It is more standard way to compare runoff across different catchments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "df03fffed797054a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:50.988728Z",
     "start_time": "2025-06-23T15:08:50.984914Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_full_years(df):\n",
    "    \"\"\"\n",
    "    Keep only (station, year) pairs with 12 valid months.\n",
    "    Returns filtered DataFrame and number of dropped rows.\n",
    "    \"\"\"\n",
    "    original_len = len(df)\n",
    "\n",
    "    # Count valid months per station-year\n",
    "    valid_counts = (\n",
    "        df.groupby(['station_name', 'year'])['Q']\n",
    "        .apply(lambda x: x.notna().sum())\n",
    "        .reset_index(name='valid_months')\n",
    "    )\n",
    "\n",
    "    # Only keep those with all 12 months\n",
    "    full_years = valid_counts[valid_counts['valid_months'] == 12]\n",
    "\n",
    "    # Merge to filter original DataFrame\n",
    "    df_filtered = df.merge(full_years[['station_name', 'year']], on=['station_name', 'year'])\n",
    "\n",
    "    removed_rows = original_len - len(df_filtered)\n",
    "    print(f\"Removed {removed_rows} rows â kept {len(df_filtered)} only full (12-month) years.\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def discharge_to_runoff(df):\n",
    "    \"\"\"\n",
    "    Convert annual discharge (mÂ³/s) into annual runoff (mm/year) at the station level.\n",
    "\n",
    "    The equation used is:\n",
    "\n",
    "        runoff_mm = (Î£(Q_monthly_avg Ã 86400 Ã days_in_month)) / area_m2 Ã 1000\n",
    "\n",
    "    Where:\n",
    "        - Q is discharge in mÂ³/s\n",
    "        - 86400 is seconds per day\n",
    "        - days_in_month accounts for monthly totals\n",
    "        - area is the catchment area in mÂ²\n",
    "        - 1000 converts meters to millimeters\n",
    "\n",
    "    Assumes:\n",
    "        - Input DataFrame has one row per station/month\n",
    "        - Years are complete (12 months per station)\n",
    "\n",
    "    Returns:\n",
    "        - DataFrame with columns: station_label, year, runoff_mm_year\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Add number of days in each month\n",
    "    df['days_in_month'] = pd.to_datetime(df['time']).dt.days_in_month\n",
    "\n",
    "    # Convert area to mÂ²\n",
    "    df['area_m2'] = df['area'] * 1e6\n",
    "\n",
    "    # Compute volume in mÂ³ for each month\n",
    "    df['volume_m3'] = df['Q'] * 86400 * df['days_in_month']\n",
    "\n",
    "    # Sum monthly volumes per station-year\n",
    "    runoff_by_year = (\n",
    "        df.groupby(['station_name', 'year'])\n",
    "        .apply(lambda x: x['volume_m3'].sum() / x['area_m2'].iloc[0] * 1000, include_groups=False)  # m to mm\n",
    "        .reset_index(name='runoff_mm_year')\n",
    "    )\n",
    "\n",
    "    return runoff_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6b0b80b52d97ebea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:51.965297Z",
     "start_time": "2025-06-23T15:08:51.023924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 198480 rows â kept 25872 only full (12-month) years.\n"
     ]
    }
   ],
   "source": [
    "# Convert discharge data (m3-s) to runoff in (mm-year)\n",
    "data_station_filtered = filter_full_years(data_station)\n",
    "data_runoff = discharge_to_runoff(data_station_filtered)\n",
    "# Save to CSV\n",
    "data_runoff.round(0).pivot(index=\"year\", columns=\"station_name\", values=\"runoff_mm_year\").to_csv(os.path.join(folder_out, f'grdc_runoff_mm-year.csv'), index=True)\n",
    "\n",
    "location = data_station[['station_name', 'geo_x', 'geo_y']].drop_duplicates().reset_index(drop=True)\n",
    "# Merge with runoff_by_year data\n",
    "data_runoff = data_runoff.merge(location, on='station_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9f6106c05e9dd",
   "metadata": {},
   "source": [
    "## 5. Add hydropowerplant data & associate with river data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ade120360afe7c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:53.256414Z",
     "start_time": "2025-06-23T15:08:51.998668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Last Researched</th>\n",
       "      <th>Country/Area 1</th>\n",
       "      <th>Country/Area 2</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Project Name (local lang/script)</th>\n",
       "      <th>Other name(s)</th>\n",
       "      <th>Capacity (MW)</th>\n",
       "      <th>Binational</th>\n",
       "      <th>Country/Area 1 Capacity (MW)</th>\n",
       "      <th>Country/Area 2 Capacity (MW)</th>\n",
       "      <th>...</th>\n",
       "      <th>Region 1</th>\n",
       "      <th>City 2</th>\n",
       "      <th>Local Area 2</th>\n",
       "      <th>Major Area 2</th>\n",
       "      <th>State/Province 2</th>\n",
       "      <th>Subregion 2</th>\n",
       "      <th>Region 2</th>\n",
       "      <th>GEM location ID</th>\n",
       "      <th>GEM unit ID</th>\n",
       "      <th>Wiki URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caculo CabaÃ§a hydroelectric plant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2172</td>\n",
       "      <td>No</td>\n",
       "      <td>2172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L100000600012</td>\n",
       "      <td>G100000600012</td>\n",
       "      <td>https://www.gem.wiki/Caculo_CabaÃ§a_hydroelectr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cambambe I hydroelectric plant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180</td>\n",
       "      <td>No</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L100000600013</td>\n",
       "      <td>G100000600013</td>\n",
       "      <td>https://www.gem.wiki/Cambambe_I_hydroelectric_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cambambe II hydroelectric plant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>No</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L100000600014</td>\n",
       "      <td>G100000600014</td>\n",
       "      <td>https://www.gem.wiki/Cambambe_II_hydroelectric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Capanda hydroelectric plant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>520</td>\n",
       "      <td>No</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L100000600015</td>\n",
       "      <td>G100000600015</td>\n",
       "      <td>https://www.gem.wiki/Capanda_hydroelectric_plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024-11-25</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gove Dam hydroelectric plant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>No</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L100001025775</td>\n",
       "      <td>G100001030660</td>\n",
       "      <td>https://www.gem.wiki/Gove_Dam_hydroelectric_plant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date Last Researched Country/Area 1 Country/Area 2  \\\n",
       "21           2024-11-22         Angola            NaN   \n",
       "22           2022-09-12         Angola            NaN   \n",
       "23           2022-09-12         Angola            NaN   \n",
       "24           2022-09-12         Angola            NaN   \n",
       "25           2024-11-25         Angola            NaN   \n",
       "\n",
       "                         Project Name Project Name (local lang/script)  \\\n",
       "21  Caculo CabaÃ§a hydroelectric plant                              NaN   \n",
       "22     Cambambe I hydroelectric plant                              NaN   \n",
       "23    Cambambe II hydroelectric plant                              NaN   \n",
       "24        Capanda hydroelectric plant                              NaN   \n",
       "25       Gove Dam hydroelectric plant                              NaN   \n",
       "\n",
       "   Other name(s)  Capacity (MW) Binational  Country/Area 1 Capacity (MW)  \\\n",
       "21           NaN           2172         No                        2172.0   \n",
       "22           NaN            180         No                         180.0   \n",
       "23           NaN            700         No                         700.0   \n",
       "24           NaN            520         No                         520.0   \n",
       "25           NaN             60         No                          60.0   \n",
       "\n",
       "    Country/Area 2 Capacity (MW)  ... Region 1 City 2 Local Area 2  \\\n",
       "21                           0.0  ...   Africa    NaN          NaN   \n",
       "22                           0.0  ...   Africa    NaN          NaN   \n",
       "23                           0.0  ...   Africa    NaN          NaN   \n",
       "24                           0.0  ...   Africa    NaN          NaN   \n",
       "25                           0.0  ...   Africa    NaN          NaN   \n",
       "\n",
       "    Major Area 2  State/Province 2 Subregion 2 Region 2 GEM location ID  \\\n",
       "21           NaN               NaN         NaN      NaN   L100000600012   \n",
       "22           NaN               NaN         NaN      NaN   L100000600013   \n",
       "23           NaN               NaN         NaN      NaN   L100000600014   \n",
       "24           NaN               NaN         NaN      NaN   L100000600015   \n",
       "25           NaN               NaN         NaN      NaN   L100001025775   \n",
       "\n",
       "      GEM unit ID                                           Wiki URL  \n",
       "21  G100000600012  https://www.gem.wiki/Caculo_CabaÃ§a_hydroelectr...  \n",
       "22  G100000600013  https://www.gem.wiki/Cambambe_I_hydroelectric_...  \n",
       "23  G100000600014  https://www.gem.wiki/Cambambe_II_hydroelectric...  \n",
       "24  G100000600015   https://www.gem.wiki/Capanda_hydroelectric_plant  \n",
       "25  G100001030660  https://www.gem.wiki/Gove_Dam_hydroelectric_plant  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_hpp = 'data_grdc_hydro_capp/Global-Hydropower-Tracker-April-2025.xlsx'\n",
    "if not os.path.exists(path_hpp):\n",
    "    raise FileNotFoundError(f\"Hydropower data file not found: {path_hpp}\")\n",
    "\n",
    "data_hpp = pd.read_excel(path_hpp, sheet_name='Data', header=[0], index_col=None)\n",
    "\n",
    "data_hpp = data_hpp.loc[data_hpp['Country/Area 1'].isin(countries), :]\n",
    "\n",
    "display(data_hpp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e5bfdc5ba323a88f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:53.298165Z",
     "start_time": "2025-06-23T15:08:53.290826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add river name to runoff data\n",
    "meta_df = data_discharge[[\"station_name\", \"river_name\"]].to_dataframe().reset_index()\n",
    "meta_df = meta_df.drop_duplicates(subset=[\"station_name\"])\n",
    "\n",
    "data_station_filtered_river = data_station_filtered.merge(meta_df, on=\"station_name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e4bb1452b01dadad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:53.374453Z",
     "start_time": "2025-06-23T15:08:53.367211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>river_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KINSHASA</td>\n",
       "      <td>CONGO RIVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>BANGUI</td>\n",
       "      <td>UBANGI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>LAMBARENE</td>\n",
       "      <td>OGOOUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>NDJAMENA(FORT LAMY)</td>\n",
       "      <td>CHARI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>SARH(FORT ARCHAMBAULT)</td>\n",
       "      <td>CHARI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24100</th>\n",
       "      <td>KAGA-BANDORO</td>\n",
       "      <td>GRIBINGUI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24106</th>\n",
       "      <td>BANGASSOU</td>\n",
       "      <td>MBOMOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>MUYANGE_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>MUYANGE_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24518</th>\n",
       "      <td>GISURU</td>\n",
       "      <td>RUMPUNGWE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 station_name   river_name\n",
       "0                    KINSHASA  CONGO RIVER\n",
       "133                    BANGUI       UBANGI\n",
       "372                 LAMBARENE       OGOOUE\n",
       "470       NDJAMENA(FORT LAMY)        CHARI\n",
       "640    SARH(FORT ARCHAMBAULT)        CHARI\n",
       "...                       ...          ...\n",
       "24100            KAGA-BANDORO    GRIBINGUI\n",
       "24106               BANGASSOU       MBOMOU\n",
       "24125               MUYANGE_3          NaN\n",
       "24126               MUYANGE_4          NaN\n",
       "24518                  GISURU    RUMPUNGWE\n",
       "\n",
       "[159 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_station_filtered_river[['station_name', 'river_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6787093ba0cc8bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:53.438285Z",
     "start_time": "2025-06-23T15:08:53.402376Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_river_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize unicode and remove accents (e.g. Ã© â e)\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove common river words and clean up spacing\n",
    "    name = re.sub(r\"\\b(RIVER|RIVIERE|RIVIÃRE|R\\.|R)\\b\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip().upper()\n",
    "\n",
    "    return name\n",
    "\n",
    "# Prepare cleaned lists\n",
    "station_rivers_raw = data_station_filtered_river['river_name'].dropna().unique()\n",
    "station_rivers_cleaned = {clean_river_name(r): r for r in station_rivers_raw}\n",
    "\n",
    "matches = []\n",
    "\n",
    "# Iterate over all HPPs (row-wise)\n",
    "for _, row in data_hpp.dropna(subset=[\"River / Watercourse\"]).iterrows():\n",
    "    hpp_river_orig = row[\"River / Watercourse\"]\n",
    "    hpp_river_clean = clean_river_name(hpp_river_orig)\n",
    "\n",
    "    # Find best match among station rivers\n",
    "    match, score, _ = process.extractOne(\n",
    "        hpp_river_clean, station_rivers_cleaned.keys(), scorer=fuzz.token_set_ratio\n",
    "    )\n",
    "\n",
    "    if score >= 85:\n",
    "        matched_station_river = station_rivers_cleaned[match]\n",
    "        stations = data_station_filtered_river[\n",
    "            data_station_filtered_river[\"river_name\"] == matched_station_river\n",
    "        ][\"station_name\"].tolist()\n",
    "    else:\n",
    "        matched_station_river = None\n",
    "        stations = []\n",
    "\n",
    "    matches.append({\n",
    "        \"hpp_name\": row[\"Project Name\"],\n",
    "        \"hpp_river\": hpp_river_orig,\n",
    "        \"matched_river\": matched_station_river,\n",
    "        \"score\": score,\n",
    "        \"stations\": set(stations),\n",
    "        \"hpp_capacity\": row[\"Capacity (MW)\"],\n",
    "        \"hpp_status\": row[\"Status\"]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy display or export\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.to_csv(os.path.join(folder_out, 'hpp_grdc_hydro_matches.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3eef254f4db555e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:53.743189Z",
     "start_time": "2025-06-23T15:08:53.457801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Associate hydropower plants with gauging stations based on proximity\n",
    "def associate_hpp_station(data_hpp, data_station, matches_df):\n",
    "    # Step 1: Build GeoDataFrames\n",
    "    gdf_hpp = gpd.GeoDataFrame(\n",
    "        data_hpp.copy(),\n",
    "        geometry=gpd.points_from_xy(data_hpp[\"Longitude\"], data_hpp[\"Latitude\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    gdf_station = gpd.GeoDataFrame(\n",
    "        data_station.copy(),\n",
    "        geometry=gpd.points_from_xy(data_station[\"geo_x\"], data_station[\"geo_y\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Project both to a metric CRS (meters)\n",
    "    gdf_hpp = gdf_hpp.to_crs(epsg=3857)\n",
    "    gdf_station = gdf_station.to_crs(epsg=3857)\n",
    "\n",
    "    # Step 3: Build dictionary from matches_df\n",
    "    station_lookup = {\n",
    "        row[\"hpp_name\"]: row[\"stations\"]\n",
    "        for _, row in matches_df.iterrows()\n",
    "        if row[\"stations\"]\n",
    "    }\n",
    "\n",
    "    # Step 4: For each HPP, associate station\n",
    "    results = []\n",
    "\n",
    "    for idx, hpp in gdf_hpp.iterrows():\n",
    "        hpp_name = hpp[\"Project Name\"]\n",
    "        if hpp_name in station_lookup and station_lookup[hpp_name]:\n",
    "            # Use the first station from the match (same river)\n",
    "            station_name = list(station_lookup[hpp_name])[0]\n",
    "            station = gdf_station[gdf_station[\"station_name\"] == station_name].iloc[0]\n",
    "            distance = hpp.geometry.distance(station.geometry)\n",
    "            results.append({\n",
    "                \"hpp_index\": idx,\n",
    "                \"nearest_station\": station_name,\n",
    "                \"distance_to_station_m\": distance,\n",
    "                \"same_river\": True\n",
    "            })\n",
    "        else:\n",
    "            # Find nearest station by distance\n",
    "            distances = gdf_station.geometry.distance(hpp.geometry)\n",
    "            min_idx = distances.idxmin()\n",
    "            nearest_station = gdf_station.loc[min_idx]\n",
    "            results.append({\n",
    "                \"hpp_index\": idx,\n",
    "                \"nearest_station\": nearest_station[\"station_name\"],\n",
    "                \"distance_to_station_m\": distances[min_idx],\n",
    "                \"same_river\": False\n",
    "            })\n",
    "\n",
    "    # Step 5: Merge back into HPP GeoDataFrame\n",
    "    nearest_df = pd.DataFrame(results)\n",
    "    gdf_hpp[\"nearest_station\"] = nearest_df[\"nearest_station\"].values\n",
    "    gdf_hpp[\"distance_to_station_m\"] = nearest_df[\"distance_to_station_m\"].values\n",
    "    gdf_hpp[\"same_river\"] = nearest_df[\"same_river\"].values\n",
    "\n",
    "    return gdf_hpp\n",
    "\n",
    "gdf_hpp = associate_hpp_station(data_hpp, data_station_filtered, matches_df)\n",
    "data_hpp = data_hpp.merge(\n",
    "    gdf_hpp[[\"Project Name\", \"nearest_station\", \"distance_to_station_m\", 'same_river']],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b70daca162c8b",
   "metadata": {},
   "source": [
    "## 6. Visualize all data on the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6536c22fdbe8e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:53.749772Z",
     "start_time": "2025-06-23T15:08:53.746760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to clip rivers to the bounding box of runoff stations\n",
    "def clip_rivers_to_stations(rivers_path, df_runoff, ord_stra=6):\n",
    "    # Step 1: Load the HydroRIVERS shapefile\n",
    "\n",
    "    # To be downloaded from: https://www.hydrosheds.org/products/hydrorivers using shapefile format\n",
    "    # Load available layers (should return e.g. 'HydroRIVERS_v10_af')\n",
    "\n",
    "    if not os.path.exists(rivers_path):\n",
    "        raise FileNotFoundError(f\"Geodatabase not found: {rivers_path}\")\n",
    "\n",
    "    # Load rivers\n",
    "    rivers_gdf = gpd.read_file(rivers_path)\n",
    "\n",
    "    # Filter based on average discharge and stream order\n",
    "    rivers_gdf = rivers_gdf[(rivers_gdf[\"DIS_AV_CMS\"] > 50) & (rivers_gdf[\"ORD_STRA\"] >= 6) & (rivers_gdf[\"ORD_FLOW\"] <= 5)]\n",
    "    #rivers_gdf = rivers_gdf[(rivers_gdf[\"ORD_STRA\"] >= ord_stra)]\n",
    "\n",
    "    # Step 2: Prepare the bounding box for clipping\n",
    "    # Create GeoDataFrame from stations\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        df_runoff,\n",
    "        geometry=gpd.points_from_xy(df_runoff[\"geo_x\"], df_runoff[\"geo_y\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Get bounding box coordinates from the stations\n",
    "    roi_bounds = stations_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Create a polygon box from the bounds\n",
    "    roi_polygon = box(*roi_bounds).buffer(1.0)  # optional padding of 1 degree\n",
    "\n",
    "    # Create a GeoDataFrame for clipping\n",
    "    roi_gdf = gpd.GeoDataFrame(geometry=[roi_polygon], crs=\"EPSG:4326\")\n",
    "\n",
    "    rivers_clipped = gpd.clip(rivers_gdf, roi_gdf)\n",
    "\n",
    "    return rivers_clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "90fa62608c26fb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:09:11.072922Z",
     "start_time": "2025-06-23T15:08:53.818148Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell can take a while to run (some minutes), depending on the number of stations and rivers\n",
    "def make_maps(df_runoff, data_hpp=None, rivers_path=None, folder_out=folder_out):\n",
    "\n",
    "    # Read station data\n",
    "    agg = (\n",
    "        df_runoff.groupby([\"station_name\", \"geo_x\", \"geo_y\"])\n",
    "          .agg(\n",
    "              avg_runoff=(\"runoff_mm_year\", \"mean\"),\n",
    "              n_years=(\"year\", \"count\")\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create base map centered on the region\n",
    "    center_lat = agg[\"geo_y\"].mean()\n",
    "    center_lon = agg[\"geo_x\"].mean()\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=5, tiles=\"CartoDB positron\")\n",
    "\n",
    "    # Add rivers from HydroRivers\n",
    "    name = f'station_runoff_map.html'\n",
    "    if rivers_path is not None:\n",
    "        rivers_clipped = clip_rivers_to_stations(rivers_path, df_runoff)\n",
    "\n",
    "        colormap = plt.cm.viridis\n",
    "        norm = mcolors.Normalize(\n",
    "            vmin=rivers_clipped[\"DIS_AV_CMS\"].min(),\n",
    "            vmax=rivers_clipped[\"DIS_AV_CMS\"].max())\n",
    "\n",
    "\n",
    "        # Add rivers to the map with color based on discharge\n",
    "        for _, row in rivers_clipped.iterrows():\n",
    "            discharge = row[\"DIS_AV_CMS\"]\n",
    "            color = mcolors.to_hex(colormap(norm(discharge)))\n",
    "\n",
    "            folium.GeoJson(\n",
    "                row[\"geometry\"],\n",
    "                style_function=lambda feature, color=color: {\n",
    "                    \"color\": color,\n",
    "                    \"weight\": 2,\n",
    "                    \"opacity\": 0.8\n",
    "                }\n",
    "            ).add_to(m)\n",
    "\n",
    "\n",
    "        # Define vmin and vmax\n",
    "        vmin = rivers_clipped[\"DIS_AV_CMS\"].min()\n",
    "        vmax = rivers_clipped[\"DIS_AV_CMS\"].max()\n",
    "\n",
    "        ticks_raw = np.linspace(vmin, vmax, 5)\n",
    "        ticks = [round(t, -int(np.floor(np.log10(t))) + 1) for t in ticks_raw]  # e.g., 72 â 70, 1502 â 1500\n",
    "\n",
    "        norm = mcolors.Normalize(vmin=min(ticks), vmax=max(ticks))\n",
    "        colors = [mcolors.to_hex(plt.cm.viridis(norm(t))) for t in ticks]\n",
    "\n",
    "        # Add high-contrast legend\n",
    "        discharge_colormap = LinearColormap(\n",
    "            colors=colors,\n",
    "            index=ticks,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            caption=\"Avg River Discharge (mÂ³/s)\"\n",
    "        )\n",
    "        discharge_colormap.add_to(m)\n",
    "\n",
    "        name = f'station_runoff_map_with_rivers.html'\n",
    "\n",
    "    # Include station\n",
    "    if agg is not None:\n",
    "        norm_years = LogNorm(vmin=agg[\"n_years\"].min(), vmax=agg[\"n_years\"].max())\n",
    "        colormap = plt.cm.viridis  # or any other colormap\n",
    "\n",
    "        for _, row in agg.iterrows():\n",
    "\n",
    "            # Normalize color from colormap\n",
    "            rgba = colormap(norm_years(row[\"n_years\"]))\n",
    "            hex_color = mcolors.to_hex(rgba)\n",
    "\n",
    "            # Scale size (radius) based on average runoff\n",
    "            radius = max(4, (row[\"avg_runoff\"] / agg[\"avg_runoff\"].max()) * 20)\n",
    "\n",
    "            # Add circle marker\n",
    "            folium.CircleMarker(\n",
    "                location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                radius=radius,\n",
    "                color=hex_color,\n",
    "                fill=True,\n",
    "                fill_opacity=0.8,\n",
    "                popup=(f\"<b>{row['station_name']}</b><br>\"\n",
    "                       f\"Avg Runoff: {row['avg_runoff']:.0f} mm/year<br>\"\n",
    "                       f\"Years: {row['n_years']}\")\n",
    "            ).add_to(m)\n",
    "\n",
    "\n",
    "        # FeatureGroup for removed stations\n",
    "        removed_stations_group = folium.FeatureGroup(name=\"Removed Stations\", show=True)\n",
    "\n",
    "        # Plot removed due to missing area (red X)\n",
    "        if stations_removed_area_df is not None and not stations_removed_area_df.empty:\n",
    "            for _, row in stations_removed_area_df.iterrows():\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                    number_of_sides=4,\n",
    "                    radius=5,\n",
    "                    rotation=45,\n",
    "                    color=\"red\",\n",
    "                    fill=True,\n",
    "                    fill_color=\"red\",\n",
    "                    fill_opacity=1.0,\n",
    "                    popup=f\"<b>{row['station_name']}</b><br>Removed: Area â¤ 0\"\n",
    "                ).add_to(removed_stations_group)\n",
    "\n",
    "        # Plot removed due to low flow (orange X)\n",
    "        if stations_removed_lowflow_df is not None and not stations_removed_lowflow_df.empty:\n",
    "            for _, row in stations_removed_lowflow_df.iterrows():\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                    number_of_sides=4,\n",
    "                    radius=5,\n",
    "                    rotation=45,\n",
    "                    color=\"orange\",\n",
    "                    fill=True,\n",
    "                    fill_color=\"orange\",\n",
    "                    fill_opacity=1.0,\n",
    "                    popup=f\"<b>{row['station_name']}</b><br>Removed: Flow < threshold\"\n",
    "                ).add_to(removed_stations_group)\n",
    "\n",
    "        # Add group to map\n",
    "        removed_stations_group.add_to(m)\n",
    "\n",
    "        # Create a combined legend for stations color and size\n",
    "        years_vals = np.percentile(agg[\"n_years\"], [0, 50, 100]).round(0).astype(int)\n",
    "        runoff_vals = np.percentile(agg[\"avg_runoff\"], [0, 50, 100]).round(0)\n",
    "\n",
    "        combined_legend_html = f'''\n",
    "        <div style=\"\n",
    "            position: fixed;\n",
    "            bottom: 50px; left: 50px; width: 260px;\n",
    "            background-color: white;\n",
    "            border: 2px solid grey;\n",
    "            z-index: 9999;\n",
    "            font-size: 13px;\n",
    "            padding: 12px;\">\n",
    "\n",
    "            <b style=\"font-size:14px;\">Gauging Stations</b><br><br>\n",
    "\n",
    "            <b>Years of Available Data (Color)</b><br>\n",
    "            <i style=\"background: #440154; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[0]}<br>\n",
    "            <i style=\"background: #21918c; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[1]}<br>\n",
    "            <i style=\"background: #fde725; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[2]}<br><br>\n",
    "\n",
    "            <b>Avg Runoff (mm/year)</b><br>\n",
    "            <svg width=\"120\" height=\"90\">\n",
    "              <circle cx=\"30\" cy=\"20\" r=\"6\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"25\" font-size=\"12\">{int(runoff_vals[0])}</text>\n",
    "              <circle cx=\"30\" cy=\"45\" r=\"10\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"50\" font-size=\"12\">{int(runoff_vals[1])}</text>\n",
    "              <circle cx=\"30\" cy=\"75\" r=\"14\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"80\" font-size=\"12\">{int(runoff_vals[2])}</text>\n",
    "            </svg>\n",
    "\n",
    "            <b>Removed Stations</b><br>\n",
    "            <i class=\"fa fa-times\" style=\"color:red; margin-right: 6px;\"></i> Area missing<br>\n",
    "            <i class=\"fa fa-times\" style=\"color:orange; margin-right: 6px;\"></i> Low flow\n",
    "        </div>\n",
    "        '''\n",
    "\n",
    "        # Add both legend to the map\n",
    "        m.get_root().html.add_child(folium.Element(combined_legend_html))\n",
    "\n",
    "    # Include hydropower plants\n",
    "    if data_hpp is not None:\n",
    "\n",
    "        # Define fixed status colors\n",
    "        custom_status_colors = {\n",
    "            'operating': '#2ca02c',           # green\n",
    "            'construction': '#ff7f0e',        # orange\n",
    "            'announced': '#999999',           # grey\n",
    "            'pre-construction': '#9467bd'     # purple\n",
    "        }\n",
    "\n",
    "        # Filter data_hpp for selected statuses only\n",
    "        valid_statuses = list(custom_status_colors.keys())\n",
    "        data_hpp = data_hpp[data_hpp['Status'].isin(valid_statuses)]\n",
    "\n",
    "\n",
    "        # Normalize capacity for marker sizing\n",
    "        cap_min = data_hpp['Capacity (MW)'].min()\n",
    "        cap_max = data_hpp['Capacity (MW)'].max()\n",
    "\n",
    "        def normalize_capacity(value):\n",
    "            return 6 + ((value - cap_min) / (cap_max - cap_min + 1e-6)) * 10  # radius 4â14\n",
    "\n",
    "\n",
    "        # Convert RGBA to HEX for Folium\n",
    "        status_colors = {k: mcolors.to_hex(v) for k, v in custom_status_colors.items()}\n",
    "\n",
    "        # Add each HPP as a square marker\n",
    "        for _, row in data_hpp.iterrows():\n",
    "            popup_html = \"<br>\".join([f\"<b>{k}</b>: {v}\" for k, v in row.items()])\n",
    "            color = status_colors.get(row['Status'], '#555')\n",
    "            size = normalize_capacity(row['Capacity (MW)'])\n",
    "\n",
    "            folium.RegularPolygonMarker(\n",
    "                location=[row['Latitude'], row['Longitude']],\n",
    "                number_of_sides=4,  # square\n",
    "                radius=size,\n",
    "                color=None,\n",
    "                fill=True,\n",
    "                fill_color=color,\n",
    "                fill_opacity=0.9,\n",
    "                popup=folium.Popup(popup_html, max_width=300)\n",
    "            ).add_to(m)\n",
    "\n",
    "        # Prepare capacity values for size scale legend\n",
    "        capacity_vals = np.percentile(data_hpp['Capacity (MW)'], [0, 50, 100]).round(0).astype(int)\n",
    "        circle_sizes = [6, 10, 14]  # Match values used in marker radius scaling\n",
    "\n",
    "\n",
    "        # Create status color legend HTML\n",
    "        status_legend_items = \"\"\n",
    "        for status in valid_statuses:\n",
    "            hex_color = custom_status_colors[status]\n",
    "            status_legend_items += f'''\n",
    "                <i style=\"background:{hex_color}; width: 18px; height: 18px;\n",
    "                float: left; margin-right: 6px;\"></i> {status}<br>\n",
    "            '''\n",
    "\n",
    "        # Full HTML block\n",
    "        hpp_legend_html = f'''\n",
    "        <div style=\"\n",
    "            position: fixed;\n",
    "            bottom: 50px; left: 340px; width: 280px;\n",
    "            background-color: white;\n",
    "            border: 2px solid grey;\n",
    "            z-index: 9999;\n",
    "            font-size: 13px;\n",
    "            padding: 12px;\">\n",
    "\n",
    "            <b style=\"font-size:14px;\">Hydropower Plants</b><br><br>\n",
    "\n",
    "            <b>Status (Color)</b><br>\n",
    "            {status_legend_items}\n",
    "            <br>\n",
    "\n",
    "            <b>Capacity (MW)</b><br>\n",
    "            <svg width=\"160\" height=\"100\">\n",
    "              <rect x=\"20\" y=\"10\" width=\"{circle_sizes[0]*2}\" height=\"{circle_sizes[0]*2}\" fill=\"#999\"/>\n",
    "              <text x=\"60\" y=\"25\" font-size=\"12\">{capacity_vals[0]}</text>\n",
    "              <rect x=\"20\" y=\"40\" width=\"{circle_sizes[1]*2}\" height=\"{circle_sizes[1]*2}\" fill=\"#999\"/>\n",
    "              <text x=\"60\" y=\"55\" font-size=\"12\">{capacity_vals[1]}</text>\n",
    "              <rect x=\"20\" y=\"70\" width=\"{circle_sizes[2]*2}\" height=\"{circle_sizes[2]*2}\" fill=\"#999\"/>\n",
    "              <text x=\"60\" y=\"85\" font-size=\"12\">{capacity_vals[2]}</text>\n",
    "            </svg>\n",
    "        </div>\n",
    "        '''\n",
    "        m.get_root().html.add_child(folium.Element(hpp_legend_html))\n",
    "\n",
    "\n",
    "    # Step 4: Save or display\n",
    "    m.save(os.path.join(folder_out, name))\n",
    "    m\n",
    "\n",
    "\n",
    "# Add info\n",
    "\n",
    "\n",
    "make_maps(data_runoff, data_hpp=data_hpp, rivers_path = \"data_grdc_hydro_capp/hydro_input/HydroRIVERS_v10_af_shp/HydroRIVERS_v10_af.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309cd3489781576",
   "metadata": {},
   "source": [
    "## 7. Plot station runoff for each hydropower plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6517211419d7211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:09:11.085204Z",
     "start_time": "2025-06-23T15:09:11.082052Z"
    }
   },
   "outputs": [],
   "source": [
    "stations = data_hpp['nearest_station'].unique()\n",
    "# Filter data_runoff to include only stations that have hydropower plants associated\n",
    "temp = data_runoff[data_runoff['station_name'].isin(stations)].copy()\n",
    "# Subplots for each station and evolution of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e477dc719cb3b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:09:11.155851Z",
     "start_time": "2025-06-23T15:09:11.154166Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e7d4577ad21f9b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:49:26.693711Z",
     "start_time": "2025-06-23T15:49:24.414197Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_runoff_evolution(data_runoff, data_hpp, folder_out=None):\n",
    "    stations = data_hpp['nearest_station'].unique()\n",
    "    temp = data_runoff[data_runoff['station_name'].isin(stations)].copy()\n",
    "\n",
    "    # Map station_name to project name and river match info\n",
    "    station_to_project = data_hpp.set_index('nearest_station')['Project Name'].to_dict()\n",
    "    station_to_river_match = data_hpp.set_index('nearest_station')['same_river'].to_dict()\n",
    "\n",
    "    temp['Project Name'] = temp['station_name'].map(station_to_project)\n",
    "    temp['same_river'] = temp['station_name'].map(station_to_river_match)\n",
    "\n",
    "    n_stations = len(stations)\n",
    "    ncols = 3\n",
    "    nrows = (n_stations + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), sharey=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    min_year = temp['year'].min()\n",
    "    max_year = temp['year'].max()\n",
    "\n",
    "    for i, station in enumerate(stations):\n",
    "        ax = axs[i]\n",
    "        subset = temp[temp['station_name'] == station]\n",
    "        ax.plot(subset['year'].astype(int), subset['runoff_mm_year'], marker='o', linestyle='-')\n",
    "        match_label = \"Same River\" if station_to_river_match[station] else \"Closest Station\"\n",
    "        ax.set_title(f\"{station} / {station_to_project[station]} ({match_label})\", fontsize=10)\n",
    "        ax.set_ylabel(\"Runoff (mm/year)\")\n",
    "        ax.set_xlim(min_year, max_year)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if folder_out:\n",
    "        plt.savefig(os.path.join(folder_out, 'runoff_evolution_with_river_info.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_runoff_evolution(data_runoff, data_hpp, folder_out=folder_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "60ed38604ed1657b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:10:06.466764Z",
     "start_time": "2025-06-23T16:10:04.117874Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_discharge_month(data_station_filtered, data_hpp, folder_out=None):\n",
    "\n",
    "    stations = data_hpp['nearest_station'].unique()\n",
    "    temp = data_station_filtered[data_station_filtered['station_name'].isin(stations)].copy()\n",
    "\n",
    "    # Map station_name to project name and river match info\n",
    "    station_to_project = data_hpp.set_index('nearest_station')['Project Name'].to_dict()\n",
    "    station_to_river_match = data_hpp.set_index('nearest_station')['same_river'].to_dict()\n",
    "\n",
    "    temp['Project Name'] = temp['station_name'].map(station_to_project)\n",
    "    temp['same_river'] = temp['station_name'].map(station_to_river_match)\n",
    "\n",
    "    n_stations = len(stations)\n",
    "    ncols = 3\n",
    "    nrows = (n_stations + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), sharey=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, station in enumerate(stations):\n",
    "        ax = axs[i]\n",
    "        subset = temp[temp['station_name'] == station]\n",
    "        subset = subset.groupby(['Project Name', 'month', 'same_river'])['Q'].mean().reset_index()\n",
    "        subset['month_abbr'] = subset['month'].astype(int).apply(lambda x: calendar.month_abbr[x])\n",
    "        ax.plot(subset['month_abbr'], subset['Q'], marker='o', linestyle='-')\n",
    "        match_label = \"Same River\" if station_to_river_match[station] else \"Closest Station\"\n",
    "        ax.set_title(f\"{station} / {station_to_project[station]} ({match_label})\", fontsize=10)\n",
    "        ax.set_ylabel(\"Discharge (m3/s)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if folder_out:\n",
    "        plt.savefig(os.path.join(folder_out, 'runoff_evolution_with_river_info.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "plot_discharge_month(data_station_filtered, data_hpp, folder_out=folder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75611ccde26c67b",
   "metadata": {},
   "source": [
    "## Filling missing values (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57011395a5bcb5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T13:01:28.005778Z",
     "start_time": "2025-06-23T13:01:27.998644Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_missing_climatology(df, min_months=3):\n",
    "    \"\"\"\n",
    "    Fill missing values using monthly climatology (station-wise).\n",
    "    Returns a filled DataFrame and a mask of filled values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Filter out sparse years\n",
    "    valid_years = (\n",
    "        df.groupby([\"station_name\", \"year\"])[\"Q\"]\n",
    "        .apply(lambda x: x.notna().sum() >= min_months)\n",
    "        .reset_index(name=\"keep\")\n",
    "    )\n",
    "\n",
    "    # Merge to filter out sparse rows\n",
    "    df = df.merge(valid_years[valid_years[\"keep\"]], on=[\"station_name\", \"year\"])\n",
    "    df.drop(columns=\"keep\", inplace=True)\n",
    "\n",
    "    # Step 2: Build climatology\n",
    "    climatology = (\n",
    "        df.groupby([\"station_name\", \"month\"])[\"Q\"]\n",
    "        .mean()\n",
    "        .rename(\"Q_clim\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df = df.merge(climatology, on=[\"station_name\", \"month\"], how=\"left\")\n",
    "\n",
    "    # Step 3: Fill missing with climatology\n",
    "    fill_mask = df[\"Q\"].isna()\n",
    "    df.loc[fill_mask, \"Q\"] = df.loc[fill_mask, \"Q_clim\"]\n",
    "\n",
    "    df.drop(columns=\"Q_clim\", inplace=True)\n",
    "\n",
    "    return df, fill_mask\n",
    "\n",
    "def drop_sparse_years_and_interpolate(df, min_months=9):\n",
    "    \"\"\"\n",
    "    Drops years with too much missing data and interpolates gaps per station.\n",
    "    Assumes monthly data. Returns cleaned & interpolated DataFrame.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Count non-NaN entries per year-station\n",
    "    valid_counts = (\n",
    "        df_clean.groupby([\"station_name\", \"year\"])[\"Q\"]\n",
    "        .apply(lambda x: x.notna().sum())\n",
    "        .rename(\"valid_months\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep only rows with enough months\n",
    "    valid_years = valid_counts[valid_counts[\"valid_months\"] >= min_months]\n",
    "    df_clean = df_clean.merge(valid_years[[\"station_name\", \"year\"]], on=[\"station_name\", \"year\"])\n",
    "\n",
    "    # Sort for interpolation\n",
    "    df_clean = df_clean.sort_values([\"station_name\", \"year\", \"month\"])\n",
    "\n",
    "    # Interpolate per station\n",
    "    df_clean[\"Q\"] = df_clean.groupby(\"station_name\")[\"Q\"].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def plot_all_fill_methods(df_original, df_clim, df_interp, output_dir):\n",
    "    \"\"\"\n",
    "    Plots original, climatology-filled, and interpolated runoff data per station in one plot.\n",
    "\n",
    "    Assumes all DataFrames have columns: ['year', 'month', 'station_name', 'Q'].\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_dir = os.path.join(output_dir, 'stations')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Combine 'year' and 'month' into datetime\n",
    "    def add_datetime(df):\n",
    "        return df.assign(\n",
    "            date=pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2))\n",
    "        )\n",
    "\n",
    "    df_original = add_datetime(df_original)\n",
    "    df_clim = add_datetime(df_clim)\n",
    "    df_interp = add_datetime(df_interp)\n",
    "\n",
    "    # Loop over each station\n",
    "    stations = df_original['station_name'].unique()\n",
    "\n",
    "    for station in stations:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "        # Subsets for current station\n",
    "        df_o = df_original[df_original['station_name'] == station]\n",
    "        df_c = df_clim[df_clim['station_name'] == station]\n",
    "        df_i = df_interp[df_interp['station_name'] == station]\n",
    "\n",
    "        # Plot all\n",
    "        ax.plot(df_o['date'], df_o['Q'], label=\"Original\", alpha=0.5, marker='o', linestyle='-', color='black')\n",
    "        ax.plot(df_c['date'], df_c['Q'], label=\"Climatology Fill\", linestyle='--', color='orange')\n",
    "        ax.plot(df_i['date'], df_i['Q'], label=\"Interpolated\", linestyle='-', color='blue')\n",
    "\n",
    "        ax.set_title(f\"Station: {station}\")\n",
    "        ax.set_ylabel(\"Discharge / Runoff (Q)\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{station}.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e919231d3dacdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T13:01:46.877202Z",
     "start_time": "2025-06-23T13:01:28.037004Z"
    }
   },
   "outputs": [],
   "source": [
    "df_filled, filled_mask = fill_missing_climatology(df_filtered, min_months=6)\n",
    "df_interpolated = drop_sparse_years_and_interpolate(df_filtered, min_months=9)\n",
    "plot_all_fill_methods(df_filtered, df_filled, df_interpolated, output_dir=folder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2efda41dbea017de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T13:01:46.895286Z",
     "start_time": "2025-06-23T13:01:46.888880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>Q</th>\n",
       "      <th>station_name</th>\n",
       "      <th>geo_x</th>\n",
       "      <th>geo_y</th>\n",
       "      <th>area</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>station_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1903-01-01</td>\n",
       "      <td>1926500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MANUEL CAROCA</td>\n",
       "      <td>6.650000</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>1903</td>\n",
       "      <td>1</td>\n",
       "      <td>MANUEL CAROCA (0.12, 6.65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1903-01-01</td>\n",
       "      <td>1626100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANDOK-FOULA</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>1903</td>\n",
       "      <td>1</td>\n",
       "      <td>ANDOK-FOULA (0.37, 10.23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1903-01-01</td>\n",
       "      <td>1643100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LAMBARENE</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>-0.68000</td>\n",
       "      <td>205000.000000</td>\n",
       "      <td>1903</td>\n",
       "      <td>1</td>\n",
       "      <td>LAMBARENE (-0.68, 10.23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1903-01-01</td>\n",
       "      <td>1643150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOUGAMOU</td>\n",
       "      <td>10.593750</td>\n",
       "      <td>-1.21041</td>\n",
       "      <td>22000.000000</td>\n",
       "      <td>1903</td>\n",
       "      <td>1</td>\n",
       "      <td>FOUGAMOU (-1.21, 10.59)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903-01-01</td>\n",
       "      <td>1643200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NDJOLE</td>\n",
       "      <td>10.770000</td>\n",
       "      <td>-0.18000</td>\n",
       "      <td>158100.000000</td>\n",
       "      <td>1903</td>\n",
       "      <td>1</td>\n",
       "      <td>NDJOLE (-0.18, 10.77)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224347</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>1670141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUYANGE_3</td>\n",
       "      <td>29.860600</td>\n",
       "      <td>-3.39190</td>\n",
       "      <td>1505.599976</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>MUYANGE_3 (-3.39, 29.86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224348</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>1670145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUYANGE_4</td>\n",
       "      <td>29.809700</td>\n",
       "      <td>-3.52360</td>\n",
       "      <td>662.000000</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>MUYANGE_4 (-3.52, 29.81)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224349</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>1670150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUBUGA</td>\n",
       "      <td>30.041401</td>\n",
       "      <td>-3.37220</td>\n",
       "      <td>933.400024</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>MUBUGA (-3.37, 30.04)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224350</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>1670160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYANKANDA</td>\n",
       "      <td>30.318001</td>\n",
       "      <td>-3.29030</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>NYANKANDA (-3.29, 30.32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224351</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>1670200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NGOZI-BUTARE</td>\n",
       "      <td>29.719400</td>\n",
       "      <td>-2.81000</td>\n",
       "      <td>1470.000000</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>NGOZI-BUTARE (-2.81, 29.72)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224352 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time       id   Q   station_name      geo_x    geo_y  \\\n",
       "0      1903-01-01  1926500 NaN  MANUEL CAROCA   6.650000  0.12000   \n",
       "1      1903-01-01  1626100 NaN    ANDOK-FOULA  10.230000  0.37000   \n",
       "2      1903-01-01  1643100 NaN      LAMBARENE  10.230000 -0.68000   \n",
       "3      1903-01-01  1643150 NaN       FOUGAMOU  10.593750 -1.21041   \n",
       "4      1903-01-01  1643200 NaN         NDJOLE  10.770000 -0.18000   \n",
       "...           ...      ...  ..            ...        ...      ...   \n",
       "224347 2016-12-01  1670141 NaN      MUYANGE_3  29.860600 -3.39190   \n",
       "224348 2016-12-01  1670145 NaN      MUYANGE_4  29.809700 -3.52360   \n",
       "224349 2016-12-01  1670150 NaN         MUBUGA  30.041401 -3.37220   \n",
       "224350 2016-12-01  1670160 NaN      NYANKANDA  30.318001 -3.29030   \n",
       "224351 2016-12-01  1670200 NaN   NGOZI-BUTARE  29.719400 -2.81000   \n",
       "\n",
       "                 area  year  month                station_label  \n",
       "0          103.199997  1903      1   MANUEL CAROCA (0.12, 6.65)  \n",
       "1         1700.000000  1903      1    ANDOK-FOULA (0.37, 10.23)  \n",
       "2       205000.000000  1903      1     LAMBARENE (-0.68, 10.23)  \n",
       "3        22000.000000  1903      1      FOUGAMOU (-1.21, 10.59)  \n",
       "4       158100.000000  1903      1        NDJOLE (-0.18, 10.77)  \n",
       "...               ...   ...    ...                          ...  \n",
       "224347    1505.599976  2016     12     MUYANGE_3 (-3.39, 29.86)  \n",
       "224348     662.000000  2016     12     MUYANGE_4 (-3.52, 29.81)  \n",
       "224349     933.400024  2016     12        MUBUGA (-3.37, 30.04)  \n",
       "224350     682.000000  2016     12     NYANKANDA (-3.29, 30.32)  \n",
       "224351    1470.000000  2016     12  NGOZI-BUTARE (-2.81, 29.72)  \n",
       "\n",
       "[224352 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6989afbb929d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T13:01:46.950884Z",
     "start_time": "2025-06-23T13:01:46.949411Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
