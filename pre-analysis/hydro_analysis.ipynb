{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35df81a60a3c6ca1",
   "metadata": {},
   "source": [
    "# GRDC Hydro Data Processing and Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook processes and analyzes streamflow data from the Global Runoff Data Centre (GRDC) to create a comprehensive dataset of river discharge and runoff. The analysis is particularly useful for hydropower assessment, water resource management, and climate impact studies in Central African regions.\n",
    "\n",
    "## Required Input Data\n",
    "This analysis requires three types of input data, all available from open-source repositories. You must download these datasets separately before running this notebook:\n",
    "\n",
    "1. **Monthly Streamflow Data from GRDC Stations**\n",
    "   - Source: [Global Runoff Data Centre (GRDC)](https://www.bafg.de/GRDC/)\n",
    "   - Format: NetCDF files containing monthly discharge measurements\n",
    "   - Required fields: station metadata (coordinates, catchment area), discharge time series\n",
    "\n",
    "2. **Hydropower Plant Data from the Global Hydropower Tracker**\n",
    "   - Source: [Global Energy Monitor](https://globalenergymonitor.org/projects/global-hydropower-tracker/)\n",
    "   - Format: Excel spreadsheet (.xlsx)\n",
    "   - Required fields: plant names, locations, capacities, status, river names\n",
    "\n",
    "3. **River Network Data from HydroRIVERS**\n",
    "   - Source: [HydroSHEDS](https://www.hydrosheds.org/products/hydrorivers)\n",
    "   - Format: Shapefile (.shp)\n",
    "   - Required fields: river geometries, names, discharge estimates\n",
    "\n",
    "## Expected Outputs\n",
    "This notebook will generate:\n",
    "- Filtered and processed discharge and runoff datasets\n",
    "- Interactive maps showing stations, rivers, and hydropower plants\n",
    "- Time series plots of runoff evolution\n",
    "- Monthly discharge patterns for hydropower-relevant stations\n",
    "- CSV files with processed data for further analysis\n",
    "\n",
    "## Directory Structure\n",
    "Before running, ensure you have the following directory structure:\n",
    "```\n",
    "hydro/\n",
    "├── input/\n",
    "│   ├── grdc_input/         # GRDC station data (NetCDF files)\n",
    "│   ├── river_input/        # HydroRIVERS shapefiles\n",
    "│   └── Global-Hydropower-Tracker-April-2025.xlsx\n",
    "└── output/                 # Will be created if it doesn't exist\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a43cbf66c979286",
   "metadata": {},
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import folium\n",
    "import folium.plugins\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from branca.colormap import LinearColormap\n",
    "from folium import CircleMarker\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import box\n",
    "import calendar\n",
    "import numpy as np\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Import local utilities\n",
    "from utils import map_grdc_stationbasins_and_subregions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e396e191b773eaf8",
   "metadata": {},
   "source": [
    "## User Input Configuration\n",
    "\n",
    "### Purpose\n",
    "This section defines the geographical scope of the analysis and sets up the directory structure for input and output files. You need to customize these parameters to match your specific analysis requirements.\n",
    "\n",
    "### Parameters to Configure\n",
    "- **countries**: List of countries to include in the analysis. The hydropower plant data will be filtered to include only plants in these countries.\n",
    "- **base_dir**: Base directory for all hydro-related data. This should point to where your input data is stored.\n",
    "- **folder_in**: Directory containing all input data files (GRDC data, hydropower tracker, river shapefiles).\n",
    "- **folder_out**: Directory where all output files will be saved (automatically created if it doesn't exist).\n",
    "\n",
    "### Before Running\n",
    "1. Ensure all required input data is downloaded and placed in the correct directories\n",
    "2. Verify that the country names match exactly those used in the Global Hydropower Tracker dataset\n",
    "3. Check that you have write permissions for the output directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "97ba7cbd5074f56e",
   "metadata": {},
   "source": [
    "countries = ['Angola', 'Burundi', 'Cameroon', 'Central African Republic', 'Chad', 'Republic of the Congo', 'DR Congo', 'Equatorial Guinea', 'Gabon']\n",
    "\n",
    "# Define the base input directory\n",
    "base_dir = 'hydro'\n",
    "folder_in  = os.path.join(base_dir, 'input')\n",
    "folder_out = os.path.join(base_dir, 'output')\n",
    "if not os.path.exists(folder_out): os.makedirs(folder_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b7361de753733d",
   "metadata": {},
   "source": [
    "## 1. Load Streamflow Data\n",
    "\n",
    "### Purpose\n",
    "This section loads monthly streamflow data from GRDC NetCDF files. The data contains time series of river discharge measurements from gauging stations across the selected region, along with station metadata (coordinates, catchment area, etc.).\n",
    "\n",
    "### Process\n",
    "1. The code recursively searches through the `folder_grdc` directory for files named 'GRDC-Monthly.nc'\n",
    "2. Each NetCDF file is loaded as an xarray Dataset\n",
    "3. All datasets are concatenated into a single dataset for further processing\n",
    "\n",
    "### Technical Details\n",
    "- GRDC data is stored in NetCDF format (.nc files)\n",
    "- The xarray library is used to handle the multidimensional data efficiently\n",
    "- Each station has a unique identifier ('id') used to merge datasets\n",
    "- Key variables include:\n",
    "  - 'runoff_mean': Monthly discharge values (m³/s)\n",
    "  - 'area': Catchment area (km²)\n",
    "  - 'geo_x', 'geo_y': Station coordinates (longitude, latitude)\n",
    "  - 'station_name': Name of the gauging station\n",
    "  - 'river_name': Name of the river where the station is located\n",
    "\n",
    "### Troubleshooting\n",
    "- If no data is found, check that the GRDC files are in the correct directory structure\n",
    "- Ensure NetCDF files are properly formatted with the expected variables\n",
    "- Large datasets may require significant memory; consider filtering by region earlier if performance issues occur"
   ]
  },
  {
   "cell_type": "code",
   "id": "9824c38b4d3ac539",
   "metadata": {},
   "source": [
    "folder_grdc = os.path.join(folder_in, 'grdc_input')\n",
    "\n",
    "# Initialize a list to hold all xarray Datasets\n",
    "datasets = []\n",
    "\n",
    "# Walk through all directories under 'input'\n",
    "for root, dirs, files in os.walk(folder_grdc):\n",
    "    if 'GRDC-Monthly.nc' in files:\n",
    "        file_path = os.path.join(root, 'GRDC-Monthly.nc')\n",
    "        try:\n",
    "            data_discharge = xr.open_dataset(file_path)\n",
    "            datasets.append(data_discharge)\n",
    "            print(f\"Loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path}: {e}\")\n",
    "\n",
    "# Merge all datasets\n",
    "if datasets:\n",
    "    try:\n",
    "        data_discharge = xr.concat(datasets, dim='id')  # You can use another dim if needed\n",
    "        print(f\"Total merged dimensions: {data_discharge.dims}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge datasets: {e}\")\n",
    "else:\n",
    "    print(\"No streamflow data found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65647f4297482749",
   "metadata": {},
   "source": [
    "## 2. Filter Data for Quality Control\n",
    "\n",
    "### Purpose\n",
    "This section applies quality control filters to the streamflow data to ensure reliable analysis. Poor quality or incomplete data can lead to misleading results, so it's important to apply appropriate filtering criteria.\n",
    "\n",
    "### Filtering Steps\n",
    "1. **Remove Stations with Invalid Catchment Area**\n",
    "   - Stations with missing or non-positive catchment area values are removed\n",
    "   - Catchment area is essential for calculating runoff in mm/year\n",
    "   - These stations are flagged and their metadata is preserved for reference\n",
    "\n",
    "2. **Remove Stations with Low Runoff Values**\n",
    "   - Stations with consistently low discharge values (below 15 m³/s) are removed\n",
    "   - Low values may indicate intermittent streams, measurement errors, or stations on very small tributaries\n",
    "   - This threshold can be adjusted based on your specific analysis needs\n",
    "\n",
    "### Interpretation\n",
    "- The filtering process typically removes a subset of stations from the original dataset\n",
    "- The summary at the end shows how many stations were removed for each reason\n",
    "- Removed stations are not discarded completely but kept in separate dataframes for reference\n",
    "- In the visualization section, these removed stations will be shown as separate layers on the map\n",
    "\n",
    "### Considerations\n",
    "- Increasing the runoff threshold will result in fewer stations but potentially more reliable data\n",
    "- Decreasing the threshold will include more stations but may introduce noise from small or intermittent streams\n",
    "- For specific regional analyses, you might want to adjust these thresholds based on local hydrology"
   ]
  },
  {
   "cell_type": "code",
   "id": "3fc1228995adf798",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 1. Filter out stations with no valid area\n",
    "# -------------------------------\n",
    "\n",
    "# Identify stations with invalid or missing area\n",
    "invalid_area_mask = (data_discharge[\"area\"] <= 0) | data_discharge[\"area\"].isnull()\n",
    "removed_area_ids = data_discharge[\"id\"].values[invalid_area_mask.values]\n",
    "\n",
    "# Extract metadata of removed stations\n",
    "removed_area_meta = data_discharge.sel(id=removed_area_ids)[[\"station_name\", \"geo_x\", \"geo_y\", \"area\"]]\n",
    "stations_removed_area_df = removed_area_meta.to_dataframe().reset_index()\n",
    "\n",
    "# Remove stations with invalid area\n",
    "data_discharge = data_discharge.where(data_discharge[\"area\"] > 0, drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Filter out stations with low runoff values\n",
    "# -------------------------------\n",
    "\n",
    "threshold_runoff = 15  # m³/s\n",
    "\n",
    "# Identify stations where all runoff values are below threshold or NaN\n",
    "runoff = data_discharge[\"runoff_mean\"]\n",
    "low_flow_mask = (runoff < threshold_runoff) | runoff.isnull()\n",
    "stations_to_remove = low_flow_mask.all(dim=\"time\")\n",
    "removed_ids = data_discharge[\"id\"].values[stations_to_remove.values]\n",
    "\n",
    "# Extract metadata of removed stations\n",
    "removed_meta = data_discharge.sel(id=removed_ids)[[\"station_name\", \"geo_x\", \"geo_y\"]]\n",
    "stations_removed_lowflow_df = removed_meta.to_dataframe().reset_index()\n",
    "\n",
    "# Remove those stations from the dataset\n",
    "data_discharge = data_discharge.sel(id=~stations_to_remove)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Summary (optional display or export)\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Stations removed due to invalid area: {len(stations_removed_area_df)}\")\n",
    "print(f\"Stations removed due to low runoff: {len(stations_removed_lowflow_df)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d83376f05f63e5",
   "metadata": {},
   "source": [
    "## 3. Format Data for Analysis\n",
    "\n",
    "### Purpose\n",
    "This section transforms the filtered xarray Dataset into a more analysis-friendly format. The data is restructured to facilitate time series analysis, station comparison, and export to standard file formats.\n",
    "\n",
    "### Key Transformations\n",
    "1. **Convert to Pandas DataFrame**\n",
    "   - The xarray Dataset is converted to a pandas DataFrame for easier manipulation\n",
    "   - Station metadata (coordinates, names) is merged with the discharge data\n",
    "   - Time information is extracted into separate year and month columns\n",
    "\n",
    "2. **Handle Duplicate Station Names**\n",
    "   - The code checks for and resolves duplicate station names\n",
    "   - Duplicates are renamed with unique identifiers to prevent data confusion\n",
    "   - This is important because station names are used as identifiers in later analyses\n",
    "\n",
    "3. **Create Pivot Tables**\n",
    "   - Data is pivoted to create a wide-format table with years as rows and stations as columns\n",
    "   - This format is ideal for time series analysis and visualization\n",
    "   - The pivoted data is saved as a CSV file for external use\n",
    "\n",
    "### Output Files\n",
    "- **grdc_discharge_monthly-m3-s.csv**: Monthly discharge data in cubic meters per second\n",
    "  - Rows represent years\n",
    "  - Columns represent combinations of station names and months\n",
    "  - This format allows for easy filtering and aggregation by station or time period\n",
    "\n",
    "### Data Structure\n",
    "After processing, the main DataFrame (`data_station`) contains:\n",
    "- **station_name**: Name of the gauging station\n",
    "- **id**: Unique station identifier\n",
    "- **time**: Original timestamp\n",
    "- **year, month**: Extracted time components\n",
    "- **Q**: Discharge value in m³/s\n",
    "- **geo_x, geo_y**: Station coordinates\n",
    "- **area**: Catchment area in km²\n",
    "- **station_label**: Formatted label combining name and coordinates\n",
    "\n",
    "### Troubleshooting\n",
    "- If you encounter issues with duplicate station names, check the original data source\n",
    "- Missing values in the pivot table indicate months with no data for that station\n",
    "- Large datasets may cause memory issues; consider filtering to specific regions if needed"
   ]
  },
  {
   "cell_type": "code",
   "id": "4284d220a3fa2ae2",
   "metadata": {},
   "source": [
    "def checking_duplicates_grdc(df):\n",
    "    \"\"\"\n",
    "    Check for duplicated (year, station_name, month) entries in the DataFrame.\n",
    "\n",
    "    This function identifies records that have the same year, station_name, and month,\n",
    "    which should be unique in a properly formatted dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing at least the columns 'year', 'station_name', and 'month'\n",
    "\n",
    "    Returns:\n",
    "        list: List of station names that have duplicate entries\n",
    "    \"\"\"\n",
    "    dupes = (\n",
    "        df.groupby([\"year\", \"station_name\", \"month\"])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .query(\"count > 1\")\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(dupes)} duplicated (year, station_name, month) entries\")\n",
    "\n",
    "    problem_stations = dupes['station_name'].unique()\n",
    "    print(f'Duplicated stations: {problem_stations}')\n",
    "\n",
    "    # Merge the duplicate keys back into df to see full rows\n",
    "    duplicated_rows = df.merge(dupes[[\"year\", \"station_name\", \"month\"]], on=[\"year\", \"station_name\", \"month\"])\n",
    "    #display(duplicated_rows.sort_values([\"station_name\", \"year\", \"month\"]))\n",
    "\n",
    "    return problem_stations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f64c637dae24cb0a",
   "metadata": {},
   "source": [
    "var_name = 'runoff_mean'\n",
    "\n",
    "meta = data_discharge[[\"station_name\", \"geo_x\", \"geo_y\"]].to_dataframe().reset_index()\n",
    "area = data_discharge[\"area\"].to_dataframe().reset_index()\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_station = data_discharge[var_name].to_dataframe(name=\"Q\").reset_index()\n",
    "\n",
    "# Merge metadata into the main DataFrame\n",
    "data_station = data_station.merge(meta, on=\"id\")\n",
    "data_station = data_station.merge(area, on=\"id\")\n",
    "\n",
    "# Add year and month columns\n",
    "data_station[\"year\"] = data_station[\"time\"].dt.year\n",
    "data_station[\"month\"] = data_station[\"time\"].dt.month\n",
    "\n",
    "# Check if station_name has duplicates\n",
    "if data_station.duplicated(subset=[\"station_name\"]).any():\n",
    "    problem_stations = checking_duplicates_grdc(data_station)\n",
    "\n",
    "    # Step 3: Get corresponding station IDs\n",
    "    problem_ids = data_station[data_station[\"station_name\"].isin(problem_stations)][\"id\"].unique()\n",
    "\n",
    "    # Step 4: Assign unique station names for problematic IDs only\n",
    "    rename_map = {\n",
    "        id_: f\"{data_station[data_station['id'] == id_]['station_name'].iloc[0]}_{i+1}\"\n",
    "        for i, id_ in enumerate(problem_ids)\n",
    "    }\n",
    "\n",
    "    # Step 5: Apply renaming based on `id` consistently\n",
    "    data_station[\"station_name\"] = data_station.apply(\n",
    "        lambda row: rename_map[row[\"id\"]] if row[\"id\"] in rename_map else row[\"station_name\"],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Create a unique label for each station using name + coordinates (optional)\n",
    "data_station[\"station_label\"] = data_station[\"station_name\"].str.strip() + \" (\" + data_station[\"geo_y\"].round(2).astype(str) + \", \" + data_station[\"geo_x\"].round(2).astype(str) + \")\"\n",
    "\n",
    "# Pivot to wide format: year as index, MultiIndex (month, id) as columns\n",
    "data_station_pivot = data_station.pivot(index=\"year\", columns=[\"station_name\", \"month\"], values=\"Q\")\n",
    "data_station_pivot.dropna(axis=1, how='all', inplace=True)\n",
    "data_station_pivot.sort_index(ascending=True, axis=1, inplace=True)\n",
    "data_station_pivot.to_csv(os.path.join(folder_out, f'grdc_discharge_monthly-m3-s.csv'), index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dd1c17ea795f1b1",
   "metadata": {},
   "source": [
    "## 4. Calculate Runoff Data\n",
    "\n",
    "### Purpose\n",
    "This section converts river discharge measurements (m³/s) into runoff depth (mm/year). Runoff depth normalizes flow by catchment area, allowing for direct comparison between watersheds of different sizes. This is crucial for:\n",
    "- Comparing water yield across different basins\n",
    "- Assessing regional water resources\n",
    "- Evaluating potential for hydropower development\n",
    "- Analyzing climate change impacts on water availability\n",
    "\n",
    "### Conversion Formula\n",
    "Runoff data (mm/year) is calculated from monthly discharge data (m³/s) using the formula:\n",
    "\n",
    "```\n",
    "runoff_mm = (Σ(Q_monthly_avg × 86400 × days_in_month)) / area_m2 × 1000\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Q** is discharge in m³/s (cubic meters per second)\n",
    "- **86400** is seconds per day (60 × 60 × 24)\n",
    "- **days_in_month** accounts for varying month lengths (28-31 days)\n",
    "- **area_m2** is the catchment area in square meters\n",
    "- **1000** converts meters to millimeters\n",
    "\n",
    "### Process\n",
    "1. The code first filters for complete years (with all 12 months of data)\n",
    "2. It then applies the conversion formula to calculate annual runoff for each station\n",
    "3. Results are saved as a CSV file with years as rows and stations as columns\n",
    "\n",
    "### Output Files\n",
    "- **grdc_runoff_mm-year.csv**: Annual runoff depth in mm/year\n",
    "  - This standardized unit allows for comparison across different watersheds\n",
    "  - Typical values range from 100-2000 mm/year depending on climate and geography\n",
    "\n",
    "### Interpretation\n",
    "- Higher runoff values (>1000 mm/year) typically indicate wet regions with high rainfall\n",
    "- Lower values (<300 mm/year) suggest drier conditions or high evapotranspiration\n",
    "- Sudden changes in runoff patterns may indicate:\n",
    "  - Climate shifts\n",
    "  - Land use changes in the catchment\n",
    "  - Dam construction upstream\n",
    "  - Data quality issues\n",
    "\n",
    "### Troubleshooting\n",
    "- If runoff values seem unreasonably high or low, check:\n",
    "  - Catchment area values (common source of error)\n",
    "  - Unit conversion factors\n",
    "  - Completeness of monthly data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "df03fffed797054a",
   "metadata": {},
   "source": [
    "def filter_full_years(df):\n",
    "    \"\"\"\n",
    "    Keep only (station, year) pairs with 12 valid months.\n",
    "    Returns filtered DataFrame and number of dropped rows.\n",
    "    \"\"\"\n",
    "    original_len = len(df)\n",
    "\n",
    "    # Count valid months per station-year\n",
    "    valid_counts = (\n",
    "        df.groupby(['station_name', 'year'])['Q']\n",
    "        .apply(lambda x: x.notna().sum())\n",
    "        .reset_index(name='valid_months')\n",
    "    )\n",
    "\n",
    "    # Only keep those with all 12 months\n",
    "    full_years = valid_counts[valid_counts['valid_months'] == 12]\n",
    "\n",
    "    # Merge to filter original DataFrame\n",
    "    df_filtered = df.merge(full_years[['station_name', 'year']], on=['station_name', 'year'])\n",
    "\n",
    "    removed_rows = original_len - len(df_filtered)\n",
    "    print(f\"Removed {removed_rows} rows — kept {len(df_filtered)} only full (12-month) years.\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def discharge_to_runoff(df):\n",
    "    \"\"\"\n",
    "    Convert monthly discharge (m³/s) into annual runoff (mm/year) at the station level.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Calculates the number of days in each month\n",
    "    2. Converts catchment area from km² to m²\n",
    "    3. Computes monthly water volume (m³) from discharge (m³/s)\n",
    "    4. Sums monthly volumes for each station-year\n",
    "    5. Divides by catchment area to get runoff depth (m)\n",
    "    6. Converts from m to mm by multiplying by 1000\n",
    "\n",
    "    The equation used is:\n",
    "        runoff_mm = (Σ(Q_monthly_avg × 86400 × days_in_month)) / area_m2 × 1000\n",
    "\n",
    "    Where:\n",
    "        - Q is discharge in m³/s\n",
    "        - 86400 is seconds per day\n",
    "        - days_in_month accounts for monthly totals\n",
    "        - area is the catchment area in m²\n",
    "        - 1000 converts meters to millimeters\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing monthly discharge data with columns:\n",
    "            - 'station_name': Name of the gauging station\n",
    "            - 'year': Year of the measurement\n",
    "            - 'time': Datetime of the measurement\n",
    "            - 'Q': Discharge in m³/s\n",
    "            - 'area': Catchment area in km²\n",
    "\n",
    "    Assumes:\n",
    "        - Input DataFrame has one row per station/month\n",
    "        - Years are complete (12 months per station)\n",
    "        - Area is in km²\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with columns:\n",
    "            - 'station_name': Name of the gauging station\n",
    "            - 'year': Year of the measurement\n",
    "            - 'runoff_mm_year': Annual runoff in mm/year\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Add number of days in each month\n",
    "    df['days_in_month'] = pd.to_datetime(df['time']).dt.days_in_month\n",
    "\n",
    "    # Convert area to m²\n",
    "    df['area_m2'] = df['area'] * 1e6\n",
    "\n",
    "    # Compute volume in m³ for each month\n",
    "    df['volume_m3'] = df['Q'] * 86400 * df['days_in_month']\n",
    "\n",
    "    # Sum monthly volumes per station-year\n",
    "    runoff_by_year = (\n",
    "        df.groupby(['station_name', 'year'])\n",
    "        .apply(lambda x: x['volume_m3'].sum() / x['area_m2'].iloc[0] * 1000, include_groups=False)  # m to mm\n",
    "        .reset_index(name='runoff_mm_year')\n",
    "    )\n",
    "\n",
    "    return runoff_by_year"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b0b80b52d97ebea",
   "metadata": {},
   "source": [
    "# Convert discharge data (m3-s) to runoff in (mm-year)\n",
    "data_station_filtered = filter_full_years(data_station)\n",
    "data_runoff = discharge_to_runoff(data_station_filtered)\n",
    "# Save to CSV\n",
    "data_runoff.round(0).pivot(index=\"year\", columns=\"station_name\", values=\"runoff_mm_year\").to_csv(os.path.join(folder_out, f'grdc_runoff_mm-year.csv'), index=True)\n",
    "\n",
    "location = data_station[['station_name', 'geo_x', 'geo_y']].drop_duplicates().reset_index(drop=True)\n",
    "# Merge with runoff_by_year data\n",
    "data_runoff = data_runoff.merge(location, on='station_name')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82d9f6106c05e9dd",
   "metadata": {},
   "source": [
    "## 5. Add Hydropower Plant Data & Associate with River Data\n",
    "\n",
    "### Purpose\n",
    "This section integrates hydropower plant (HPP) data with the streamflow analysis. It links each hydropower plant to the most relevant gauging station, either on the same river or by proximity. This connection is crucial for:\n",
    "- Assessing water availability for existing and planned hydropower plants\n",
    "- Evaluating seasonal flow patterns at hydropower locations\n",
    "- Analyzing long-term trends that might affect energy production\n",
    "- Supporting feasibility studies for new hydropower development\n",
    "\n",
    "### Process\n",
    "1. **Load Hydropower Plant Data**\n",
    "   - Data is loaded from the Global Hydropower Tracker Excel file\n",
    "   - Plants are filtered to include only those in the countries of interest\n",
    "   - Basic validation checks ensure required columns are present\n",
    "\n",
    "2. **Match Hydropower Plants to Rivers**\n",
    "   - River names from HPP data are cleaned and standardized\n",
    "   - Fuzzy matching algorithms find the best matches between HPP rivers and station rivers\n",
    "   - A match score indicates the confidence level of each river name match\n",
    "\n",
    "3. **Associate HPPs with Gauging Stations**\n",
    "   - For HPPs with matched rivers, the station on the same river is preferred\n",
    "   - For unmatched rivers, the nearest station by geographic distance is used\n",
    "   - Each HPP is tagged with its nearest station and whether it's on the same river\n",
    "\n",
    "### Output Data\n",
    "The resulting dataset includes:\n",
    "- All hydropower plants in the selected countries\n",
    "- The nearest gauging station for each plant\n",
    "- Distance to the station (in meters)\n",
    "- Flag indicating whether the plant and station are on the same river\n",
    "\n",
    "### Interpretation\n",
    "- HPPs matched to stations on the same river (same_river = True) provide more reliable flow estimates\n",
    "- For plants with same_river = False, flow patterns should be used with caution as they represent nearby but different watersheds\n",
    "- Distance metrics help assess the reliability of the station-plant association\n",
    "\n",
    "### Considerations\n",
    "- River name matching is challenging due to spelling variations, translations, and naming conventions\n",
    "- The fuzzy matching threshold (currently 85%) can be adjusted to be more or less strict\n",
    "- Geographic proximity doesn't always indicate hydrological similarity, especially in mountainous regions"
   ]
  },
  {
   "cell_type": "code",
   "id": "ade120360afe7c35",
   "metadata": {},
   "source": [
    "path_hpp = os.path.join(folder_in, 'Global-Hydropower-Tracker-April-2025.xlsx')\n",
    "\n",
    "# Improved error handling for hydropower data loading\n",
    "try:\n",
    "    if not os.path.exists(path_hpp):\n",
    "        raise FileNotFoundError(f\"Hydropower data file not found: {path_hpp}\")\n",
    "\n",
    "    data_hpp = pd.read_excel(path_hpp, sheet_name='Data', header=[0], index_col=None)\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_columns = ['Country/Area 1', 'Project Name', 'River / Watercourse', 'Capacity (MW)', 'Status', 'Latitude', 'Longitude']\n",
    "    missing_columns = [col for col in required_columns if col not in data_hpp.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns in hydropower data: {missing_columns}\")\n",
    "\n",
    "    # Filter for countries of interest\n",
    "    data_hpp = data_hpp.loc[data_hpp['Country/Area 1'].isin(countries), :]\n",
    "\n",
    "    print(f\"Loaded {len(data_hpp)} hydropower plants from {len(countries)} countries\")\n",
    "\n",
    "    # Check if we have any data after filtering\n",
    "    if len(data_hpp) == 0:\n",
    "        print(\"Warning: No hydropower plants found for the specified countries\")\n",
    "\n",
    "    display(data_hpp.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading hydropower data: {e}\")\n",
    "    # Create an empty DataFrame with the required columns to avoid breaking the rest of the code\n",
    "    data_hpp = pd.DataFrame(columns=required_columns)\n",
    "    print(\"Created empty hydropower DataFrame to continue processing\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5bfdc5ba323a88f",
   "metadata": {},
   "source": [
    "# Add river name to runoff data\n",
    "meta_df = data_discharge[[\"station_name\", \"river_name\"]].to_dataframe().reset_index()\n",
    "meta_df = meta_df.drop_duplicates(subset=[\"station_name\"])\n",
    "\n",
    "data_station_filtered_river = data_station_filtered.merge(meta_df, on=\"station_name\", how=\"left\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6787093ba0cc8bf7",
   "metadata": {},
   "source": [
    "def clean_river_name(name):\n",
    "    \"\"\"\n",
    "    Clean and standardize river names for better matching.\n",
    "\n",
    "    This function:\n",
    "    1. Handles NaN/None values\n",
    "    2. Normalizes Unicode characters and removes accents\n",
    "    3. Removes common river-related words (RIVER, RIVIERE, etc.)\n",
    "    4. Standardizes spacing and converts to uppercase\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The river name to clean\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and standardized river name\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize unicode and remove accents (e.g. é → e)\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove common river words and clean up spacing\n",
    "    name = re.sub(r\"\\b(RIVER|RIVIERE|RIVIÈRE|R\\.|R)\\b\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip().upper()\n",
    "\n",
    "    return name\n",
    "\n",
    "# Prepare cleaned lists\n",
    "station_rivers_raw = data_station_filtered_river['river_name'].dropna().unique()\n",
    "station_rivers_cleaned = {clean_river_name(r): r for r in station_rivers_raw}\n",
    "\n",
    "matches = []\n",
    "\n",
    "# Iterate over all HPPs (row-wise)\n",
    "for _, row in data_hpp.dropna(subset=[\"River / Watercourse\"]).iterrows():\n",
    "    hpp_river_orig = row[\"River / Watercourse\"]\n",
    "    hpp_river_clean = clean_river_name(hpp_river_orig)\n",
    "\n",
    "    # Find best match among station rivers\n",
    "    match, score, _ = process.extractOne(\n",
    "        hpp_river_clean, station_rivers_cleaned.keys(), scorer=fuzz.token_set_ratio\n",
    "    )\n",
    "\n",
    "    if score >= 85:\n",
    "        matched_station_river = station_rivers_cleaned[match]\n",
    "        stations = data_station_filtered_river[\n",
    "            data_station_filtered_river[\"river_name\"] == matched_station_river\n",
    "        ][\"station_name\"].tolist()\n",
    "    else:\n",
    "        matched_station_river = None\n",
    "        stations = []\n",
    "\n",
    "    matches.append({\n",
    "        \"hpp_name\": row[\"Project Name\"],\n",
    "        \"hpp_river\": hpp_river_orig,\n",
    "        \"matched_river\": matched_station_river,\n",
    "        \"score\": score,\n",
    "        \"stations\": set(stations),\n",
    "        \"hpp_capacity\": row[\"Capacity (MW)\"],\n",
    "        \"hpp_status\": row[\"Status\"]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy display or export\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.to_csv(os.path.join(folder_out, 'hpp_grdc_hydro_matches.csv'), index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3eef254f4db555e3",
   "metadata": {},
   "source": [
    "# Associate hydropower plants with gauging stations based on proximity\n",
    "def associate_hpp_station(data_hpp, data_station, matches_df):\n",
    "    # Step 1: Build GeoDataFrames\n",
    "    gdf_hpp = gpd.GeoDataFrame(\n",
    "        data_hpp.copy(),\n",
    "        geometry=gpd.points_from_xy(data_hpp[\"Longitude\"], data_hpp[\"Latitude\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    gdf_station = gpd.GeoDataFrame(\n",
    "        data_station.copy(),\n",
    "        geometry=gpd.points_from_xy(data_station[\"geo_x\"], data_station[\"geo_y\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Project both to a metric CRS (meters)\n",
    "    gdf_hpp = gdf_hpp.to_crs(epsg=3857)\n",
    "    gdf_station = gdf_station.to_crs(epsg=3857)\n",
    "\n",
    "    # Step 3: Build dictionary from matches_df\n",
    "    station_lookup = {\n",
    "        row[\"hpp_name\"]: row[\"stations\"]\n",
    "        for _, row in matches_df.iterrows()\n",
    "        if row[\"stations\"]\n",
    "    }\n",
    "\n",
    "    # Step 4: For each HPP, associate station\n",
    "    results = []\n",
    "\n",
    "    for idx, hpp in gdf_hpp.iterrows():\n",
    "        hpp_name = hpp[\"Project Name\"]\n",
    "        if hpp_name in station_lookup and station_lookup[hpp_name]:\n",
    "            # Use the first station from the match (same river)\n",
    "            station_name = list(station_lookup[hpp_name])[0]\n",
    "            station = gdf_station[gdf_station[\"station_name\"] == station_name].iloc[0]\n",
    "            distance = hpp.geometry.distance(station.geometry)\n",
    "            results.append({\n",
    "                \"hpp_index\": idx,\n",
    "                \"nearest_station\": station_name,\n",
    "                \"distance_to_station_m\": distance,\n",
    "                \"same_river\": True\n",
    "            })\n",
    "        else:\n",
    "            # Find nearest station by distance\n",
    "            distances = gdf_station.geometry.distance(hpp.geometry)\n",
    "            min_idx = distances.idxmin()\n",
    "            nearest_station = gdf_station.loc[min_idx]\n",
    "            results.append({\n",
    "                \"hpp_index\": idx,\n",
    "                \"nearest_station\": nearest_station[\"station_name\"],\n",
    "                \"distance_to_station_m\": distances[min_idx],\n",
    "                \"same_river\": False\n",
    "            })\n",
    "\n",
    "    # Step 5: Merge back into HPP GeoDataFrame\n",
    "    nearest_df = pd.DataFrame(results)\n",
    "    gdf_hpp[\"nearest_station\"] = nearest_df[\"nearest_station\"].values\n",
    "    gdf_hpp[\"distance_to_station_m\"] = nearest_df[\"distance_to_station_m\"].values\n",
    "    gdf_hpp[\"same_river\"] = nearest_df[\"same_river\"].values\n",
    "\n",
    "    return gdf_hpp\n",
    "\n",
    "gdf_hpp = associate_hpp_station(data_hpp, data_station_filtered, matches_df)\n",
    "data_hpp = data_hpp.merge(\n",
    "    gdf_hpp[[\"Project Name\", \"nearest_station\", \"distance_to_station_m\", 'same_river']],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b6b70daca162c8b",
   "metadata": {},
   "source": [
    "## 6. Visualize All Data on Interactive Maps\n",
    "\n",
    "### Purpose\n",
    "This section creates comprehensive interactive maps that integrate all the processed data: gauging stations, river networks, and hydropower plants. These visualizations serve multiple purposes:\n",
    "- Providing a spatial overview of water resources and infrastructure\n",
    "- Identifying relationships between rivers, stations, and hydropower plants\n",
    "- Enabling exploration of regional patterns in runoff and discharge\n",
    "- Supporting decision-making for water resource management and energy planning\n",
    "\n",
    "### Map Features\n",
    "The interactive map includes several key components:\n",
    "\n",
    "1. **Base Layers**\n",
    "   - Multiple base maps (light, dark, satellite imagery, OpenStreetMap)\n",
    "   - Toggle controls to switch between different background maps\n",
    "\n",
    "2. **River Network Layer**\n",
    "   - Rivers colored by discharge magnitude\n",
    "   - Interactive tooltips showing river names and average discharge\n",
    "   - Filtered to show only significant rivers (stream order ≥ 6)\n",
    "\n",
    "3. **Gauging Station Layer**\n",
    "   - Stations sized by average runoff (larger circles = higher runoff)\n",
    "   - Stations colored by years of available data (more years = brighter colors)\n",
    "   - Popup information showing detailed station metadata and statistics\n",
    "   - Separate layer for stations removed during filtering\n",
    "\n",
    "4. **Hydropower Plant Layer**\n",
    "   - Plants represented as squares sized by capacity (MW)\n",
    "   - Color-coded by status (operating, construction, announced, etc.)\n",
    "   - Detailed popups showing plant information and associated station data\n",
    "   - Information on whether the plant is on the same river as its associated station\n",
    "\n",
    "### Interactive Tools\n",
    "The map includes several interactive features:\n",
    "- Layer controls to toggle visibility of different data elements\n",
    "- Fullscreen mode for detailed exploration\n",
    "- Measurement tool for calculating distances between features\n",
    "- Popups with detailed information on each map element\n",
    "\n",
    "### Output Files\n",
    "- **station_runoff_map.html**: Basic map without river data\n",
    "- **station_runoff_map_with_rivers.html**: Complete map with river network (if river data is available)\n",
    "\n",
    "### Viewing and Sharing\n",
    "- Open the HTML files in any modern web browser\n",
    "- No internet connection required (all data is embedded)\n",
    "- Maps can be shared as standalone files\n",
    "- For large datasets, file sizes may be substantial\n",
    "\n",
    "### Troubleshooting\n",
    "- If rivers don't appear, check that the HydroRIVERS shapefile path is correct\n",
    "- Large river datasets may slow down map rendering\n",
    "- If the map appears blank, try a different web browser\n",
    "- For memory issues, reduce the number of rivers by increasing the stream order threshold"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6536c22fdbe8e78",
   "metadata": {},
   "source": [
    "# Function to clip rivers to the bounding box of runoff stations\n",
    "def clip_rivers_to_stations(rivers_path, df_runoff, ord_stra=6):\n",
    "    # Step 1: Load the HydroRIVERS shapefile\n",
    "\n",
    "    # To be downloaded from: https://www.hydrosheds.org/products/hydrorivers using shapefile format\n",
    "    # Load available layers (should return e.g. 'HydroRIVERS_v10_af')\n",
    "\n",
    "    if not os.path.exists(rivers_path):\n",
    "        raise FileNotFoundError(f\"Geodatabase not found: {rivers_path}\")\n",
    "\n",
    "    # Load rivers\n",
    "    rivers_gdf = gpd.read_file(rivers_path)\n",
    "\n",
    "    # Filter based on average discharge and stream order\n",
    "    rivers_gdf = rivers_gdf[(rivers_gdf[\"DIS_AV_CMS\"] > 50) & (rivers_gdf[\"ORD_STRA\"] >= 6) & (rivers_gdf[\"ORD_FLOW\"] <= 5)]\n",
    "    #rivers_gdf = rivers_gdf[(rivers_gdf[\"ORD_STRA\"] >= ord_stra)]\n",
    "\n",
    "    # Step 2: Prepare the bounding box for clipping\n",
    "    # Create GeoDataFrame from stations\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        df_runoff,\n",
    "        geometry=gpd.points_from_xy(df_runoff[\"geo_x\"], df_runoff[\"geo_y\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Get bounding box coordinates from the stations\n",
    "    roi_bounds = stations_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Create a polygon box from the bounds\n",
    "    roi_polygon = box(*roi_bounds).buffer(1.0)  # optional padding of 1 degree\n",
    "\n",
    "    # Create a GeoDataFrame for clipping\n",
    "    roi_gdf = gpd.GeoDataFrame(geometry=[roi_polygon], crs=\"EPSG:4326\")\n",
    "\n",
    "    rivers_clipped = gpd.clip(rivers_gdf, roi_gdf)\n",
    "\n",
    "    return rivers_clipped\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90fa62608c26fb54",
   "metadata": {},
   "source": [
    "# This cell can take a while to run (some minutes), depending on the number of stations and rivers\n",
    "def make_maps(df_runoff, data_hpp=None, rivers_path=None, folder_out=folder_out):\n",
    "    \"\"\"\n",
    "    Create interactive maps showing runoff stations, rivers, and hydropower plants.\n",
    "\n",
    "    This function generates a Folium map with multiple layers:\n",
    "    1. River network colored by discharge (if rivers_path is provided)\n",
    "    2. Gauging stations with size based on runoff and color based on years of data\n",
    "    3. Hydropower plants with size based on capacity and color based on status\n",
    "\n",
    "    Parameters:\n",
    "        df_runoff (DataFrame): DataFrame containing runoff data with station coordinates\n",
    "        data_hpp (DataFrame, optional): DataFrame containing hydropower plant data\n",
    "        rivers_path (str, optional): Path to the HydroRIVERS shapefile\n",
    "        folder_out (str): Output folder for saving the map\n",
    "\n",
    "    Returns:\n",
    "        folium.Map: The interactive map object\n",
    "    \"\"\"\n",
    "    # Read station data\n",
    "    agg = (\n",
    "        df_runoff.groupby([\"station_name\", \"geo_x\", \"geo_y\"])\n",
    "          .agg(\n",
    "              avg_runoff=(\"runoff_mm_year\", \"mean\"),\n",
    "              n_years=(\"year\", \"count\")\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create base map centered on the region\n",
    "    center_lat = agg[\"geo_y\"].mean()\n",
    "    center_lon = agg[\"geo_x\"].mean()\n",
    "\n",
    "    # Create map with improved base layer options\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon], \n",
    "        zoom_start=5, \n",
    "        tiles=\"CartoDB positron\"\n",
    "    )\n",
    "\n",
    "    # Add additional base layers for better visualization options\n",
    "    folium.TileLayer('CartoDB dark_matter', name='Dark Map').add_to(m)\n",
    "    folium.TileLayer('OpenStreetMap', name='OpenStreetMap').add_to(m)\n",
    "    folium.TileLayer(\n",
    "        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr='Esri',\n",
    "        name='Satellite'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add rivers from HydroRIVERS\n",
    "    name = f'station_runoff_map.html'\n",
    "    if rivers_path is not None:\n",
    "        try:\n",
    "            rivers_clipped = clip_rivers_to_stations(rivers_path, df_runoff)\n",
    "\n",
    "            # Create a feature group for rivers to allow toggling\n",
    "            rivers_group = folium.FeatureGroup(name=\"Rivers\", show=True)\n",
    "\n",
    "            colormap = plt.cm.viridis\n",
    "            norm = mcolors.Normalize(\n",
    "                vmin=rivers_clipped[\"DIS_AV_CMS\"].min(),\n",
    "                vmax=rivers_clipped[\"DIS_AV_CMS\"].max())\n",
    "\n",
    "            # Add rivers to the map with color based on discharge\n",
    "            for _, row in rivers_clipped.iterrows():\n",
    "                discharge = row[\"DIS_AV_CMS\"]\n",
    "                color = mcolors.to_hex(colormap(norm(discharge)))\n",
    "\n",
    "                folium.GeoJson(\n",
    "                    row[\"geometry\"],\n",
    "                    style_function=lambda feature, color=color: {\n",
    "                        \"color\": color,\n",
    "                        \"weight\": 2,\n",
    "                        \"opacity\": 0.8\n",
    "                    },\n",
    "                    tooltip=f\"River: {row.get('RIVER_NAME', 'Unknown')}<br>Discharge: {discharge:.1f} m³/s\"\n",
    "                ).add_to(rivers_group)\n",
    "\n",
    "            # Add the rivers group to the map\n",
    "            rivers_group.add_to(m)\n",
    "\n",
    "            # Define vmin and vmax\n",
    "            vmin = rivers_clipped[\"DIS_AV_CMS\"].min()\n",
    "            vmax = rivers_clipped[\"DIS_AV_CMS\"].max()\n",
    "\n",
    "            ticks_raw = np.linspace(vmin, vmax, 5)\n",
    "            ticks = [round(t, -int(np.floor(np.log10(t))) + 1) for t in ticks_raw]  # e.g., 72 → 70, 1502 → 1500\n",
    "\n",
    "            norm = mcolors.Normalize(vmin=min(ticks), vmax=max(ticks))\n",
    "            colors = [mcolors.to_hex(plt.cm.viridis(norm(t))) for t in ticks]\n",
    "\n",
    "            # Add high-contrast legend\n",
    "            discharge_colormap = LinearColormap(\n",
    "                colors=colors,\n",
    "                index=ticks,\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                caption=\"Avg River Discharge (m³/s)\"\n",
    "            )\n",
    "            discharge_colormap.add_to(m)\n",
    "\n",
    "            name = f'station_runoff_map_with_rivers.html'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading river data: {e}\")\n",
    "            print(\"Continuing without river data\")\n",
    "\n",
    "    # Create a feature group for active stations\n",
    "    stations_group = folium.FeatureGroup(name=\"Gauging Stations\", show=True)\n",
    "\n",
    "    # Include station\n",
    "    if agg is not None:\n",
    "        norm_years = LogNorm(vmin=agg[\"n_years\"].min(), vmax=agg[\"n_years\"].max())\n",
    "        colormap = plt.cm.viridis  # or any other colormap\n",
    "\n",
    "        for _, row in agg.iterrows():\n",
    "            # Normalize color from colormap\n",
    "            rgba = colormap(norm_years(row[\"n_years\"]))\n",
    "            hex_color = mcolors.to_hex(rgba)\n",
    "\n",
    "            # Scale size (radius) based on average runoff\n",
    "            radius = max(4, (row[\"avg_runoff\"] / agg[\"avg_runoff\"].max()) * 20)\n",
    "\n",
    "            # Add circle marker with enhanced popup\n",
    "            popup_content = f\"\"\"\n",
    "            <div style=\"font-family: Arial; min-width: 200px;\">\n",
    "                <h4 style=\"margin-bottom: 10px;\">{row['station_name']}</h4>\n",
    "                <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Avg Runoff:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['avg_runoff']:.0f} mm/year</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Years of Data:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['n_years']}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Coordinates:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['geo_y']:.4f}, {row['geo_x']:.4f}</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                radius=radius,\n",
    "                color=hex_color,\n",
    "                fill=True,\n",
    "                fill_opacity=0.8,\n",
    "                popup=folium.Popup(popup_content, max_width=300)\n",
    "            ).add_to(stations_group)\n",
    "\n",
    "        # Add stations group to map\n",
    "        stations_group.add_to(m)\n",
    "\n",
    "        # FeatureGroup for removed stations\n",
    "        removed_stations_group = folium.FeatureGroup(name=\"Removed Stations\", show=False)\n",
    "\n",
    "        # Plot removed due to missing area (red X)\n",
    "        if 'stations_removed_area_df' in globals() and stations_removed_area_df is not None and not stations_removed_area_df.empty:\n",
    "            for _, row in stations_removed_area_df.iterrows():\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                    number_of_sides=4,\n",
    "                    radius=5,\n",
    "                    rotation=45,\n",
    "                    color=\"red\",\n",
    "                    fill=True,\n",
    "                    fill_color=\"red\",\n",
    "                    fill_opacity=1.0,\n",
    "                    popup=f\"<b>{row['station_name']}</b><br>Removed: Area ≤ 0\"\n",
    "                ).add_to(removed_stations_group)\n",
    "\n",
    "        # Plot removed due to low flow (orange X)\n",
    "        if 'stations_removed_lowflow_df' in globals() and stations_removed_lowflow_df is not None and not stations_removed_lowflow_df.empty:\n",
    "            for _, row in stations_removed_lowflow_df.iterrows():\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                    number_of_sides=4,\n",
    "                    radius=5,\n",
    "                    rotation=45,\n",
    "                    color=\"orange\",\n",
    "                    fill=True,\n",
    "                    fill_color=\"orange\",\n",
    "                    fill_opacity=1.0,\n",
    "                    popup=f\"<b>{row['station_name']}</b><br>Removed: Flow < threshold\"\n",
    "                ).add_to(removed_stations_group)\n",
    "\n",
    "        # Add group to map\n",
    "        removed_stations_group.add_to(m)\n",
    "\n",
    "        # Create a combined legend for stations color and size\n",
    "        years_vals = np.percentile(agg[\"n_years\"], [0, 50, 100]).round(0).astype(int)\n",
    "        runoff_vals = np.percentile(agg[\"avg_runoff\"], [0, 50, 100]).round(0)\n",
    "\n",
    "        combined_legend_html = f'''\n",
    "        <div style=\"\n",
    "            position: fixed;\n",
    "            bottom: 50px; left: 50px; width: 260px;\n",
    "            background-color: white;\n",
    "            border: 2px solid grey;\n",
    "            z-index: 9999;\n",
    "            font-size: 13px;\n",
    "            padding: 12px;\">\n",
    "\n",
    "            <b style=\"font-size:14px;\">Gauging Stations</b><br><br>\n",
    "\n",
    "            <b>Years of Available Data (Color)</b><br>\n",
    "            <i style=\"background: #440154; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[0]}<br>\n",
    "            <i style=\"background: #21918c; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[1]}<br>\n",
    "            <i style=\"background: #fde725; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[2]}<br><br>\n",
    "\n",
    "            <b>Avg Runoff (mm/year)</b><br>\n",
    "            <svg width=\"120\" height=\"90\">\n",
    "              <circle cx=\"30\" cy=\"20\" r=\"6\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"25\" font-size=\"12\">{int(runoff_vals[0])}</text>\n",
    "              <circle cx=\"30\" cy=\"45\" r=\"10\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"50\" font-size=\"12\">{int(runoff_vals[1])}</text>\n",
    "              <circle cx=\"30\" cy=\"75\" r=\"14\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"80\" font-size=\"12\">{int(runoff_vals[2])}</text>\n",
    "            </svg>\n",
    "\n",
    "            <b>Removed Stations</b><br>\n",
    "            <i class=\"fa fa-times\" style=\"color:red; margin-right: 6px;\"></i> Area missing<br>\n",
    "            <i class=\"fa fa-times\" style=\"color:orange; margin-right: 6px;\"></i> Low flow\n",
    "        </div>\n",
    "        '''\n",
    "\n",
    "        # Add legend to the map\n",
    "        m.get_root().html.add_child(folium.Element(combined_legend_html))\n",
    "\n",
    "    # Include hydropower plants\n",
    "    if data_hpp is not None and not data_hpp.empty:\n",
    "        # Create a feature group for hydropower plants\n",
    "        hpp_group = folium.FeatureGroup(name=\"Hydropower Plants\", show=True)\n",
    "\n",
    "        # Define fixed status colors\n",
    "        custom_status_colors = {\n",
    "            'operating': '#2ca02c',           # green\n",
    "            'construction': '#ff7f0e',        # orange\n",
    "            'announced': '#999999',           # grey\n",
    "            'pre-construction': '#9467bd'     # purple\n",
    "        }\n",
    "\n",
    "        # Filter data_hpp for selected statuses only\n",
    "        valid_statuses = list(custom_status_colors.keys())\n",
    "        data_hpp_filtered = data_hpp[data_hpp['Status'].isin(valid_statuses)]\n",
    "\n",
    "        if not data_hpp_filtered.empty:\n",
    "            # Normalize capacity for marker sizing\n",
    "            cap_min = data_hpp_filtered['Capacity (MW)'].min()\n",
    "            cap_max = data_hpp_filtered['Capacity (MW)'].max()\n",
    "\n",
    "            def normalize_capacity(value):\n",
    "                return 6 + ((value - cap_min) / (cap_max - cap_min + 1e-6)) * 10  # radius 6-16\n",
    "\n",
    "            # Convert RGBA to HEX for Folium\n",
    "            status_colors = {k: mcolors.to_hex(v) for k, v in custom_status_colors.items()}\n",
    "\n",
    "            # Add each HPP as a square marker with enhanced popup\n",
    "            for _, row in data_hpp_filtered.iterrows():\n",
    "                # Create a more structured popup with selected fields\n",
    "                selected_fields = [\n",
    "                    'Project Name', 'Status', 'Capacity (MW)', \n",
    "                    'River / Watercourse', 'Country/Area 1'\n",
    "                ]\n",
    "\n",
    "                popup_content = f\"\"\"\n",
    "                <div style=\"font-family: Arial; min-width: 250px;\">\n",
    "                    <h4 style=\"margin-bottom: 10px;\">{row['Project Name']}</h4>\n",
    "                    <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                \"\"\"\n",
    "\n",
    "                for field in selected_fields:\n",
    "                    if field in row and field != 'Project Name':\n",
    "                        value = row[field]\n",
    "                        popup_content += f\"\"\"\n",
    "                        <tr>\n",
    "                            <td style=\"padding: 3px;\"><b>{field}:</b></td>\n",
    "                            <td style=\"padding: 3px;\">{value}</td>\n",
    "                        </tr>\n",
    "                        \"\"\"\n",
    "\n",
    "                # Add nearest station info if available\n",
    "                if 'nearest_station' in row and 'distance_to_station_m' in row:\n",
    "                    distance_km = row['distance_to_station_m'] / 1000\n",
    "                    same_river = \"Yes\" if row.get('same_river', False) else \"No\"\n",
    "                    popup_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Nearest Station:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['nearest_station']}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Distance:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{distance_km:.1f} km</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Same River:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{same_river}</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "\n",
    "                popup_content += \"\"\"\n",
    "                    </table>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "                color = status_colors.get(row['Status'], '#555')\n",
    "                size = normalize_capacity(row['Capacity (MW)'])\n",
    "\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row['Latitude'], row['Longitude']],\n",
    "                    number_of_sides=4,  # square\n",
    "                    radius=size,\n",
    "                    color=None,\n",
    "                    fill=True,\n",
    "                    fill_color=color,\n",
    "                    fill_opacity=0.9,\n",
    "                    popup=folium.Popup(popup_content, max_width=300)\n",
    "                ).add_to(hpp_group)\n",
    "\n",
    "            # Add the HPP group to the map\n",
    "            hpp_group.add_to(m)\n",
    "\n",
    "            # Prepare capacity values for size scale legend\n",
    "            capacity_vals = np.percentile(data_hpp_filtered['Capacity (MW)'], [0, 50, 100]).round(0).astype(int)\n",
    "            circle_sizes = [6, 10, 14]  # Match values used in marker radius scaling\n",
    "\n",
    "            # Create status color legend HTML\n",
    "            status_legend_items = \"\"\n",
    "            for status in valid_statuses:\n",
    "                hex_color = custom_status_colors[status]\n",
    "                status_legend_items += f'''\n",
    "                    <i style=\"background:{hex_color}; width: 18px; height: 18px;\n",
    "                    float: left; margin-right: 6px;\"></i> {status}<br>\n",
    "                '''\n",
    "\n",
    "            # Full HTML block\n",
    "            hpp_legend_html = f'''\n",
    "            <div style=\"\n",
    "                position: fixed;\n",
    "                bottom: 50px; left: 340px; width: 280px;\n",
    "                background-color: white;\n",
    "                border: 2px solid grey;\n",
    "                z-index: 9999;\n",
    "                font-size: 13px;\n",
    "                padding: 12px;\">\n",
    "\n",
    "                <b style=\"font-size:14px;\">Hydropower Plants</b><br><br>\n",
    "\n",
    "                <b>Status (Color)</b><br>\n",
    "                {status_legend_items}\n",
    "                <br>\n",
    "\n",
    "                <b>Capacity (MW)</b><br>\n",
    "                <svg width=\"160\" height=\"100\">\n",
    "                  <rect x=\"20\" y=\"10\" width=\"{circle_sizes[0]*2}\" height=\"{circle_sizes[0]*2}\" fill=\"#999\"/>\n",
    "                  <text x=\"60\" y=\"25\" font-size=\"12\">{capacity_vals[0]}</text>\n",
    "                  <rect x=\"20\" y=\"40\" width=\"{circle_sizes[1]*2}\" height=\"{circle_sizes[1]*2}\" fill=\"#999\"/>\n",
    "                  <text x=\"60\" y=\"55\" font-size=\"12\">{capacity_vals[1]}</text>\n",
    "                  <rect x=\"20\" y=\"70\" width=\"{circle_sizes[2]*2}\" height=\"{circle_sizes[2]*2}\" fill=\"#999\"/>\n",
    "                  <text x=\"60\" y=\"85\" font-size=\"12\">{capacity_vals[2]}</text>\n",
    "                </svg>\n",
    "            </div>\n",
    "            '''\n",
    "            m.get_root().html.add_child(folium.Element(hpp_legend_html))\n",
    "\n",
    "    # Add layer control to toggle different map elements\n",
    "    folium.LayerControl().add_to(m)\n",
    "\n",
    "    # Add fullscreen button for better user experience\n",
    "    folium.plugins.Fullscreen().add_to(m)\n",
    "\n",
    "    # Add measure tool for distance measurements\n",
    "    folium.plugins.MeasureControl(position='topleft', primary_length_unit='kilometers').add_to(m)\n",
    "\n",
    "    # Save the map\n",
    "    try:\n",
    "        m.save(os.path.join(folder_out, name))\n",
    "        print(f\"Map saved to {os.path.join(folder_out, name)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving map: {e}\")\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# Add info\n",
    "\n",
    "river_path = os.path.join(folder_in, 'river_input', 'HydroRIVERS_v10_af_shp', 'HydroRIVERS_v10_af.shp')\n",
    "make_maps(data_runoff, data_hpp=data_hpp, rivers_path=river_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3309cd3489781576",
   "metadata": {},
   "source": [
    "## 7. Plot Station Runoff for Each Hydropower Plant\n",
    "\n",
    "### Purpose\n",
    "This section creates visualizations that show the historical runoff patterns for gauging stations associated with hydropower plants. These plots are essential for:\n",
    "- Understanding long-term water availability at hydropower locations\n",
    "- Identifying trends and variability in water resources\n",
    "- Assessing potential risks to hydropower generation\n",
    "- Supporting operational planning and investment decisions\n",
    "\n",
    "### Visualization Types\n",
    "\n",
    "#### 1. Annual Runoff Evolution\n",
    "The first plot (`plot_runoff_evolution`) shows how annual runoff has changed over time for each station associated with a hydropower plant:\n",
    "- Each hydropower plant's nearest station gets its own subplot\n",
    "- X-axis represents years\n",
    "- Y-axis shows runoff in mm/year\n",
    "- Title indicates both station name and associated hydropower project\n",
    "- Labels indicate whether the station is on the same river as the plant\n",
    "\n",
    "#### 2. Monthly Discharge Patterns\n",
    "The second plot (`plot_discharge_month`) displays the average monthly discharge pattern for each station:\n",
    "- Shows the seasonal flow pattern throughout the year\n",
    "- Helps identify high and low flow seasons\n",
    "- Critical for understanding hydropower generation potential throughout the year\n",
    "- Useful for operational planning and maintenance scheduling\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "**For Annual Runoff Plots:**\n",
    "- Look for long-term trends (increasing, decreasing, or stable patterns)\n",
    "- Note any abrupt changes that might indicate upstream development or climate shifts\n",
    "- Compare patterns across different stations to identify regional trends\n",
    "- Pay special attention to recent years for current conditions\n",
    "\n",
    "**For Monthly Discharge Plots:**\n",
    "- Identify the high and low flow seasons\n",
    "- Note the magnitude of seasonal variation (highly seasonal vs. relatively stable)\n",
    "- Consider how the seasonal pattern aligns with energy demand patterns\n",
    "- For plants with \"Same River = False\", interpret with caution as patterns may differ\n",
    "\n",
    "### Output Files\n",
    "- **runoff_evolution_hpp.png**: Time series of annual runoff for each hydropower-associated station\n",
    "- **runoff_monthly_evolution_hpp.png**: Average monthly discharge patterns for each station\n",
    "\n",
    "### Customization Options\n",
    "- Adjust the figure size by modifying the `figsize` parameter\n",
    "- Change the number of columns in the subplot grid by adjusting `ncols`\n",
    "- Modify the output file names by changing the parameters to `folder_out`"
   ]
  },
  {
   "cell_type": "code",
   "id": "6517211419d7211",
   "metadata": {},
   "source": [
    "stations = data_hpp['nearest_station'].unique()\n",
    "# Filter data_runoff to include only stations that have hydropower plants associated\n",
    "temp = data_runoff[data_runoff['station_name'].isin(stations)].copy()\n",
    "# Subplots for each station and evolution of"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7d4577ad21f9b34",
   "metadata": {},
   "source": [
    "def plot_runoff_evolution(data_runoff, data_hpp, folder_out=None):\n",
    "    \"\"\"\n",
    "    Plot the evolution of runoff for each hydropower plant's nearest station.\n",
    "    This function creates subplots for each station associated with hydropower plants,\n",
    "    showing the annual runoff evolution over the years.\n",
    "\n",
    "    Parameters:\n",
    "        data_runoff (DataFrame): DataFrame containing runoff data with station coordinates.\n",
    "        data_hpp (DataFrame): DataFrame containing hydropower plant data with nearest stations.\n",
    "        folder_out (str, optional): Output folder to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    stations = data_hpp['nearest_station'].unique()\n",
    "    temp = data_runoff[data_runoff['station_name'].isin(stations)].copy()\n",
    "\n",
    "    # Map station_name to project name and river match info\n",
    "    station_to_project = data_hpp.set_index('nearest_station')['Project Name'].to_dict()\n",
    "    station_to_river_match = data_hpp.set_index('nearest_station')['same_river'].to_dict()\n",
    "\n",
    "    temp['Project Name'] = temp['station_name'].map(station_to_project)\n",
    "    temp['same_river'] = temp['station_name'].map(station_to_river_match)\n",
    "\n",
    "    n_stations = len(stations)\n",
    "    ncols = 3\n",
    "    nrows = (n_stations + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), sharey=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    min_year = temp['year'].min()\n",
    "    max_year = temp['year'].max()\n",
    "\n",
    "    for i, station in enumerate(stations):\n",
    "        ax = axs[i]\n",
    "        subset = temp[temp['station_name'] == station]\n",
    "        ax.plot(subset['year'].astype(int), subset['runoff_mm_year'], marker='o', linestyle='-')\n",
    "        match_label = \"Same River\" if station_to_river_match[station] else \"Closest Station\"\n",
    "        ax.set_title(f\"{station} / {station_to_project[station]} ({match_label})\", fontsize=10)\n",
    "        ax.set_ylabel(\"Runoff (mm/year)\")\n",
    "        ax.set_xlim(min_year, max_year)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if folder_out:\n",
    "        plt.savefig(os.path.join(folder_out, 'runoff_evolution_hpp.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_runoff_evolution(data_runoff, data_hpp, folder_out=folder_out)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60ed38604ed1657b",
   "metadata": {},
   "source": [
    "def plot_discharge_month(data_station_filtered, data_hpp, folder_out=None):\n",
    "    \"\"\"\n",
    "    Plot monthly discharge evolution for each hydropower plant's nearest station.\n",
    "    This function creates subplots for each station associated with hydropower plants,\n",
    "    showing the average monthly discharge over the years.\n",
    "\n",
    "    Parameters:\n",
    "        data_station_filtered (DataFrame): DataFrame containing station discharge data.\n",
    "        data_hpp (DataFrame): DataFrame containing hydropower plant data with nearest stations.\n",
    "        folder_out (str, optional): Output folder to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    stations = data_hpp['nearest_station'].unique()\n",
    "    temp = data_station_filtered[data_station_filtered['station_name'].isin(stations)].copy()\n",
    "\n",
    "    # Map station_name to project name and river match info\n",
    "    station_to_project = data_hpp.set_index('nearest_station')['Project Name'].to_dict()\n",
    "    station_to_river_match = data_hpp.set_index('nearest_station')['same_river'].to_dict()\n",
    "\n",
    "    temp['Project Name'] = temp['station_name'].map(station_to_project)\n",
    "    temp['same_river'] = temp['station_name'].map(station_to_river_match)\n",
    "\n",
    "    n_stations = len(stations)\n",
    "    ncols = 3\n",
    "    nrows = (n_stations + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), sharey=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, station in enumerate(stations):\n",
    "        ax = axs[i]\n",
    "        subset = temp[temp['station_name'] == station]\n",
    "        subset = subset.groupby(['Project Name', 'month', 'same_river'])['Q'].mean().reset_index()\n",
    "        subset['month_abbr'] = subset['month'].astype(int).apply(lambda x: calendar.month_abbr[x])\n",
    "        ax.plot(subset['month_abbr'], subset['Q'], marker='o', linestyle='-')\n",
    "        match_label = \"Same River\" if station_to_river_match[station] else \"Closest Station\"\n",
    "        ax.set_title(f\"{station} / {station_to_project[station]} ({match_label})\", fontsize=10)\n",
    "        ax.set_ylabel(\"Discharge (m3/s)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if folder_out:\n",
    "        plt.savefig(os.path.join(folder_out, 'runoff_monthly_evolution_hpp.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "plot_discharge_month(data_station_filtered, data_hpp, folder_out=folder_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d75611ccde26c67b",
   "metadata": {},
   "source": [
    "## Filling Missing Values in Time Series Data\n",
    "\n",
    "### Purpose\n",
    "This experimental section demonstrates methods for handling missing values in hydrological time series data. Missing data is a common challenge in hydrology due to equipment failures, maintenance periods, or data transmission issues. This section explores two approaches:\n",
    "\n",
    "1. **Climatology-Based Filling**: Replacing missing values with long-term monthly averages\n",
    "2. **Interpolation-Based Filling**: Using linear interpolation to estimate missing values\n",
    "\n",
    "### Why This Matters\n",
    "Complete time series are essential for:\n",
    "- Reliable trend analysis\n",
    "- Accurate seasonal pattern identification\n",
    "- Input to hydrological and energy models\n",
    "- Consistent comparison between stations and time periods\n",
    "\n",
    "### Methodology\n",
    "\n",
    "#### Climatology-Based Filling (`fill_missing_climatology`)\n",
    "- Calculates the long-term average for each month at each station\n",
    "- Replaces missing values with these monthly averages\n",
    "- Preserves the seasonal pattern but loses inter-annual variability\n",
    "- Works well for stations with strong seasonal patterns\n",
    "- Requires at least 3 months of data per year to be considered valid\n",
    "\n",
    "#### Interpolation-Based Filling (`drop_sparse_years_and_interpolate`)\n",
    "- Removes years with too many missing months (less than 9 valid months)\n",
    "- Uses linear interpolation to fill gaps in the remaining time series\n",
    "- Better preserves trends and inter-annual patterns\n",
    "- Works well for short gaps but may create unrealistic values for longer gaps\n",
    "- More sensitive to outliers than the climatology method\n",
    "\n",
    "### Visualization and Comparison\n",
    "The `plot_all_fill_methods` function creates comparison plots showing:\n",
    "- Original data with gaps\n",
    "- Climatology-filled data\n",
    "- Interpolation-filled data\n",
    "\n",
    "These plots help assess which method is more appropriate for each station based on the pattern and extent of missing data.\n",
    "\n",
    "### Interpretation Guidelines\n",
    "- **For climatology-filled data**: Good for preserving seasonal patterns but may mask long-term trends\n",
    "- **For interpolated data**: Better for trend analysis but may create unrealistic values for long gaps\n",
    "- **When comparing methods**: Look for divergence between methods, which indicates higher uncertainty\n",
    "\n",
    "### Status: Experimental\n",
    "This section is marked as \"in progress\" because:\n",
    "- The methods are still being refined\n",
    "- Parameter values (min_months) may need adjustment for specific applications\n",
    "- Additional methods (e.g., ARIMA models, machine learning approaches) could be added\n",
    "- Validation against known values has not been fully implemented\n",
    "\n",
    "### Next Steps for Development\n",
    "- Add validation metrics to quantify filling accuracy\n",
    "- Implement more sophisticated filling methods\n",
    "- Add uncertainty estimates for filled values\n",
    "- Create diagnostic plots to help select the best method for each station"
   ]
  },
  {
   "cell_type": "code",
   "id": "57011395a5bcb5b",
   "metadata": {},
   "source": [
    "def fill_missing_climatology(df, min_months=3):\n",
    "    \"\"\"\n",
    "    Fill missing values using monthly climatology (station-wise).\n",
    "    Returns a filled DataFrame and a mask of filled values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Filter out sparse years\n",
    "    valid_years = (\n",
    "        df.groupby([\"station_name\", \"year\"])[\"Q\"]\n",
    "        .apply(lambda x: x.notna().sum() >= min_months)\n",
    "        .reset_index(name=\"keep\")\n",
    "    )\n",
    "\n",
    "    # Merge to filter out sparse rows\n",
    "    df = df.merge(valid_years[valid_years[\"keep\"]], on=[\"station_name\", \"year\"])\n",
    "    df.drop(columns=\"keep\", inplace=True)\n",
    "\n",
    "    # Step 2: Build climatology\n",
    "    climatology = (\n",
    "        df.groupby([\"station_name\", \"month\"])[\"Q\"]\n",
    "        .mean()\n",
    "        .rename(\"Q_clim\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df = df.merge(climatology, on=[\"station_name\", \"month\"], how=\"left\")\n",
    "\n",
    "    # Step 3: Fill missing with climatology\n",
    "    fill_mask = df[\"Q\"].isna()\n",
    "    df.loc[fill_mask, \"Q\"] = df.loc[fill_mask, \"Q_clim\"]\n",
    "\n",
    "    df.drop(columns=\"Q_clim\", inplace=True)\n",
    "\n",
    "    return df, fill_mask\n",
    "\n",
    "def drop_sparse_years_and_interpolate(df, min_months=9):\n",
    "    \"\"\"\n",
    "    Drops years with too much missing data and interpolates gaps per station.\n",
    "    Assumes monthly data. Returns cleaned & interpolated DataFrame.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Count non-NaN entries per year-station\n",
    "    valid_counts = (\n",
    "        df_clean.groupby([\"station_name\", \"year\"])[\"Q\"]\n",
    "        .apply(lambda x: x.notna().sum())\n",
    "        .rename(\"valid_months\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep only rows with enough months\n",
    "    valid_years = valid_counts[valid_counts[\"valid_months\"] >= min_months]\n",
    "    df_clean = df_clean.merge(valid_years[[\"station_name\", \"year\"]], on=[\"station_name\", \"year\"])\n",
    "\n",
    "    # Sort for interpolation\n",
    "    df_clean = df_clean.sort_values([\"station_name\", \"year\", \"month\"])\n",
    "\n",
    "    # Interpolate per station\n",
    "    df_clean[\"Q\"] = df_clean.groupby(\"station_name\")[\"Q\"].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def plot_all_fill_methods(df_original, df_clim, df_interp, output_dir):\n",
    "    \"\"\"\n",
    "    Plots original, climatology-filled, and interpolated runoff data per station in one plot.\n",
    "\n",
    "    Assumes all DataFrames have columns: ['year', 'month', 'station_name', 'Q'].\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_dir = os.path.join(output_dir, 'stations')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Combine 'year' and 'month' into datetime\n",
    "    def add_datetime(df):\n",
    "        return df.assign(\n",
    "            date=pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2))\n",
    "        )\n",
    "\n",
    "    df_original = add_datetime(df_original)\n",
    "    df_clim = add_datetime(df_clim)\n",
    "    df_interp = add_datetime(df_interp)\n",
    "\n",
    "    # Loop over each station\n",
    "    stations = df_original['station_name'].unique()\n",
    "\n",
    "    for station in stations:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "        # Subsets for current station\n",
    "        df_o = df_original[df_original['station_name'] == station]\n",
    "        df_c = df_clim[df_clim['station_name'] == station]\n",
    "        df_i = df_interp[df_interp['station_name'] == station]\n",
    "\n",
    "        # Plot all\n",
    "        ax.plot(df_o['date'], df_o['Q'], label=\"Original\", alpha=0.5, marker='o', linestyle='-', color='black')\n",
    "        ax.plot(df_c['date'], df_c['Q'], label=\"Climatology Fill\", linestyle='--', color='orange')\n",
    "        ax.plot(df_i['date'], df_i['Q'], label=\"Interpolated\", linestyle='-', color='blue')\n",
    "\n",
    "        ax.set_title(f\"Station: {station}\")\n",
    "        ax.set_ylabel(\"Discharge / Runoff (Q)\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{station}.png\"))\n",
    "        plt.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52e919231d3dacdf",
   "metadata": {},
   "source": [
    "df_filled, filled_mask = fill_missing_climatology(df_filtered, min_months=6)\n",
    "df_interpolated = drop_sparse_years_and_interpolate(df_filtered, min_months=9)\n",
    "plot_all_fill_methods(df_filtered, df_filled, df_interpolated, output_dir=folder_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ec6989afbb929d1",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatic_env",
   "language": "python",
   "name": "climatic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
