{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRDC Hydro Data Processing\n",
    "\n",
    "**Objective**\n",
    "Transform raw GRDC discharge records, river shapefiles, and hydropower tracker data into QA-ready runoff indicators, spatial maps, and plots for Central Africa.\n",
    "\n",
    "**Data Requirements & Methods**\n",
    "- Populate `pre-analysis/open-data/input/` with GRDC NetCDF exports (`GRDC-Monthly.nc`), `stationbasins.geojson`, hydropower tracker spreadsheets, and river shapefiles.\n",
    "- Ensure `xarray`, `geopandas`, `folium`, `pandas`, `matplotlib`, `rapidfuzz`, and supporting scientific libraries are installed.\n",
    "- The notebook stitches multiple NetCDFs, cleans/filters the time series, joins hydropower metadata, and publishes both interactive maps and static analyses.\n",
    "\n",
    "**Overview of Steps**\n",
    "1. Step 1 - Configure user inputs and folder paths.\n",
    "2. Step 2 - Load and merge GRDC streamflow datasets.\n",
    "3. Step 3 - Apply quality-control filters to retain robust stations.\n",
    "4. Step 4 - Reformat and aggregate runoff metrics.\n",
    "5. Step 5 - Calculate runoff indicators used by EPM.\n",
    "6. Step 6 - Associate hydropower assets with nearby hydrology data.\n",
    "7. Step 7 - Visualize stations and runoff on interactive maps.\n",
    "8. Step 8 - Plot station-level runoff diagnostics for hydropower assets.\n",
    "9. Step 9 - Experiment with climatology/interpolation gap-filling to complete time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df81a60a3c6ca1",
   "metadata": {},
   "source": [
    "# GRDC Hydro Data Processing and Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook processes and analyzes streamflow data from the Global Runoff Data Centre (GRDC) to create a comprehensive dataset of river discharge and runoff. The analysis is particularly useful for hydropower assessment, water resource management, and climate impact studies.\n",
    "\n",
    "## Setting Up Your Environment\n",
    "\n",
    "### Installation Instructions\n",
    "To run this notebook, you need to set up a Python environment with the required packages. Follow these steps:\n",
    "\n",
    "1. **Create a virtual environment** (recommended):\n",
    "   ```bash\n",
    "   # Using conda\n",
    "   conda create -n hydro-env python=3.8\n",
    "   conda activate hydro-env\n",
    "   ```\n",
    "\n",
    "2. **Install required packages**:\n",
    "   ```bash\n",
    "   # Navigate to the pre-analysis directory\n",
    "   cd epm/pre-analysis\n",
    "\n",
    "   # Install requirements\n",
    "   pip install -r requirements_hydro.txt\n",
    "   ```\n",
    "\n",
    "3. **Create a Jupyter kernel**:\n",
    "   ```bash\n",
    "   python -m ipykernel install --user --name hydro-env --display-name \"Hydro Analysis\"\n",
    "   ```\n",
    "\n",
    "4. **Launch Jupyter and select the kernel**:\n",
    "   ```bash\n",
    "   jupyter notebook\n",
    "   ```\n",
    "   When opening this notebook, select the \"Hydro Analysis\" kernel from the kernel menu.\n",
    "\n",
    "### Required Packages\n",
    "The following packages are required and will be installed from the requirements.txt file:\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- geopandas\n",
    "- folium\n",
    "- xarray\n",
    "- branca\n",
    "- shapely\n",
    "- rapidfuzz\n",
    "\n",
    "## Required Input Data\n",
    "This analysis requires three types of input data, all available from open-source repositories. You must download these datasets separately before running this notebook:\n",
    "\n",
    "1. **Monthly Streamflow Data from GRDC Stations**\n",
    "   - Source: [Global Runoff Data Centre (GRDC)](https://www.bafg.de/GRDC/)\n",
    "   - Format: NetCDF files containing monthly discharge measurements\n",
    "   - Required fields: station metadata (coordinates, catchment area), discharge time series\n",
    "   - Download by Station/Select Table in the left corner/Filer by country/Select all Stations/Download by country\n",
    "\n",
    "2. **Hydropower Plant Data from the Global Hydropower Tracker**\n",
    "   - Source: [Global Energy Monitor](https://globalenergymonitor.org/projects/global-hydropower-tracker/)\n",
    "   - Format: Excel spreadsheet (.xlsx)\n",
    "   - Required fields: plant names, locations, capacities, status, river names\n",
    "\n",
    "3. **River Network Data from HydroRIVERS**\n",
    "   - Source: [HydroSHEDS](https://www.hydrosheds.org/products/hydrorivers)\n",
    "   - Format: Shapefile (.shp)\n",
    "   - Required fields: river geometries, names, discharge estimates\n",
    "\n",
    "## Expected Outputs\n",
    "This notebook will generate:\n",
    "- Filtered and processed discharge and runoff datasets\n",
    "- Interactive maps showing stations, rivers, and hydropower plants\n",
    "- Time series plots of runoff evolution\n",
    "- Monthly discharge patterns for hydropower-relevant stations\n",
    "- CSV files with processed data for further analysis\n",
    "\n",
    "## Directory Structure\n",
    "Before running, ensure you have the following directory structure:\n",
    "```\n",
    "hydro/\n",
    "\u251c\u2500\u2500 input/\n",
    "\u2502   \u251c\u2500\u2500 grdc_input/         # GRDC station data (NetCDF files)\n",
    "\u2502   \u251c\u2500\u2500 river_input/        # HydroRIVERS shapefiles\n",
    "    \u2502   \u2514\u2500\u2500 HydroRIVERS_v10_af_shp\n",
    "\u2502   \u2514\u2500\u2500 Global-Hydropower-Tracker-April-2025.xlsx\n",
    "\u2514\u2500\u2500 output/                 # Will be created if it doesn't exist\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a43cbf66c979286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T06:30:08.274627Z",
     "start_time": "2025-06-24T06:30:08.260249Z"
    }
   },
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import folium\n",
    "import folium.plugins\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from branca.colormap import LinearColormap\n",
    "from folium import CircleMarker\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import box\n",
    "import calendar\n",
    "import numpy as np\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Import local utilities\n",
    "from utils import map_grdc_stationbasins_and_subregions\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "e396e191b773eaf8",
   "metadata": {},
   "source": [
    "## Step 1 - Configure user inputs and folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "97ba7cbd5074f56e",
   "metadata": {},
   "source": [
    "countries = ['Angola', 'Burundi', 'Cameroon', 'Central African Republic', 'Chad', 'Republic of the Congo', 'DR Congo', 'Equatorial Guinea', 'Gabon']\n",
    "\n",
    "# Define the base input directory\n",
    "base_dir = os.getcwd()\n",
    "folder_in  = os.path.join(base_dir, 'input')\n",
    "folder_out = os.path.join(base_dir, 'output')\n",
    "if not os.path.exists(folder_out): os.makedirs(folder_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b7361de753733d",
   "metadata": {},
   "source": [
    "## Step 2 - Load streamflow data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9824c38b4d3ac539",
   "metadata": {},
   "source": [
    "folder_grdc = os.path.join(folder_in, 'grdc_input')\n",
    "\n",
    "# Initialize a list to hold all xarray Datasets\n",
    "datasets = []\n",
    "\n",
    "# Walk through all directories under 'input'\n",
    "for root, dirs, files in os.walk(folder_grdc):\n",
    "    if 'GRDC-Monthly.nc' in files:\n",
    "        file_path = os.path.join(root, 'GRDC-Monthly.nc')\n",
    "        try:\n",
    "            data_discharge = xr.open_dataset(file_path)\n",
    "            datasets.append(data_discharge)\n",
    "            print(f\"Loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path}: {e}\")\n",
    "\n",
    "# Merge all datasets\n",
    "if datasets:\n",
    "    try:\n",
    "        data_discharge = xr.concat(datasets, dim='id')  # You can use another dim if needed\n",
    "        print(f\"Total merged dimensions: {data_discharge.dims}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to merge datasets: {e}\")\n",
    "else:\n",
    "    print(\"No streamflow data found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65647f4297482749",
   "metadata": {},
   "source": [
    "## Step 3 - Filter data for quality control\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3fc1228995adf798",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 1. Filter out stations with no valid area\n",
    "# -------------------------------\n",
    "\n",
    "# Identify stations with invalid or missing area\n",
    "invalid_area_mask = (data_discharge[\"area\"] <= 0) | data_discharge[\"area\"].isnull()\n",
    "removed_area_ids = data_discharge[\"id\"].values[invalid_area_mask.values]\n",
    "\n",
    "# Extract metadata of removed stations\n",
    "removed_area_meta = data_discharge.sel(id=removed_area_ids)[[\"station_name\", \"geo_x\", \"geo_y\", \"area\"]]\n",
    "stations_removed_area_df = removed_area_meta.to_dataframe().reset_index()\n",
    "\n",
    "# Remove stations with invalid area\n",
    "data_discharge = data_discharge.where(data_discharge[\"area\"] > 0, drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Filter out stations with low runoff values\n",
    "# -------------------------------\n",
    "\n",
    "threshold_runoff = 15  # m\u00b3/s\n",
    "\n",
    "# Identify stations where all runoff values are below threshold or NaN\n",
    "runoff = data_discharge[\"runoff_mean\"]\n",
    "low_flow_mask = (runoff < threshold_runoff) | runoff.isnull()\n",
    "stations_to_remove = low_flow_mask.all(dim=\"time\")\n",
    "removed_ids = data_discharge[\"id\"].values[stations_to_remove.values]\n",
    "\n",
    "# Extract metadata of removed stations\n",
    "removed_meta = data_discharge.sel(id=removed_ids)[[\"station_name\", \"geo_x\", \"geo_y\"]]\n",
    "stations_removed_lowflow_df = removed_meta.to_dataframe().reset_index()\n",
    "\n",
    "# Remove those stations from the dataset\n",
    "data_discharge = data_discharge.sel(id=~stations_to_remove)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Summary (optional display or export)\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Stations removed due to invalid area: {len(stations_removed_area_df)}\")\n",
    "print(f\"Stations removed due to low runoff: {len(stations_removed_lowflow_df)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d83376f05f63e5",
   "metadata": {},
   "source": [
    "## Step 4 - Format data for analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4284d220a3fa2ae2",
   "metadata": {},
   "source": [
    "def checking_duplicates_grdc(df):\n",
    "    \"\"\"\n",
    "    Check for duplicated (year, station_name, month) entries in the DataFrame.\n",
    "\n",
    "    This function identifies records that have the same year, station_name, and month,\n",
    "    which should be unique in a properly formatted dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing at least the columns 'year', 'station_name', and 'month'\n",
    "\n",
    "    Returns:\n",
    "        list: List of station names that have duplicate entries\n",
    "    \"\"\"\n",
    "    dupes = (\n",
    "        df.groupby([\"year\", \"station_name\", \"month\"])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .query(\"count > 1\")\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(dupes)} duplicated (year, station_name, month) entries\")\n",
    "\n",
    "    problem_stations = dupes['station_name'].unique()\n",
    "    print(f'Duplicated stations: {problem_stations}')\n",
    "\n",
    "    # Merge the duplicate keys back into df to see full rows\n",
    "    duplicated_rows = df.merge(dupes[[\"year\", \"station_name\", \"month\"]], on=[\"year\", \"station_name\", \"month\"])\n",
    "    #display(duplicated_rows.sort_values([\"station_name\", \"year\", \"month\"]))\n",
    "\n",
    "    return problem_stations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f64c637dae24cb0a",
   "metadata": {},
   "source": [
    "var_name = 'runoff_mean'\n",
    "\n",
    "meta = data_discharge[[\"station_name\", \"geo_x\", \"geo_y\"]].to_dataframe().reset_index()\n",
    "area = data_discharge[\"area\"].to_dataframe().reset_index()\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_station = data_discharge[var_name].to_dataframe(name=\"Q\").reset_index()\n",
    "\n",
    "# Merge metadata into the main DataFrame\n",
    "data_station = data_station.merge(meta, on=\"id\")\n",
    "data_station = data_station.merge(area, on=\"id\")\n",
    "\n",
    "# Add year and month columns\n",
    "data_station[\"year\"] = data_station[\"time\"].dt.year\n",
    "data_station[\"month\"] = data_station[\"time\"].dt.month\n",
    "\n",
    "# Check if station_name has duplicates\n",
    "if data_station.duplicated(subset=[\"station_name\"]).any():\n",
    "    problem_stations = checking_duplicates_grdc(data_station)\n",
    "\n",
    "    # Step 3: Get corresponding station IDs\n",
    "    problem_ids = data_station[data_station[\"station_name\"].isin(problem_stations)][\"id\"].unique()\n",
    "\n",
    "    # Step 4: Assign unique station names for problematic IDs only\n",
    "    rename_map = {\n",
    "        id_: f\"{data_station[data_station['id'] == id_]['station_name'].iloc[0]}_{i+1}\"\n",
    "        for i, id_ in enumerate(problem_ids)\n",
    "    }\n",
    "\n",
    "    # Step 5: Apply renaming based on `id` consistently\n",
    "    data_station[\"station_name\"] = data_station.apply(\n",
    "        lambda row: rename_map[row[\"id\"]] if row[\"id\"] in rename_map else row[\"station_name\"],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Create a unique label for each station using name + coordinates (optional)\n",
    "data_station[\"station_label\"] = data_station[\"station_name\"].str.strip() + \" (\" + data_station[\"geo_y\"].round(2).astype(str) + \", \" + data_station[\"geo_x\"].round(2).astype(str) + \")\"\n",
    "\n",
    "# Pivot to wide format: year as index, MultiIndex (month, id) as columns\n",
    "data_station_pivot = data_station.pivot(index=\"year\", columns=[\"station_name\", \"month\"], values=\"Q\")\n",
    "data_station_pivot.dropna(axis=1, how='all', inplace=True)\n",
    "data_station_pivot.sort_index(ascending=True, axis=1, inplace=True)\n",
    "data_station_pivot.to_csv(os.path.join(folder_out, f'grdc_discharge_monthly-m3-s.csv'), index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dd1c17ea795f1b1",
   "metadata": {},
   "source": [
    "## Step 5 - Calculate runoff metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "df03fffed797054a",
   "metadata": {},
   "source": [
    "def filter_full_years(df):\n",
    "    \"\"\"\n",
    "    Keep only (station, year) pairs with 12 valid months.\n",
    "    Returns filtered DataFrame and number of dropped rows.\n",
    "    \"\"\"\n",
    "    original_len = len(df)\n",
    "\n",
    "    # Count valid months per station-year\n",
    "    valid_counts = (\n",
    "        df.groupby(['station_name', 'year'])['Q']\n",
    "        .apply(lambda x: x.notna().sum())\n",
    "        .reset_index(name='valid_months')\n",
    "    )\n",
    "\n",
    "    # Only keep those with all 12 months\n",
    "    full_years = valid_counts[valid_counts['valid_months'] == 12]\n",
    "\n",
    "    # Merge to filter original DataFrame\n",
    "    df_filtered = df.merge(full_years[['station_name', 'year']], on=['station_name', 'year'])\n",
    "\n",
    "    removed_rows = original_len - len(df_filtered)\n",
    "    print(f\"Removed {removed_rows} rows \u2014 kept {len(df_filtered)} only full (12-month) years.\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def discharge_to_runoff(df):\n",
    "    \"\"\"\n",
    "    Convert monthly discharge (m\u00b3/s) into annual runoff (mm/year) at the station level.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Calculates the number of days in each month\n",
    "    2. Converts catchment area from km\u00b2 to m\u00b2\n",
    "    3. Computes monthly water volume (m\u00b3) from discharge (m\u00b3/s)\n",
    "    4. Sums monthly volumes for each station-year\n",
    "    5. Divides by catchment area to get runoff depth (m)\n",
    "    6. Converts from m to mm by multiplying by 1000\n",
    "\n",
    "    The equation used is:\n",
    "        runoff_mm = (\u03a3(Q_monthly_avg \u00d7 86400 \u00d7 days_in_month)) / area_m2 \u00d7 1000\n",
    "\n",
    "    Where:\n",
    "        - Q is discharge in m\u00b3/s\n",
    "        - 86400 is seconds per day\n",
    "        - days_in_month accounts for monthly totals\n",
    "        - area is the catchment area in m\u00b2\n",
    "        - 1000 converts meters to millimeters\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing monthly discharge data with columns:\n",
    "            - 'station_name': Name of the gauging station\n",
    "            - 'year': Year of the measurement\n",
    "            - 'time': Datetime of the measurement\n",
    "            - 'Q': Discharge in m\u00b3/s\n",
    "            - 'area': Catchment area in km\u00b2\n",
    "\n",
    "    Assumes:\n",
    "        - Input DataFrame has one row per station/month\n",
    "        - Years are complete (12 months per station)\n",
    "        - Area is in km\u00b2\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with columns:\n",
    "            - 'station_name': Name of the gauging station\n",
    "            - 'year': Year of the measurement\n",
    "            - 'runoff_mm_year': Annual runoff in mm/year\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Add number of days in each month\n",
    "    df['days_in_month'] = pd.to_datetime(df['time']).dt.days_in_month\n",
    "\n",
    "    # Convert area to m\u00b2\n",
    "    df['area_m2'] = df['area'] * 1e6\n",
    "\n",
    "    # Compute volume in m\u00b3 for each month\n",
    "    df['volume_m3'] = df['Q'] * 86400 * df['days_in_month']\n",
    "\n",
    "    # Sum monthly volumes per station-year\n",
    "    runoff_by_year = (\n",
    "        df.groupby(['station_name', 'year'])\n",
    "        .apply(lambda x: x['volume_m3'].sum() / x['area_m2'].iloc[0] * 1000, include_groups=False)  # m to mm\n",
    "        .reset_index(name='runoff_mm_year')\n",
    "    )\n",
    "\n",
    "    return runoff_by_year"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b0b80b52d97ebea",
   "metadata": {},
   "source": [
    "# Convert discharge data (m3-s) to runoff in (mm-year)\n",
    "data_station_filtered = filter_full_years(data_station)\n",
    "data_runoff = discharge_to_runoff(data_station_filtered)\n",
    "# Save to CSV\n",
    "data_runoff.round(0).pivot(index=\"year\", columns=\"station_name\", values=\"runoff_mm_year\").to_csv(os.path.join(folder_out, f'grdc_runoff_mm-year.csv'), index=True)\n",
    "\n",
    "location = data_station[['station_name', 'geo_x', 'geo_y']].drop_duplicates().reset_index(drop=True)\n",
    "# Merge with runoff_by_year data\n",
    "data_runoff = data_runoff.merge(location, on='station_name')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82d9f6106c05e9dd",
   "metadata": {},
   "source": [
    "## Step 6 - Associate hydropower plants with river data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ade120360afe7c35",
   "metadata": {},
   "source": [
    "path_hpp = os.path.join(folder_in, 'Global-Hydropower-Tracker-April-2025.xlsx')\n",
    "\n",
    "# Improved error handling for hydropower data loading\n",
    "try:\n",
    "    if not os.path.exists(path_hpp):\n",
    "        raise FileNotFoundError(f\"Hydropower data file not found: {path_hpp}\")\n",
    "\n",
    "    data_hpp = pd.read_excel(path_hpp, sheet_name='Data', header=[0], index_col=None)\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_columns = ['Country/Area 1', 'Project Name', 'River / Watercourse', 'Capacity (MW)', 'Status', 'Latitude', 'Longitude']\n",
    "    missing_columns = [col for col in required_columns if col not in data_hpp.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns in hydropower data: {missing_columns}\")\n",
    "\n",
    "    # Filter for countries of interest\n",
    "    data_hpp = data_hpp.loc[data_hpp['Country/Area 1'].isin(countries), :]\n",
    "\n",
    "    print(f\"Loaded {len(data_hpp)} hydropower plants from {len(countries)} countries\")\n",
    "\n",
    "    # Check if we have any data after filtering\n",
    "    if len(data_hpp) == 0:\n",
    "        print(\"Warning: No hydropower plants found for the specified countries\")\n",
    "\n",
    "    display(data_hpp.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading hydropower data: {e}\")\n",
    "    # Create an empty DataFrame with the required columns to avoid breaking the rest of the code\n",
    "    data_hpp = pd.DataFrame(columns=required_columns)\n",
    "    print(\"Created empty hydropower DataFrame to continue processing\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5bfdc5ba323a88f",
   "metadata": {},
   "source": [
    "# Add river name to runoff data\n",
    "meta_df = data_discharge[[\"station_name\", \"river_name\"]].to_dataframe().reset_index()\n",
    "meta_df = meta_df.drop_duplicates(subset=[\"station_name\"])\n",
    "\n",
    "data_station_filtered_river = data_station_filtered.merge(meta_df, on=\"station_name\", how=\"left\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6787093ba0cc8bf7",
   "metadata": {},
   "source": [
    "def clean_river_name(name):\n",
    "    \"\"\"\n",
    "    Clean and standardize river names for better matching.\n",
    "\n",
    "    This function:\n",
    "    1. Handles NaN/None values\n",
    "    2. Normalizes Unicode characters and removes accents\n",
    "    3. Removes common river-related words (RIVER, RIVIERE, etc.)\n",
    "    4. Standardizes spacing and converts to uppercase\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The river name to clean\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and standardized river name\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize unicode and remove accents (e.g. \u00e9 \u2192 e)\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove common river words and clean up spacing\n",
    "    name = re.sub(r\"\\b(RIVER|RIVIERE|RIVI\u00c8RE|R\\.|R)\\b\", \"\", name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip().upper()\n",
    "\n",
    "    return name\n",
    "\n",
    "# Prepare cleaned lists\n",
    "station_rivers_raw = data_station_filtered_river['river_name'].dropna().unique()\n",
    "station_rivers_cleaned = {clean_river_name(r): r for r in station_rivers_raw}\n",
    "\n",
    "matches = []\n",
    "\n",
    "# Iterate over all HPPs (row-wise)\n",
    "for _, row in data_hpp.dropna(subset=[\"River / Watercourse\"]).iterrows():\n",
    "    hpp_river_orig = row[\"River / Watercourse\"]\n",
    "    hpp_river_clean = clean_river_name(hpp_river_orig)\n",
    "\n",
    "    # Find best match among station rivers\n",
    "    match, score, _ = process.extractOne(\n",
    "        hpp_river_clean, station_rivers_cleaned.keys(), scorer=fuzz.token_set_ratio\n",
    "    )\n",
    "\n",
    "    if score >= 85:\n",
    "        matched_station_river = station_rivers_cleaned[match]\n",
    "        stations = data_station_filtered_river[\n",
    "            data_station_filtered_river[\"river_name\"] == matched_station_river\n",
    "        ][\"station_name\"].tolist()\n",
    "    else:\n",
    "        matched_station_river = None\n",
    "        stations = []\n",
    "\n",
    "    matches.append({\n",
    "        \"hpp_name\": row[\"Project Name\"],\n",
    "        \"hpp_river\": hpp_river_orig,\n",
    "        \"matched_river\": matched_station_river,\n",
    "        \"score\": score,\n",
    "        \"stations\": set(stations),\n",
    "        \"hpp_capacity\": row[\"Capacity (MW)\"],\n",
    "        \"hpp_status\": row[\"Status\"]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy display or export\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.to_csv(os.path.join(folder_out, 'hpp_grdc_hydro_matches.csv'), index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3eef254f4db555e3",
   "metadata": {},
   "source": [
    "# Associate hydropower plants with gauging stations based on proximity\n",
    "def associate_hpp_station(data_hpp, data_station, matches_df):\n",
    "    # Step 1: Build GeoDataFrames\n",
    "    gdf_hpp = gpd.GeoDataFrame(\n",
    "        data_hpp.copy(),\n",
    "        geometry=gpd.points_from_xy(data_hpp[\"Longitude\"], data_hpp[\"Latitude\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    gdf_station = gpd.GeoDataFrame(\n",
    "        data_station.copy(),\n",
    "        geometry=gpd.points_from_xy(data_station[\"geo_x\"], data_station[\"geo_y\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Project both to a metric CRS (meters)\n",
    "    gdf_hpp = gdf_hpp.to_crs(epsg=3857)\n",
    "    gdf_station = gdf_station.to_crs(epsg=3857)\n",
    "\n",
    "    # Step 3: Build dictionary from matches_df\n",
    "    station_lookup = {\n",
    "        row[\"hpp_name\"]: row[\"stations\"]\n",
    "        for _, row in matches_df.iterrows()\n",
    "        if row[\"stations\"]\n",
    "    }\n",
    "\n",
    "    # Step 4: For each HPP, associate station\n",
    "    results = []\n",
    "\n",
    "    for idx, hpp in gdf_hpp.iterrows():\n",
    "        hpp_name = hpp[\"Project Name\"]\n",
    "        if hpp_name in station_lookup and station_lookup[hpp_name]:\n",
    "            # Use the first station from the match (same river)\n",
    "            station_name = list(station_lookup[hpp_name])[0]\n",
    "            station = gdf_station[gdf_station[\"station_name\"] == station_name].iloc[0]\n",
    "            distance = hpp.geometry.distance(station.geometry)\n",
    "            results.append({\n",
    "                \"hpp_index\": idx,\n",
    "                \"nearest_station\": station_name,\n",
    "                \"distance_to_station_m\": distance,\n",
    "                \"same_river\": True\n",
    "            })\n",
    "        else:\n",
    "            # Find nearest station by distance\n",
    "            distances = gdf_station.geometry.distance(hpp.geometry)\n",
    "            min_idx = distances.idxmin()\n",
    "            nearest_station = gdf_station.loc[min_idx]\n",
    "            results.append({\n",
    "                \"hpp_index\": idx,\n",
    "                \"nearest_station\": nearest_station[\"station_name\"],\n",
    "                \"distance_to_station_m\": distances[min_idx],\n",
    "                \"same_river\": False\n",
    "            })\n",
    "\n",
    "    # Step 5: Merge back into HPP GeoDataFrame\n",
    "    nearest_df = pd.DataFrame(results)\n",
    "    gdf_hpp[\"nearest_station\"] = nearest_df[\"nearest_station\"].values\n",
    "    gdf_hpp[\"distance_to_station_m\"] = nearest_df[\"distance_to_station_m\"].values\n",
    "    gdf_hpp[\"same_river\"] = nearest_df[\"same_river\"].values\n",
    "\n",
    "    return gdf_hpp\n",
    "\n",
    "gdf_hpp = associate_hpp_station(data_hpp, data_station_filtered, matches_df)\n",
    "data_hpp = data_hpp.merge(\n",
    "    gdf_hpp[[\"Project Name\", \"nearest_station\", \"distance_to_station_m\", 'same_river']],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b6b70daca162c8b",
   "metadata": {},
   "source": [
    "## Step 7 - Visualize data on interactive maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6536c22fdbe8e78",
   "metadata": {},
   "source": [
    "# Function to clip rivers to the bounding box of runoff stations\n",
    "def clip_rivers_to_stations(rivers_path, df_runoff, ord_stra=6):\n",
    "    # Step 1: Load the HydroRIVERS shapefile\n",
    "\n",
    "    # To be downloaded from: https://www.hydrosheds.org/products/hydrorivers using shapefile format\n",
    "    # Load available layers (should return e.g. 'HydroRIVERS_v10_af')\n",
    "\n",
    "    if not os.path.exists(rivers_path):\n",
    "        raise FileNotFoundError(f\"Geodatabase not found: {rivers_path}\")\n",
    "\n",
    "    # Load rivers\n",
    "    rivers_gdf = gpd.read_file(rivers_path)\n",
    "\n",
    "    # Filter based on average discharge and stream order\n",
    "    rivers_gdf = rivers_gdf[(rivers_gdf[\"DIS_AV_CMS\"] > 50) & (rivers_gdf[\"ORD_STRA\"] >= 6) & (rivers_gdf[\"ORD_FLOW\"] <= 5)]\n",
    "    #rivers_gdf = rivers_gdf[(rivers_gdf[\"ORD_STRA\"] >= ord_stra)]\n",
    "\n",
    "    # Step 2: Prepare the bounding box for clipping\n",
    "    # Create GeoDataFrame from stations\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        df_runoff,\n",
    "        geometry=gpd.points_from_xy(df_runoff[\"geo_x\"], df_runoff[\"geo_y\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Get bounding box coordinates from the stations\n",
    "    roi_bounds = stations_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Create a polygon box from the bounds\n",
    "    roi_polygon = box(*roi_bounds).buffer(1.0)  # optional padding of 1 degree\n",
    "\n",
    "    # Create a GeoDataFrame for clipping\n",
    "    roi_gdf = gpd.GeoDataFrame(geometry=[roi_polygon], crs=\"EPSG:4326\")\n",
    "\n",
    "    rivers_clipped = gpd.clip(rivers_gdf, roi_gdf)\n",
    "\n",
    "    return rivers_clipped\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90fa62608c26fb54",
   "metadata": {},
   "source": [
    "# This cell can take a while to run (some minutes), depending on the number of stations and rivers\n",
    "def make_maps(df_runoff, data_hpp=None, rivers_path=None, folder_out=folder_out):\n",
    "    \"\"\"\n",
    "    Create interactive maps showing runoff stations, rivers, and hydropower plants.\n",
    "\n",
    "    This function generates a Folium map with multiple layers:\n",
    "    1. River network colored by discharge (if rivers_path is provided)\n",
    "    2. Gauging stations with size based on runoff and color based on years of data\n",
    "    3. Hydropower plants with size based on capacity and color based on status\n",
    "\n",
    "    Parameters:\n",
    "        df_runoff (DataFrame): DataFrame containing runoff data with station coordinates\n",
    "        data_hpp (DataFrame, optional): DataFrame containing hydropower plant data\n",
    "        rivers_path (str, optional): Path to the HydroRIVERS shapefile\n",
    "        folder_out (str): Output folder for saving the map\n",
    "\n",
    "    Returns:\n",
    "        folium.Map: The interactive map object\n",
    "    \"\"\"\n",
    "    # Read station data\n",
    "    agg = (\n",
    "        df_runoff.groupby([\"station_name\", \"geo_x\", \"geo_y\"])\n",
    "          .agg(\n",
    "              avg_runoff=(\"runoff_mm_year\", \"mean\"),\n",
    "              n_years=(\"year\", \"count\")\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create base map centered on the region\n",
    "    center_lat = agg[\"geo_y\"].mean()\n",
    "    center_lon = agg[\"geo_x\"].mean()\n",
    "\n",
    "    # Create map with improved base layer options\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon], \n",
    "        zoom_start=5, \n",
    "        tiles=\"CartoDB positron\"\n",
    "    )\n",
    "\n",
    "    # Add additional base layers for better visualization options\n",
    "    folium.TileLayer('CartoDB dark_matter', name='Dark Map').add_to(m)\n",
    "    folium.TileLayer('OpenStreetMap', name='OpenStreetMap').add_to(m)\n",
    "    folium.TileLayer(\n",
    "        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr='Esri',\n",
    "        name='Satellite'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add rivers from HydroRIVERS\n",
    "    name = f'station_runoff_map.html'\n",
    "    if rivers_path is not None:\n",
    "        try:\n",
    "            rivers_clipped = clip_rivers_to_stations(rivers_path, df_runoff)\n",
    "\n",
    "            # Create a feature group for rivers to allow toggling\n",
    "            rivers_group = folium.FeatureGroup(name=\"Rivers\", show=True)\n",
    "\n",
    "            colormap = plt.cm.viridis\n",
    "            norm = mcolors.Normalize(\n",
    "                vmin=rivers_clipped[\"DIS_AV_CMS\"].min(),\n",
    "                vmax=rivers_clipped[\"DIS_AV_CMS\"].max())\n",
    "\n",
    "            # Add rivers to the map with color based on discharge\n",
    "            for _, row in rivers_clipped.iterrows():\n",
    "                discharge = row[\"DIS_AV_CMS\"]\n",
    "                color = mcolors.to_hex(colormap(norm(discharge)))\n",
    "\n",
    "                folium.GeoJson(\n",
    "                    row[\"geometry\"],\n",
    "                    style_function=lambda feature, color=color: {\n",
    "                        \"color\": color,\n",
    "                        \"weight\": 2,\n",
    "                        \"opacity\": 0.8\n",
    "                    },\n",
    "                    tooltip=f\"River: {row.get('RIVER_NAME', 'Unknown')}<br>Discharge: {discharge:.1f} m\u00b3/s\"\n",
    "                ).add_to(rivers_group)\n",
    "\n",
    "            # Add the rivers group to the map\n",
    "            rivers_group.add_to(m)\n",
    "\n",
    "            # Define vmin and vmax\n",
    "            vmin = rivers_clipped[\"DIS_AV_CMS\"].min()\n",
    "            vmax = rivers_clipped[\"DIS_AV_CMS\"].max()\n",
    "\n",
    "            ticks_raw = np.linspace(vmin, vmax, 5)\n",
    "            ticks = [round(t, -int(np.floor(np.log10(t))) + 1) for t in ticks_raw]  # e.g., 72 \u2192 70, 1502 \u2192 1500\n",
    "\n",
    "            norm = mcolors.Normalize(vmin=min(ticks), vmax=max(ticks))\n",
    "            colors = [mcolors.to_hex(plt.cm.viridis(norm(t))) for t in ticks]\n",
    "\n",
    "            # Add high-contrast legend\n",
    "            discharge_colormap = LinearColormap(\n",
    "                colors=colors,\n",
    "                index=ticks,\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                caption=\"Avg River Discharge (m\u00b3/s)\"\n",
    "            )\n",
    "            discharge_colormap.add_to(m)\n",
    "\n",
    "            name = f'station_runoff_map_with_rivers.html'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading river data: {e}\")\n",
    "            print(\"Continuing without river data\")\n",
    "\n",
    "    # Create a feature group for active stations\n",
    "    stations_group = folium.FeatureGroup(name=\"Gauging Stations\", show=True)\n",
    "\n",
    "    # Include station\n",
    "    if agg is not None:\n",
    "        norm_years = LogNorm(vmin=agg[\"n_years\"].min(), vmax=agg[\"n_years\"].max())\n",
    "        colormap = plt.cm.viridis  # or any other colormap\n",
    "\n",
    "        for _, row in agg.iterrows():\n",
    "            # Normalize color from colormap\n",
    "            rgba = colormap(norm_years(row[\"n_years\"]))\n",
    "            hex_color = mcolors.to_hex(rgba)\n",
    "\n",
    "            # Scale size (radius) based on average runoff\n",
    "            radius = max(4, (row[\"avg_runoff\"] / agg[\"avg_runoff\"].max()) * 20)\n",
    "\n",
    "            # Add circle marker with enhanced popup\n",
    "            popup_content = f\"\"\"\n",
    "            <div style=\"font-family: Arial; min-width: 200px;\">\n",
    "                <h4 style=\"margin-bottom: 10px;\">{row['station_name']}</h4>\n",
    "                <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Avg Runoff:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['avg_runoff']:.0f} mm/year</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Years of Data:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['n_years']}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Coordinates:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['geo_y']:.4f}, {row['geo_x']:.4f}</td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                radius=radius,\n",
    "                color=hex_color,\n",
    "                fill=True,\n",
    "                fill_opacity=0.8,\n",
    "                popup=folium.Popup(popup_content, max_width=300)\n",
    "            ).add_to(stations_group)\n",
    "\n",
    "        # Add stations group to map\n",
    "        stations_group.add_to(m)\n",
    "\n",
    "        # FeatureGroup for removed stations\n",
    "        removed_stations_group = folium.FeatureGroup(name=\"Removed Stations\", show=False)\n",
    "\n",
    "        # Plot removed due to missing area (red X)\n",
    "        if 'stations_removed_area_df' in globals() and stations_removed_area_df is not None and not stations_removed_area_df.empty:\n",
    "            for _, row in stations_removed_area_df.iterrows():\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                    number_of_sides=4,\n",
    "                    radius=5,\n",
    "                    rotation=45,\n",
    "                    color=\"red\",\n",
    "                    fill=True,\n",
    "                    fill_color=\"red\",\n",
    "                    fill_opacity=1.0,\n",
    "                    popup=f\"<b>{row['station_name']}</b><br>Removed: Area \u2264 0\"\n",
    "                ).add_to(removed_stations_group)\n",
    "\n",
    "        # Plot removed due to low flow (orange X)\n",
    "        if 'stations_removed_lowflow_df' in globals() and stations_removed_lowflow_df is not None and not stations_removed_lowflow_df.empty:\n",
    "            for _, row in stations_removed_lowflow_df.iterrows():\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row[\"geo_y\"], row[\"geo_x\"]],\n",
    "                    number_of_sides=4,\n",
    "                    radius=5,\n",
    "                    rotation=45,\n",
    "                    color=\"orange\",\n",
    "                    fill=True,\n",
    "                    fill_color=\"orange\",\n",
    "                    fill_opacity=1.0,\n",
    "                    popup=f\"<b>{row['station_name']}</b><br>Removed: Flow < threshold\"\n",
    "                ).add_to(removed_stations_group)\n",
    "\n",
    "        # Add group to map\n",
    "        removed_stations_group.add_to(m)\n",
    "\n",
    "        # Create a combined legend for stations color and size\n",
    "        years_vals = np.percentile(agg[\"n_years\"], [0, 50, 100]).round(0).astype(int)\n",
    "        runoff_vals = np.percentile(agg[\"avg_runoff\"], [0, 50, 100]).round(0)\n",
    "\n",
    "        combined_legend_html = f'''\n",
    "        <div style=\"\n",
    "            position: fixed;\n",
    "            bottom: 50px; left: 50px; width: 260px;\n",
    "            background-color: white;\n",
    "            border: 2px solid grey;\n",
    "            z-index: 9999;\n",
    "            font-size: 13px;\n",
    "            padding: 12px;\">\n",
    "\n",
    "            <b style=\"font-size:14px;\">Gauging Stations</b><br><br>\n",
    "\n",
    "            <b>Years of Available Data (Color)</b><br>\n",
    "            <i style=\"background: #440154; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[0]}<br>\n",
    "            <i style=\"background: #21918c; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[1]}<br>\n",
    "            <i style=\"background: #fde725; width: 18px; height: 18px;\n",
    "               float: left; margin-right: 5px;\"></i> {years_vals[2]}<br><br>\n",
    "\n",
    "            <b>Avg Runoff (mm/year)</b><br>\n",
    "            <svg width=\"120\" height=\"90\">\n",
    "              <circle cx=\"30\" cy=\"20\" r=\"6\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"25\" font-size=\"12\">{int(runoff_vals[0])}</text>\n",
    "              <circle cx=\"30\" cy=\"45\" r=\"10\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"50\" font-size=\"12\">{int(runoff_vals[1])}</text>\n",
    "              <circle cx=\"30\" cy=\"75\" r=\"14\" fill=\"#777\" fill-opacity=\"0.7\" stroke=\"#333\" />\n",
    "              <text x=\"50\" y=\"80\" font-size=\"12\">{int(runoff_vals[2])}</text>\n",
    "            </svg>\n",
    "\n",
    "            <b>Removed Stations</b><br>\n",
    "            <i class=\"fa fa-times\" style=\"color:red; margin-right: 6px;\"></i> Area missing<br>\n",
    "            <i class=\"fa fa-times\" style=\"color:orange; margin-right: 6px;\"></i> Low flow\n",
    "        </div>\n",
    "        '''\n",
    "\n",
    "        # Add legend to the map\n",
    "        m.get_root().html.add_child(folium.Element(combined_legend_html))\n",
    "\n",
    "    # Include hydropower plants\n",
    "    if data_hpp is not None and not data_hpp.empty:\n",
    "        # Create a feature group for hydropower plants\n",
    "        hpp_group = folium.FeatureGroup(name=\"Hydropower Plants\", show=True)\n",
    "\n",
    "        # Define fixed status colors\n",
    "        custom_status_colors = {\n",
    "            'operating': '#2ca02c',           # green\n",
    "            'construction': '#ff7f0e',        # orange\n",
    "            'announced': '#999999',           # grey\n",
    "            'pre-construction': '#9467bd'     # purple\n",
    "        }\n",
    "\n",
    "        # Filter data_hpp for selected statuses only\n",
    "        valid_statuses = list(custom_status_colors.keys())\n",
    "        data_hpp_filtered = data_hpp[data_hpp['Status'].isin(valid_statuses)]\n",
    "\n",
    "        if not data_hpp_filtered.empty:\n",
    "            # Normalize capacity for marker sizing\n",
    "            cap_min = data_hpp_filtered['Capacity (MW)'].min()\n",
    "            cap_max = data_hpp_filtered['Capacity (MW)'].max()\n",
    "\n",
    "            def normalize_capacity(value):\n",
    "                return 6 + ((value - cap_min) / (cap_max - cap_min + 1e-6)) * 10  # radius 6-16\n",
    "\n",
    "            # Convert RGBA to HEX for Folium\n",
    "            status_colors = {k: mcolors.to_hex(v) for k, v in custom_status_colors.items()}\n",
    "\n",
    "            # Add each HPP as a square marker with enhanced popup\n",
    "            for _, row in data_hpp_filtered.iterrows():\n",
    "                # Create a more structured popup with selected fields\n",
    "                selected_fields = [\n",
    "                    'Project Name', 'Status', 'Capacity (MW)', \n",
    "                    'River / Watercourse', 'Country/Area 1'\n",
    "                ]\n",
    "\n",
    "                popup_content = f\"\"\"\n",
    "                <div style=\"font-family: Arial; min-width: 250px;\">\n",
    "                    <h4 style=\"margin-bottom: 10px;\">{row['Project Name']}</h4>\n",
    "                    <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                \"\"\"\n",
    "\n",
    "                for field in selected_fields:\n",
    "                    if field in row and field != 'Project Name':\n",
    "                        value = row[field]\n",
    "                        popup_content += f\"\"\"\n",
    "                        <tr>\n",
    "                            <td style=\"padding: 3px;\"><b>{field}:</b></td>\n",
    "                            <td style=\"padding: 3px;\">{value}</td>\n",
    "                        </tr>\n",
    "                        \"\"\"\n",
    "\n",
    "                # Add nearest station info if available\n",
    "                if 'nearest_station' in row and 'distance_to_station_m' in row:\n",
    "                    distance_km = row['distance_to_station_m'] / 1000\n",
    "                    same_river = \"Yes\" if row.get('same_river', False) else \"No\"\n",
    "                    popup_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Nearest Station:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{row['nearest_station']}</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Distance:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{distance_km:.1f} km</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td style=\"padding: 3px;\"><b>Same River:</b></td>\n",
    "                        <td style=\"padding: 3px;\">{same_river}</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "\n",
    "                popup_content += \"\"\"\n",
    "                    </table>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "                color = status_colors.get(row['Status'], '#555')\n",
    "                size = normalize_capacity(row['Capacity (MW)'])\n",
    "\n",
    "                folium.RegularPolygonMarker(\n",
    "                    location=[row['Latitude'], row['Longitude']],\n",
    "                    number_of_sides=4,  # square\n",
    "                    radius=size,\n",
    "                    color=None,\n",
    "                    fill=True,\n",
    "                    fill_color=color,\n",
    "                    fill_opacity=0.9,\n",
    "                    popup=folium.Popup(popup_content, max_width=300)\n",
    "                ).add_to(hpp_group)\n",
    "\n",
    "            # Add the HPP group to the map\n",
    "            hpp_group.add_to(m)\n",
    "\n",
    "            # Prepare capacity values for size scale legend\n",
    "            capacity_vals = np.percentile(data_hpp_filtered['Capacity (MW)'], [0, 50, 100]).round(0).astype(int)\n",
    "            circle_sizes = [6, 10, 14]  # Match values used in marker radius scaling\n",
    "\n",
    "            # Create status color legend HTML\n",
    "            status_legend_items = \"\"\n",
    "            for status in valid_statuses:\n",
    "                hex_color = custom_status_colors[status]\n",
    "                status_legend_items += f'''\n",
    "                    <i style=\"background:{hex_color}; width: 18px; height: 18px;\n",
    "                    float: left; margin-right: 6px;\"></i> {status}<br>\n",
    "                '''\n",
    "\n",
    "            # Full HTML block\n",
    "            hpp_legend_html = f'''\n",
    "            <div style=\"\n",
    "                position: fixed;\n",
    "                bottom: 50px; left: 340px; width: 280px;\n",
    "                background-color: white;\n",
    "                border: 2px solid grey;\n",
    "                z-index: 9999;\n",
    "                font-size: 13px;\n",
    "                padding: 12px;\">\n",
    "\n",
    "                <b style=\"font-size:14px;\">Hydropower Plants</b><br><br>\n",
    "\n",
    "                <b>Status (Color)</b><br>\n",
    "                {status_legend_items}\n",
    "                <br>\n",
    "\n",
    "                <b>Capacity (MW)</b><br>\n",
    "                <svg width=\"160\" height=\"100\">\n",
    "                  <rect x=\"20\" y=\"10\" width=\"{circle_sizes[0]*2}\" height=\"{circle_sizes[0]*2}\" fill=\"#999\"/>\n",
    "                  <text x=\"60\" y=\"25\" font-size=\"12\">{capacity_vals[0]}</text>\n",
    "                  <rect x=\"20\" y=\"40\" width=\"{circle_sizes[1]*2}\" height=\"{circle_sizes[1]*2}\" fill=\"#999\"/>\n",
    "                  <text x=\"60\" y=\"55\" font-size=\"12\">{capacity_vals[1]}</text>\n",
    "                  <rect x=\"20\" y=\"70\" width=\"{circle_sizes[2]*2}\" height=\"{circle_sizes[2]*2}\" fill=\"#999\"/>\n",
    "                  <text x=\"60\" y=\"85\" font-size=\"12\">{capacity_vals[2]}</text>\n",
    "                </svg>\n",
    "            </div>\n",
    "            '''\n",
    "            m.get_root().html.add_child(folium.Element(hpp_legend_html))\n",
    "\n",
    "    # Add layer control to toggle different map elements\n",
    "    folium.LayerControl().add_to(m)\n",
    "\n",
    "    # Add fullscreen button for better user experience\n",
    "    folium.plugins.Fullscreen().add_to(m)\n",
    "\n",
    "    # Add measure tool for distance measurements\n",
    "    folium.plugins.MeasureControl(position='topleft', primary_length_unit='kilometers').add_to(m)\n",
    "\n",
    "    # Save the map\n",
    "    try:\n",
    "        m.save(os.path.join(folder_out, name))\n",
    "        print(f\"Map saved to {os.path.join(folder_out, name)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving map: {e}\")\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# Add info\n",
    "\n",
    "river_path = os.path.join(folder_in, 'river_input', 'HydroRIVERS_v10_af_shp', 'HydroRIVERS_v10_af.shp')\n",
    "make_maps(data_runoff, data_hpp=data_hpp, rivers_path=river_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3309cd3489781576",
   "metadata": {},
   "source": [
    "## Step 8 - Plot station runoff for each hydropower plant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6517211419d7211",
   "metadata": {},
   "source": [
    "stations = data_hpp['nearest_station'].unique()\n",
    "# Filter data_runoff to include only stations that have hydropower plants associated\n",
    "temp = data_runoff[data_runoff['station_name'].isin(stations)].copy()\n",
    "# Subplots for each station and evolution of"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7d4577ad21f9b34",
   "metadata": {},
   "source": [
    "def plot_runoff_evolution(data_runoff, data_hpp, folder_out=None):\n",
    "    \"\"\"\n",
    "    Plot the evolution of runoff for each hydropower plant's nearest station.\n",
    "    This function creates subplots for each station associated with hydropower plants,\n",
    "    showing the annual runoff evolution over the years.\n",
    "\n",
    "    Parameters:\n",
    "        data_runoff (DataFrame): DataFrame containing runoff data with station coordinates.\n",
    "        data_hpp (DataFrame): DataFrame containing hydropower plant data with nearest stations.\n",
    "        folder_out (str, optional): Output folder to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    stations = data_hpp['nearest_station'].unique()\n",
    "    temp = data_runoff[data_runoff['station_name'].isin(stations)].copy()\n",
    "\n",
    "    # Map station_name to project name and river match info\n",
    "    station_to_project = data_hpp.set_index('nearest_station')['Project Name'].to_dict()\n",
    "    station_to_river_match = data_hpp.set_index('nearest_station')['same_river'].to_dict()\n",
    "\n",
    "    temp['Project Name'] = temp['station_name'].map(station_to_project)\n",
    "    temp['same_river'] = temp['station_name'].map(station_to_river_match)\n",
    "\n",
    "    n_stations = len(stations)\n",
    "    ncols = 3\n",
    "    nrows = (n_stations + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), sharey=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    min_year = temp['year'].min()\n",
    "    max_year = temp['year'].max()\n",
    "\n",
    "    for i, station in enumerate(stations):\n",
    "        ax = axs[i]\n",
    "        subset = temp[temp['station_name'] == station]\n",
    "        ax.plot(subset['year'].astype(int), subset['runoff_mm_year'], marker='o', linestyle='-')\n",
    "        match_label = \"Same River\" if station_to_river_match[station] else \"Closest Station\"\n",
    "        ax.set_title(f\"{station} / {station_to_project[station]} ({match_label})\", fontsize=10)\n",
    "        ax.set_ylabel(\"Runoff (mm/year)\")\n",
    "        ax.set_xlim(min_year, max_year)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if folder_out:\n",
    "        plt.savefig(os.path.join(folder_out, 'runoff_evolution_hpp.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_runoff_evolution(data_runoff, data_hpp, folder_out=folder_out)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60ed38604ed1657b",
   "metadata": {},
   "source": [
    "def plot_discharge_month(data_station_filtered, data_hpp, folder_out=None):\n",
    "    \"\"\"\n",
    "    Plot monthly discharge evolution for each hydropower plant's nearest station.\n",
    "    This function creates subplots for each station associated with hydropower plants,\n",
    "    showing the average monthly discharge over the years.\n",
    "\n",
    "    Parameters:\n",
    "        data_station_filtered (DataFrame): DataFrame containing station discharge data.\n",
    "        data_hpp (DataFrame): DataFrame containing hydropower plant data with nearest stations.\n",
    "        folder_out (str, optional): Output folder to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    stations = data_hpp['nearest_station'].unique()\n",
    "    temp = data_station_filtered[data_station_filtered['station_name'].isin(stations)].copy()\n",
    "\n",
    "    # Map station_name to project name and river match info\n",
    "    station_to_project = data_hpp.set_index('nearest_station')['Project Name'].to_dict()\n",
    "    station_to_river_match = data_hpp.set_index('nearest_station')['same_river'].to_dict()\n",
    "\n",
    "    temp['Project Name'] = temp['station_name'].map(station_to_project)\n",
    "    temp['same_river'] = temp['station_name'].map(station_to_river_match)\n",
    "\n",
    "    n_stations = len(stations)\n",
    "    ncols = 3\n",
    "    nrows = (n_stations + ncols - 1) // ncols\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), sharey=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, station in enumerate(stations):\n",
    "        ax = axs[i]\n",
    "        subset = temp[temp['station_name'] == station]\n",
    "        subset = subset.groupby(['Project Name', 'month', 'same_river'])['Q'].mean().reset_index()\n",
    "        subset['month_abbr'] = subset['month'].astype(int).apply(lambda x: calendar.month_abbr[x])\n",
    "        ax.plot(subset['month_abbr'], subset['Q'], marker='o', linestyle='-')\n",
    "        match_label = \"Same River\" if station_to_river_match[station] else \"Closest Station\"\n",
    "        ax.set_title(f\"{station} / {station_to_project[station]} ({match_label})\", fontsize=10)\n",
    "        ax.set_ylabel(\"Discharge (m3/s)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if folder_out:\n",
    "        plt.savefig(os.path.join(folder_out, 'runoff_monthly_evolution_hpp.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "plot_discharge_month(data_station_filtered, data_hpp, folder_out=folder_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d75611ccde26c67b",
   "metadata": {},
   "source": [
    "## Step 9 - Fill missing values in time series data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "57011395a5bcb5b",
   "metadata": {},
   "source": [
    "def fill_missing_climatology(df, min_months=3):\n",
    "    \"\"\"\n",
    "    Fill missing values using monthly climatology (station-wise).\n",
    "    Returns a filled DataFrame and a mask of filled values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Filter out sparse years\n",
    "    valid_years = (\n",
    "        df.groupby([\"station_name\", \"year\"])[\"Q\"]\n",
    "        .apply(lambda x: x.notna().sum() >= min_months)\n",
    "        .reset_index(name=\"keep\")\n",
    "    )\n",
    "\n",
    "    # Merge to filter out sparse rows\n",
    "    df = df.merge(valid_years[valid_years[\"keep\"]], on=[\"station_name\", \"year\"])\n",
    "    df.drop(columns=\"keep\", inplace=True)\n",
    "\n",
    "    # Step 2: Build climatology\n",
    "    climatology = (\n",
    "        df.groupby([\"station_name\", \"month\"])[\"Q\"]\n",
    "        .mean()\n",
    "        .rename(\"Q_clim\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df = df.merge(climatology, on=[\"station_name\", \"month\"], how=\"left\")\n",
    "\n",
    "    # Step 3: Fill missing with climatology\n",
    "    fill_mask = df[\"Q\"].isna()\n",
    "    df.loc[fill_mask, \"Q\"] = df.loc[fill_mask, \"Q_clim\"]\n",
    "\n",
    "    df.drop(columns=\"Q_clim\", inplace=True)\n",
    "\n",
    "    return df, fill_mask\n",
    "\n",
    "def drop_sparse_years_and_interpolate(df, min_months=9):\n",
    "    \"\"\"\n",
    "    Drops years with too much missing data and interpolates gaps per station.\n",
    "    Assumes monthly data. Returns cleaned & interpolated DataFrame.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Count non-NaN entries per year-station\n",
    "    valid_counts = (\n",
    "        df_clean.groupby([\"station_name\", \"year\"])[\"Q\"]\n",
    "        .apply(lambda x: x.notna().sum())\n",
    "        .rename(\"valid_months\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep only rows with enough months\n",
    "    valid_years = valid_counts[valid_counts[\"valid_months\"] >= min_months]\n",
    "    df_clean = df_clean.merge(valid_years[[\"station_name\", \"year\"]], on=[\"station_name\", \"year\"])\n",
    "\n",
    "    # Sort for interpolation\n",
    "    df_clean = df_clean.sort_values([\"station_name\", \"year\", \"month\"])\n",
    "\n",
    "    # Interpolate per station\n",
    "    df_clean[\"Q\"] = df_clean.groupby(\"station_name\")[\"Q\"].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def plot_all_fill_methods(df_original, df_clim, df_interp, output_dir):\n",
    "    \"\"\"\n",
    "    Plots original, climatology-filled, and interpolated runoff data per station in one plot.\n",
    "\n",
    "    Assumes all DataFrames have columns: ['year', 'month', 'station_name', 'Q'].\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_dir = os.path.join(output_dir, 'stations')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Combine 'year' and 'month' into datetime\n",
    "    def add_datetime(df):\n",
    "        return df.assign(\n",
    "            date=pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str).str.zfill(2))\n",
    "        )\n",
    "\n",
    "    df_original = add_datetime(df_original)\n",
    "    df_clim = add_datetime(df_clim)\n",
    "    df_interp = add_datetime(df_interp)\n",
    "\n",
    "    # Loop over each station\n",
    "    stations = df_original['station_name'].unique()\n",
    "\n",
    "    for station in stations:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "        # Subsets for current station\n",
    "        df_o = df_original[df_original['station_name'] == station]\n",
    "        df_c = df_clim[df_clim['station_name'] == station]\n",
    "        df_i = df_interp[df_interp['station_name'] == station]\n",
    "\n",
    "        # Plot all\n",
    "        ax.plot(df_o['date'], df_o['Q'], label=\"Original\", alpha=0.5, marker='o', linestyle='-', color='black')\n",
    "        ax.plot(df_c['date'], df_c['Q'], label=\"Climatology Fill\", linestyle='--', color='orange')\n",
    "        ax.plot(df_i['date'], df_i['Q'], label=\"Interpolated\", linestyle='-', color='blue')\n",
    "\n",
    "        ax.set_title(f\"Station: {station}\")\n",
    "        ax.set_ylabel(\"Discharge / Runoff (Q)\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{station}.png\"))\n",
    "        plt.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52e919231d3dacdf",
   "metadata": {},
   "source": [
    "df_filled, filled_mask = fill_missing_climatology(df_filtered, min_months=6)\n",
    "df_interpolated = drop_sparse_years_and_interpolate(df_filtered, min_months=9)\n",
    "plot_all_fill_methods(df_filtered, df_filled, df_interpolated, output_dir=folder_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ec6989afbb929d1",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatic_env",
   "language": "python",
   "name": "climatic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}