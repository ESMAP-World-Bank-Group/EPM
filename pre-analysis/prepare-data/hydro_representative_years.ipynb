{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca1b31d",
   "metadata": {},
   "source": [
    "# Hydro Representative Years (in progress)\n",
    "\n",
    "**Objective**  \n",
    "Select hydrological representative years and convert them into EPM-ready availability series for each hydro plant.\n",
    "\n",
    "**Data requirements (user-provided) and method**  \n",
    "- Data requirements: Drop one CSV per hydro plant under `pre-analysis/prepare-data/input/hydro_representative_years/` (months in the index, calendar years as columns). Provide `capacity_hydro_sample.csv`, optional `impact_cc_hydro_sample.csv`, and any extra availability blocks directly under `pre-analysis/prepare-data/input/`. This repo now ships with toy CSVs so the notebook runs end-to-end after cloning.  \n",
    "- Method: Configure clustering/selection parameters, load and clean the monthly profiles, apply any climate adjustments, compute representative years, and export availability tables plus QA plots under `pre-analysis/prepare-data/output/hydro_representative_years/`.\n",
    "\n",
    "**Overview of steps**  \n",
    "1. Step 1 - Import helpers and point to the working directories.  \n",
    "2. Step 2 - Declare rename dictionaries, clustering options, and scenario filters.  \n",
    "3. Step 3 - Build helper functions to compute seasonal averages and align metadata.  \n",
    "4. Step 4 - Load all hydro CSVs, compute representative years, and format the resulting EPM tables.  \n",
    "5. Step 5 - Review the exported CSVs and diagnostic plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e528b7a",
   "metadata": {},
   "source": [
    "## 1. User Inputs\n",
    "\n",
    "Edit these parameters first. Paths can be absolute or relative to `pre-analysis/prepare-data`. By default, the notebook reads files directly from the shared `input/` and `output/` folders so it is immediately reusable across countries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths relative to `pre-analysis/prepare-data/`\n",
    "monthly_hydro_folder = 'input/hydro_representative_years'  # folder with one CSV per plant\n",
    "output_folder = 'output/hydro_representative_years'        # folder that will store CSVs/plots\n",
    "\n",
    "capacity_file = 'input/capacity_hydro_sample.csv'          # installed capacity per plant (after rename)\n",
    "impact_cc_file = 'input/impact_cc_hydro_sample.csv'        # set to None to skip climate adjustments\n",
    "availability_other_file = 'input/pAvailability_others_sample.csv'  # optional: merged with hydro availability\n",
    "existing_plants = ['kaleta', 'souapiti']                   # subset used for diagnostics\n",
    "\n",
    "n_clusters = 2                      # number of representative years to extract\n",
    "method = 'real'                     # clustering method passed to run_reduced_scenarios\n",
    "format_preparation = 'stochastic'   # 'stochastic' or 'monte_carlo' downstream formatting\n",
    "extract_extreme = True              # True to keep explicit min/max stress cases\n",
    "rename_scenario = None              # Optional dict mapping raw scenario names to low/medium/high labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef21b30",
   "metadata": {},
   "source": [
    "## 2. Setup: imports and helper paths\n",
    "\n",
    "Loads pandas, resolves the `pre-analysis/prepare-data` root regardless of where the kernel started, and imports utilities from the `representative_days` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def locate_prepare_data_dir():\n",
    "    \"\"\"Return the absolute path to `pre-analysis/prepare-data`.\"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    candidates = [\n",
    "        cwd,\n",
    "        cwd / 'pre-analysis' / 'prepare-data',\n",
    "        cwd / 'prepare-data',\n",
    "    ]\n",
    "    candidates += [parent / 'pre-analysis' / 'prepare-data' for parent in cwd.parents]\n",
    "    seen = set()\n",
    "    for candidate in candidates:\n",
    "        candidate = candidate.resolve()\n",
    "        if candidate in seen:\n",
    "            continue\n",
    "        seen.add(candidate)\n",
    "        if (candidate / 'hydro_representative_years.ipynb').exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        'Cannot locate `pre-analysis/prepare-data`. Start the kernel inside the repo or update `locate_prepare_data_dir`.'\n",
    "    )\n",
    "\n",
    "\n",
    "PREPARE_DATA_DIR = locate_prepare_data_dir()\n",
    "INPUT_DIR = PREPARE_DATA_DIR / 'input'\n",
    "OUTPUT_DIR = PREPARE_DATA_DIR / 'output'\n",
    "\n",
    "\n",
    "def resolve_path(path_like):\n",
    "    \"\"\"Resolve absolute paths while keeping `None` untouched.\"\"\"\n",
    "    if path_like is None:\n",
    "        return None\n",
    "    path = Path(path_like)\n",
    "    return path if path.is_absolute() else PREPARE_DATA_DIR / path\n",
    "\n",
    "\n",
    "if str(PREPARE_DATA_DIR) not in sys.path:\n",
    "    # Allow importing the `representative_days` package without manual PYTHONPATH tweaks.\n",
    "    sys.path.insert(0, str(PREPARE_DATA_DIR))\n",
    "\n",
    "try:\n",
    "    from representative_days.utils_reprdays import run_reduced_scenarios, plot_uncertainty, nb_days\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\n",
    "        'Install or activate the `representative_days` package so `utils_reprdays` can be imported.'\n",
    "    ) from exc\n",
    "\n",
    "\n",
    "monthly_hydro_folder = resolve_path(monthly_hydro_folder)\n",
    "output_folder = resolve_path(output_folder)\n",
    "capacity_file = resolve_path(capacity_file)\n",
    "impact_cc_file = resolve_path(impact_cc_file)\n",
    "availability_other_file = resolve_path(availability_other_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa80f56",
   "metadata": {},
   "source": [
    "## 3 - Define plant metadata helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_hpp = {\n",
    "    'amaria': 'Amaria',\n",
    "    'baneah': 'Baneah', \n",
    "    'bonkon_diaria': 'Bonkon-Diaria', \n",
    "    'boureya': 'Boureya', \n",
    "    'diallol': 'Diallol', \n",
    "    'diareguela': 'Diareguela', \n",
    "    'digan': 'Digan',\n",
    "    'donkea': 'Donkea', \n",
    "    'farankonedou': 'Farankonedou', \n",
    "    'fello_sounga': 'Fello Sounga',\n",
    "    'fetore': 'Fetore', \n",
    "    'fomi': 'Fomi', \n",
    "    'garafiri': 'Garafiri',\n",
    "    'grand_chutes': 'Grand Chutes', \n",
    "    'grand_kinkon': 'Grand Kinkon',\n",
    "    'guozoguezia': 'Guozoguezia',\n",
    "    'hakkunde': 'Hakkunde-Mitti',\n",
    "    'kaleta': 'Kaleta',\n",
    "    'kassa': 'Kassa',\n",
    "    'kogebedou': 'Kogebedou',\n",
    "    'korafindi': 'Korafindi',\n",
    "    'kouloutamba': 'Koukoutamba',\n",
    "    'kouravel': 'Kouravel',\n",
    "    'morisanako': 'Morisanako',\n",
    "    'niagara': 'Niagara',\n",
    "    'nzebela': 'Nzebela',\n",
    "    'poudalde': 'Poudalde',\n",
    "    'souapiti': 'Souapiti',\n",
    "    'tiopo_105': 'Tiopo'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seasonal_average(df):\n",
    "    \"\"\"Calculate the seasonal average of the data.\n",
    "    \n",
    "    The seasonal average is calculated for the period from November to June.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The data to calculate the seasonal average. The data should be in the format of a DataFrame with the columns\n",
    "        'year' and 'month' and the values to calculate the seasonal average.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    temp = df.stack()\n",
    "    temp.index.names = ['month', 'year']\n",
    "    temp = temp.reorder_levels(['year', 'month'])\n",
    "    temp = temp.sort_index()\n",
    "    temp = temp.reset_index(name='value')\n",
    "    temp['seasonal year'] = temp['year'].astype(int)\n",
    "    temp.loc[temp['month'] >= 11, 'seasonal year'] += 1\n",
    "    temp = temp.astype({'year': int, 'month': int, 'value': float, 'seasonal year': int})\n",
    "    temp_filtered = temp[temp['month'].isin([11, 12, 1, 2, 3, 4, 5, 6])]\n",
    "    seasonal_averages = temp_filtered.groupby('seasonal year')['value'].mean()\n",
    "    seasonal_averages.name = 'value avg'\n",
    "    temp_filtered = pd.merge(temp_filtered, seasonal_averages, left_on='seasonal year', right_index=True)\n",
    "    temp_filtered.loc[:, 'value'] = temp_filtered.loc[:, 'value avg']\n",
    "    temp_filtered = temp_filtered.loc[:, ['year', 'month', 'value']]\n",
    "    temp_filtered.rename(columns={'value': 'value avg'}, inplace=True)\n",
    "\n",
    "    temp = pd.merge(temp, temp_filtered, on=['year', 'month'], how='left')\n",
    "    # Replace the value by the average value if value avg is not missing\n",
    "    temp.loc[~temp['value avg'].isnull(), 'value'] = temp.loc[~temp['value avg'].isnull(), 'value avg']\n",
    "    \n",
    "    temp = temp.loc[:, ['year', 'month', 'value']]\n",
    "    temp = temp.pivot(index='month', columns='year', values='value') \n",
    "    temp.columns = temp.columns.astype(str)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ea621",
   "metadata": {},
   "source": [
    "## 4 - Load hydro files and compute representative years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fello_sounga.csv\n",
      "farankonedou.csv\n",
      "baneah.csv\n",
      "poudalde.csv\n",
      "nzebela.csv\n",
      "tiopo_105.csv\n",
      "niagara.csv\n",
      "digan.csv\n",
      "bonkon_diaria.csv\n",
      "grand_chutes.csv\n",
      "diallol.csv\n",
      "amaria.csv\n",
      "fetore.csv\n",
      "kaleta.csv\n",
      "kaleta\n",
      "kouloutamba.csv\n",
      "donkea.csv\n",
      "fomi.csv\n",
      "garafiri.csv\n",
      "garafiri\n",
      "guozoguezia.csv\n",
      "hakkunde.csv\n",
      "korafindi.csv\n",
      "souapiti.csv\n",
      "souapiti\n",
      "kogebedou.csv\n",
      "diareguela.csv\n",
      "kouravel.csv\n",
      "kassa.csv\n",
      "morisanako.csv\n",
      "grand_kinkon.csv\n",
      "boureya.csv\n",
      "Capacity file exist to calculate the availability\n",
      "Availability file exists for other sources\n"
     ]
    }
   ],
   "source": [
    "if monthly_hydro_folder is None:\n",
    "    raise ValueError('Set `monthly_hydro_folder` to the folder that stores the monthly hydro CSVs.')\n",
    "if not monthly_hydro_folder.exists():\n",
    "    raise FileNotFoundError(f'Cannot find monthly hydro folder: {monthly_hydro_folder}')\n",
    "\n",
    "# Default output mirrors the shared `output/` folder so teams can compare runs quickly.\n",
    "default_output = OUTPUT_DIR / 'hydro_representative_years'\n",
    "out_folder = output_folder or default_output\n",
    "out_folder = Path(out_folder)\n",
    "out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "suffix_file = f'{n_clusters}'\n",
    "if impact_cc_file is not None:\n",
    "    suffix_file = f'{suffix_file}_cc'\n",
    "if extract_extreme:\n",
    "    suffix_file = f'{suffix_file}_extreme'\n",
    "\n",
    "impact_cc = None\n",
    "if impact_cc_file is not None:\n",
    "    if not impact_cc_file.exists():\n",
    "        raise FileNotFoundError(f'Cannot find climate-impact file: {impact_cc_file}')\n",
    "    impact_cc = pd.read_csv(impact_cc_file, index_col=0).squeeze()\n",
    "\n",
    "if existing_plants is None:\n",
    "    existing_plants = []\n",
    "\n",
    "csv_files = sorted(monthly_hydro_folder.glob('*.csv'))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f'No CSV files found under {monthly_hydro_folder}')\n",
    "\n",
    "data, results = {}, {}\n",
    "tot, tot_existing = [], []\n",
    "\n",
    "# Representative years are determined from the system-level production, not individual files.\n",
    "for csv_path in csv_files:\n",
    "    print(csv_path.name)\n",
    "    name = csv_path.stem\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    # Only select years after 1980 to focus on recent hydrology.\n",
    "    df.columns = pd.to_numeric(df.columns)\n",
    "    df = df.loc[:, [i for i in df.columns if i >= 1980]]\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # Keep the average production for the dry season (Novâ€“Jun).\n",
    "    df = calculate_seasonal_average(df)\n",
    "\n",
    "    if impact_cc is not None:\n",
    "        df_cc = pd.DataFrame()\n",
    "        for scenario, factor in impact_cc.items():\n",
    "            temp = df.copy() * (1 + factor)\n",
    "            temp.columns = [f'{col}-{scenario}' for col in temp.columns]\n",
    "            df_cc = pd.concat([df_cc, temp], axis=1)\n",
    "\n",
    "        df = df_cc.copy()\n",
    "\n",
    "    data.update({name: df.copy()})\n",
    "    tot = tot + [df]\n",
    "    if name in existing_plants:\n",
    "        tot_existing = tot_existing + [df]\n",
    "\n",
    "# Sum all the hydropower production to feed the clustering algorithm.\n",
    "tot = sum(tot)\n",
    "tot_existing = sum(tot_existing)\n",
    "\n",
    "# Indicator (useful to keep track of average production levels).\n",
    "prod = (tot.T * nb_days).T * 24 / 1e3\n",
    "indicator = {'yearly_prod_avg': prod.sum().mean(), 'monthly_prod_avg': prod.mean(axis=1)}\n",
    "\n",
    "# Find the representative years and optionally tag explicit min/max chronicles.\n",
    "if extract_extreme:\n",
    "    worst_case = tot.sum().sort_values().iloc[:int(len(tot.columns) / 20)]\n",
    "    min_prod = {k: i.loc[:, list(worst_case.index)].mean(axis=1) for k, i in data.items()}\n",
    "    data = {k: i.drop(columns=list(worst_case.index)) for k, i in data.items()}\n",
    "    proba_min = worst_case.shape[0] / len(tot.columns)\n",
    "\n",
    "    best_case = tot.sum().sort_values().iloc[-int(len(tot.columns) / 20):]\n",
    "    max_prod = {k: i.loc[:, list(best_case.index)].mean(axis=1) for k, i in data.items()}\n",
    "    data = {k: i.drop(columns=list(best_case.index)) for k, i in data.items()}\n",
    "    proba_max = best_case.shape[0] / len(tot.columns)\n",
    "\n",
    "years_repr = run_reduced_scenarios(tot, n_clusters=n_clusters, method=method)\n",
    "scenarios = [i.split(' - ')[0] for i in years_repr.columns]\n",
    "\n",
    "# Select the data for the representative years\n",
    "data = {k: i.loc[:, scenarios].stack() for k, i in data.items()}\n",
    "if format_preparation == 'monte_carlo':\n",
    "    mc_folder = out_folder / 'output_monte_carlo'\n",
    "    mc_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "elif format_preparation == 'stochastic':\n",
    "    data = pd.concat(data, axis=1, names=['hpp']).T\n",
    "    data.columns.names = ['Month', 'Scenarios']\n",
    "    data = data.reorder_levels(['Scenarios', 'Month'], axis=1)\n",
    "    data = data.sort_index(axis=1)\n",
    "\n",
    "    # Assign the scenarios to low, medium and high if the user did not pass their own mapping.\n",
    "    if n_clusters == 1:\n",
    "        rename_scenario = {scenarios[0]: 'baseline'}\n",
    "        data.rename(columns=rename_scenario, level='Scenarios', inplace=True)\n",
    "    elif n_clusters == 3:\n",
    "        temp = tot.loc[:, scenarios].sum()\n",
    "        rename_scenario = {\n",
    "            temp.idxmin(): 'low',\n",
    "            temp.idxmax(): 'high',\n",
    "            [i for i in temp.index if i not in [temp.idxmin(), temp.idxmax()]][0]: 'medium',\n",
    "        }\n",
    "        data = data.rename(columns=rename_scenario, level='Scenarios')\n",
    "\n",
    "    if extract_extreme:\n",
    "        min_prod = pd.concat(min_prod, axis=1, names=['hpp']).T\n",
    "        min_prod = pd.concat([min_prod], axis=1, keys=['min'])\n",
    "        min_prod.columns.names = ['Scenarios', 'Month']\n",
    "        min_prod = min_prod.sort_index(axis=1)\n",
    "\n",
    "        data = pd.concat([data, min_prod], axis=1)\n",
    "\n",
    "        max_prod = pd.concat(max_prod, axis=1, names=['hpp']).T\n",
    "        max_prod = pd.concat([max_prod], axis=1, keys=['max'])\n",
    "        max_prod.columns.names = ['Scenarios', 'Month']\n",
    "        max_prod = max_prod.sort_index(axis=1)\n",
    "\n",
    "        data = pd.concat([data, max_prod], axis=1)\n",
    "\n",
    "    # Rename the hpp to the readable names expected downstream.\n",
    "    if rename_hpp is not None:\n",
    "        data.rename(index=rename_hpp, level='hpp', inplace=True)\n",
    "\n",
    "    if capacity_file is None:\n",
    "        raise ValueError('Capacity file is required to convert production into availability.')\n",
    "    if not capacity_file.exists():\n",
    "        raise FileNotFoundError(f'Cannot find capacity file: {capacity_file}')\n",
    "\n",
    "    print('Capacity file found -> calculate availability profiles')\n",
    "    capacity = pd.read_csv(capacity_file, index_col=0).squeeze()\n",
    "    capacity = capacity.loc[data.index.get_level_values('hpp').unique()]\n",
    "    availability = (data.T / capacity).T\n",
    "\n",
    "    # Export in EPM format\n",
    "    availability.index.names = [None]\n",
    "\n",
    "    if availability_other_file is not None and availability_other_file.exists():\n",
    "        print(f'Merging additional availability from {availability_other_file.name}')\n",
    "        availability_other = pd.read_csv(availability_other_file, index_col=0, header=0)\n",
    "        availability_other.columns.names = ['Month']\n",
    "        availability_other.columns = availability_other.columns.astype(int)\n",
    "\n",
    "        availability_other = pd.concat(\n",
    "            [availability_other] * len(availability.columns.get_level_values('Scenarios').unique()),\n",
    "            keys=availability.columns.get_level_values('Scenarios').unique(),\n",
    "            axis=1,\n",
    "        )\n",
    "        availability = pd.concat((availability, availability_other), axis=0)\n",
    "\n",
    "    availability.round(3).to_csv(out_folder / f'pAvailability_hydro_{suffix_file}.csv')\n",
    "\n",
    "    # For sensitivity analysis\n",
    "    if n_clusters == 3:\n",
    "        for i in ['low', 'medium', 'high']:\n",
    "            availability.loc(axis=1)[i].round(3).to_csv(out_folder / f'pAvailability_hydro_{i}.csv')\n",
    "\n",
    "    # Probabilities is in the name of the scenarios\n",
    "    pProbaScenarios = pd.Series(\n",
    "        [i.split(' - ')[1] for i in years_repr.columns],\n",
    "        index=pd.Index([i.split(' - ')[0] for i in years_repr.columns], name='Scenarios'),\n",
    "        name='Value',\n",
    "    )\n",
    "    if rename_scenario is not None:\n",
    "        pProbaScenarios = pProbaScenarios.rename(index=rename_scenario)\n",
    "    if extract_extreme:\n",
    "        pProbaScenarios = pProbaScenarios.astype(float) * (1 - proba_min - proba_max)\n",
    "        pProbaScenarios.loc['min'] = proba_min\n",
    "        pProbaScenarios.loc['max'] = proba_max\n",
    "\n",
    "    pProbaScenarios.to_csv(out_folder / f'pProbaScenarios_hydro_{suffix_file}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Review outputs and QA plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'baseline' in data.columns.get_level_values('Scenarios'):\n",
    "    capacity_baseline = data.xs('baseline', level='Scenarios', axis=1)\n",
    "    prod = (capacity_baseline * nb_days).T * 24 / 1e3\n",
    "    display(prod.sum(axis=1), prod.sum().sum())\n",
    "\n",
    "    prod_current = prod.loc[:, ['Kaleta', 'Souapiti']]\n",
    "    display(prod_current.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = data.mul(nb_days, level='Month', axis=1) * 24 / 1e3\n",
    "capacity = data.sum()\n",
    "existing_plants_named = [rename_hpp[i] for i in existing_plants if i in rename_hpp]\n",
    "capacity_existing = data.loc[existing_plants_named, :].sum()\n",
    "prod = prod.sum()\n",
    "prod, capacity, capacity_existing = (\n",
    "    prod.unstack('Scenarios'),\n",
    "    capacity.unstack('Scenarios'),\n",
    "    capacity_existing.unstack('Scenarios'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = out_folder / f'production_hydro_tot_{suffix_file}.png'\n",
    "plot_uncertainty(\n",
    "    tot,\n",
    "    df2=capacity,\n",
    "    title=\"Hydropower capacity (MW)\",\n",
    "    ylabel=\"MW\",\n",
    "    filename=str(filename),\n",
    "    ymax=4500,\n",
    ")\n",
    "\n",
    "filename = out_folder / f'production_hydro_existing_{suffix_file}.png'\n",
    "plot_uncertainty(\n",
    "    tot_existing,\n",
    "    df2=capacity_existing,\n",
    "    title=\"Hydropower capacity existing (MW)\",\n",
    "    ylabel=\"MW\",\n",
    "    filename=str(filename),\n",
    "    ymax=1100,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
