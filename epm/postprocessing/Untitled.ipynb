{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0e7d34-32d7-4de1-aaf3-46f64e13f71c",
   "metadata": {},
   "source": [
    "## Plotting lines with capacity ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daf8e6d8-332c-4bf9-8eac-d317c9642f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from functools import wraps\n",
    "import io\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "sys.path.insert(0, './postprocessing')\n",
    "from utils import *\n",
    "from plots import *\n",
    "\n",
    "# Verify the file exists before reading\n",
    "output_file = \"../output/simulations_run_20251216_150832/baseline/output_csv/pDispatch.csv\"\n",
    "if os.path.exists(output_file):\n",
    "\tpDispatch = pd.read_csv(output_file)\n",
    "else:\n",
    "\tprint(f\"Warning: File not found: {output_file}\")\n",
    "\tprint(f\"Available files in current directory: {os.listdir('.')}\")\n",
    "\t# Uncomment and adjust the path below once you confirm the correct location\n",
    "\t# pDispatch_df = pd.read_csv(\"path/to/correct/pDispatch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9d53762",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_components = filter_dataframe(pDispatch, {'uni': [\"Imports\", \"Exports\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f60ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>z</th>\n",
       "      <th>y</th>\n",
       "      <th>uni</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <th>d</th>\n",
       "      <th>t</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Q1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">d1</th>\n",
       "      <th>t1</th>\n",
       "      <td>South_Africa</td>\n",
       "      <td>2025</td>\n",
       "      <td>Imports</td>\n",
       "      <td>271.132542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2</th>\n",
       "      <td>South_Africa</td>\n",
       "      <td>2025</td>\n",
       "      <td>Imports</td>\n",
       "      <td>284.862346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3</th>\n",
       "      <td>South_Africa</td>\n",
       "      <td>2025</td>\n",
       "      <td>Imports</td>\n",
       "      <td>69.854085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t4</th>\n",
       "      <td>South_Africa</td>\n",
       "      <td>2025</td>\n",
       "      <td>Imports</td>\n",
       "      <td>49.692150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>South_Africa</td>\n",
       "      <td>2025</td>\n",
       "      <td>Imports</td>\n",
       "      <td>298.592150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Q4</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">d4</th>\n",
       "      <th>t16</th>\n",
       "      <td>Mozal</td>\n",
       "      <td>2035</td>\n",
       "      <td>Exports</td>\n",
       "      <td>-58.172169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t21</th>\n",
       "      <td>Mozal</td>\n",
       "      <td>2035</td>\n",
       "      <td>Exports</td>\n",
       "      <td>-43.292450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t22</th>\n",
       "      <td>Mozal</td>\n",
       "      <td>2035</td>\n",
       "      <td>Exports</td>\n",
       "      <td>-43.292450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t23</th>\n",
       "      <td>Mozal</td>\n",
       "      <td>2035</td>\n",
       "      <td>Exports</td>\n",
       "      <td>-43.292450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t24</th>\n",
       "      <td>Mozal</td>\n",
       "      <td>2035</td>\n",
       "      <td>Exports</td>\n",
       "      <td>-38.332543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68918 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      z     y      uni       value\n",
       "q  d  t                                           \n",
       "Q1 d1 t1   South_Africa  2025  Imports  271.132542\n",
       "      t2   South_Africa  2025  Imports  284.862346\n",
       "      t3   South_Africa  2025  Imports   69.854085\n",
       "      t4   South_Africa  2025  Imports   49.692150\n",
       "      t5   South_Africa  2025  Imports  298.592150\n",
       "...                 ...   ...      ...         ...\n",
       "Q4 d4 t16         Mozal  2035  Exports  -58.172169\n",
       "      t21         Mozal  2035  Exports  -43.292450\n",
       "      t22         Mozal  2035  Exports  -43.292450\n",
       "      t23         Mozal  2035  Exports  -43.292450\n",
       "      t24         Mozal  2035  Exports  -38.332543\n",
       "\n",
       "[68918 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fe1288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     z     y      uni       value\n",
      "q  d  t                                          \n",
      "Q1 d1 t1  South_Africa  2025  Imports  271.132542\n",
      "      t2  South_Africa  2025  Imports  284.862346\n",
      "      t3  South_Africa  2025  Imports   69.854085\n",
      "      t4  South_Africa  2025  Imports   49.692150\n",
      "      t5  South_Africa  2025  Imports  298.592150\n"
     ]
    }
   ],
   "source": [
    "df_plot = exchange_components.copy()\n",
    "df_plot = df_plot.set_index(['q', 'd','t'])\n",
    "print(df_plot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ef5af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m df_plot = df_plot.drop(columns=[\u001b[33m'\u001b[39m\u001b[33muni\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m df_plot_init=df_plot[df_plot[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]==\u001b[32m2025\u001b[39m].drop(columns=[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mdf_plot_init\u001b[49m\u001b[43m.\u001b[49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.plot(kind = \u001b[33m'\u001b[39m\u001b[33mbar\u001b[39m\u001b[33m'\u001b[39m, stacked  =\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#print(df_plot_init.head())\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\frame.py:9955\u001b[39m, in \u001b[36mDataFrame.unstack\u001b[39m\u001b[34m(self, level, fill_value, sort)\u001b[39m\n\u001b[32m   9891\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9892\u001b[39m \u001b[33;03mPivot a level of the (necessarily hierarchical) index labels.\u001b[39;00m\n\u001b[32m   9893\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   9951\u001b[39m \u001b[33;03mdtype: float64\u001b[39;00m\n\u001b[32m   9952\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9953\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[32m-> \u001b[39m\u001b[32m9955\u001b[39m result = \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9957\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33munstack\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:504\u001b[39m, in \u001b[36munstack\u001b[39m\u001b[34m(obj, level, fill_value, sort)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, DataFrame):\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.index, MultiIndex):\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unstack_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    506\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m obj.T.stack(future_stack=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:529\u001b[39m, in \u001b[36m_unstack_frame\u001b[39m\u001b[34m(obj, level, fill_value, sort)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_unstack_frame\u001b[39m(\n\u001b[32m    526\u001b[39m     obj: DataFrame, level, fill_value=\u001b[38;5;28;01mNone\u001b[39;00m, sort: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    527\u001b[39m ) -> DataFrame:\n\u001b[32m    528\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.index, MultiIndex)  \u001b[38;5;66;03m# checked by caller\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     unstacker = \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj._can_fast_transpose:\n\u001b[32m    534\u001b[39m         mgr = obj._mgr.unstack(unstacker, fill_value=fill_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:154\u001b[39m, in \u001b[36m_Unstacker.__init__\u001b[39m\u001b[34m(self, index, level, constructor, sort)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_cells > np.iinfo(np.int32).max:\n\u001b[32m    147\u001b[39m     warnings.warn(\n\u001b[32m    148\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cells \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    149\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min the resulting pandas object.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m         PerformanceWarning,\n\u001b[32m    151\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    152\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:210\u001b[39m, in \u001b[36m_Unstacker._make_selectors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m mask.put(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.sum() < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.index):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.group_index = comp_index\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.mask = mask\n",
      "\u001b[31mValueError\u001b[39m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "df_plot = exchange_components.copy()\n",
    "\n",
    "df_plot = df_plot.set_index(['q', 'd','t'])\n",
    "df_plot = df_plot.drop(columns=['uni'])\n",
    "df_plot_init=df_plot[df_plot['y']==2025].drop(columns=['y'])\n",
    "#df_plot_init.unstack().plot(kind = 'bar', stacked  =True)\n",
    "#print(df_plot_init.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39993c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.plot.bar(stacked  =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af68aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'qdt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[33;03m\"\"\"x_order = (df_plot[['q', 'd', 't', 'qdt']]\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m           .drop_duplicates()\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m           .sort_values(['q', 'd', 't'])['qdt']\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m           .tolist())\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Call the plotting helper from plots.py\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Column names: subplot = year, stacked = zone 'z', x-axis = 'qdt', value column = 'value'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmake_stacked_barplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_plot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdict_colors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_subplot\u001b[49m\u001b[43m=\u001b[49m\u001b[43myear_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_stacked\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_xaxis\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqdt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mselect_subplot\u001b[49m\u001b[43m=\u001b[49m\u001b[43myears\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mannotate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#select_x=x_order,\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#show_legend=True\u001b[39;49;00m\n\u001b[32m     43\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\Documents\\Projects\\EPM\\epm\\postprocessing\\plots.py:1711\u001b[39m, in \u001b[36mmake_stacked_barplot\u001b[39m\u001b[34m(df, filename, dict_colors, df_errorbars, overlay_df, legend_label, column_subplot, column_stacked, column_xaxis, column_value, select_subplot, stacked_grouping, order_scenarios, dict_scenarios, format_y, order_stacked, cap, annotate, show_total, fonttick, rotation, title, fontsize_label, format_label, figsize, hspace, cols_per_row, juxtaposed, year_ini, column_annotation, annotation_pad, annotation_joiner)\u001b[39m\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m column_subplot \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m column_stacked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1711\u001b[39m         df = (\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_subplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_stacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_xaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[column_value].sum().reset_index())\n\u001b[32m   1712\u001b[39m         df = df.set_index([column_stacked, column_xaxis, column_subplot]).squeeze().unstack(column_subplot)\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\frame.py:9210\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9213\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9216\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1331\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wb590499\\.conda\\envs\\epm_env\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'qdt'"
     ]
    }
   ],
   "source": [
    "# Plot df_plot_init as stacked bar with countries (z) stacked\n",
    "df_to_plot = df_plot_init.reset_index()\n",
    "\n",
    "# Create combined x-axis label (season|day|time)\n",
    "df_to_plot['qdt'] = df_to_plot['q'].astype(str) + '|' + df_to_plot['d'].astype(str) + '|' + df_to_plot['t'].astype(str)\n",
    "\n",
    "# Call make_stacked_barplot: stacked by zone (z), x-axis organized by qdt\n",
    "make_stacked_barplot(\n",
    "    df_to_plot,\n",
    "    filename=None,  # Set to a path (e.g., 'stacked_bar_2025.png') to save\n",
    "    dict_colors=None,  # Optional: pass a dict mapping zones to colors\n",
    "    column_subplot=None,  # No subplots for single year\n",
    "    column_stacked='z',  # Stack by country\n",
    "    column_xaxis='qdt',  # X-axis groups by season|day|time\n",
    "    column_value='value',  # Values to stack\n",
    "    annotate=False,  # Set to True to add value labels\n",
    "    figsize=(16, 6),\n",
    "    show_legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec4f6350-b259-486a-871e-3cd6252529cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_GEOJSON_HEADER = \"Geojson,EPM,region,country,division\"\n",
    "\n",
    "\n",
    "def _read_geojson_mapping(path):\n",
    "    \"\"\"\n",
    "    Read a GeoJSON-to-EPM mapping CSV while normalizing repeated header rows.\n",
    "\n",
    "    Some exported CSVs append the header again without a newline, causing pandas\n",
    "    to fail when parsing. This helper inserts the missing newline and removes\n",
    "    duplicate header rows so that `pd.read_csv` can succeed.\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf-8-sig') as fp:\n",
    "        raw_text = fp.read()\n",
    "\n",
    "    if _GEOJSON_HEADER not in raw_text:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    pattern = r'(?<=.)(?<![\\r\\n])' + re.escape(_GEOJSON_HEADER)\n",
    "    normalized_text = re.sub(pattern, '\\n' + _GEOJSON_HEADER, raw_text)\n",
    "\n",
    "    clean_lines = []\n",
    "    header_seen = False\n",
    "    for line in normalized_text.splitlines():\n",
    "        if line.strip() == _GEOJSON_HEADER:\n",
    "            if header_seen:\n",
    "                continue\n",
    "            header_seen = True\n",
    "        clean_lines.append(line)\n",
    "\n",
    "    clean_text = \"\\n\".join(clean_lines)\n",
    "    if not clean_text.endswith(\"\\n\"):\n",
    "        clean_text += \"\\n\"\n",
    "\n",
    "    return pd.read_csv(io.StringIO(clean_text))\n",
    "\n",
    "def create_zonemap(zone_map, map_geojson_to_epm):\n",
    "    \"\"\"\n",
    "    Convert zone map to the correct coordinate reference system (CRS) and extract centroids.\n",
    "\n",
    "    This function ensures that the provided `zone_map` is in EPSG:4326 (latitude/longitude),\n",
    "    extracts the centroid coordinates of each zone, and maps them to the EPM zone names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zone_map : gpd.GeoDataFrame\n",
    "        A GeoDataFrame containing zone geometries and attributes.\n",
    "    map_geojson_to_epm : dict\n",
    "        Dictionary mapping GeoJSON zone names to EPM zone names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        - zone_map (gpd.GeoDataFrame): The zone map converted to EPSG:4326.\n",
    "        - centers (dict): Dictionary mapping EPM zone names to their centroid coordinates [longitude, latitude].\n",
    "    \"\"\"\n",
    "    if zone_map.crs is not None and zone_map.crs.to_epsg() != 4326:\n",
    "        zone_map = zone_map.to_crs(epsg=4326)  # Convert to EPSG:4326 for folium\n",
    "\n",
    "    # Get the coordinates of the centers of the zones\n",
    "    centers = {\n",
    "        row['ADMIN']: [row.geometry.centroid.x, row.geometry.centroid.y]\n",
    "        for _, row in zone_map.iterrows()\n",
    "    }\n",
    "\n",
    "    centers = {map_geojson_to_epm[c]: v for c, v in centers.items() if c in map_geojson_to_epm}\n",
    "\n",
    "    return zone_map, centers\n",
    "\n",
    "\n",
    "def get_json_data(epm_results=None, selected_zones=None, dict_specs=None, geojson_to_epm=None, geo_add=None,\n",
    "                  zone_map=None):\n",
    "    \"\"\"\n",
    "    Extract and process zone map data, handling divisions for sub-national regions.\n",
    "\n",
    "    This function retrieves the zone map, identifies zones that need to be divided\n",
    "    (e.g., North-South or East-West split), applies the `divide` function, and\n",
    "    returns a processed GeoDataFrame ready for visualization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epm_results : dict\n",
    "        Dictionary containing EPM results, including transmission capacity data.\n",
    "    dict_specs : dict\n",
    "        Dictionary with mapping specifications, including:\n",
    "        - `geojson_to_epm`: Mapping from GeoJSON names to EPM zone names.\n",
    "        - `map_zones`: GeoDataFrame of all countries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        - zone_map (gpd.GeoDataFrame): Processed zone map including divided regions.\n",
    "        - geojson_to_epm (dict): Updated mapping of GeoJSON names to EPM zones.\n",
    "    \"\"\"\n",
    "    assert ((dict_specs is not None) or (geojson_to_epm is not None)), \"Mapping zone names from geojson to EPM must be provided either under dict_specs or under geojson_to_epm\"\n",
    "\n",
    "    if dict_specs is None:\n",
    "        if 'postprocessing' in os.getcwd():\n",
    "            dict_specs = read_plot_specs(folder='')\n",
    "        else:\n",
    "            dict_specs = read_plot_specs(folder='postprocessing')\n",
    "    if geojson_to_epm is None:\n",
    "        geojson_to_epm = dict_specs['geojson_to_epm']\n",
    "    else:\n",
    "        if not os.path.exists(geojson_to_epm):\n",
    "            raise FileNotFoundError(f\"GeoJSON to EPM mapping file not found: {os.path.abspath(geojson_to_epm)}\")\n",
    "        geojson_to_epm = _read_geojson_mapping(geojson_to_epm)\n",
    "    epm_to_geojson = {v: k for k, v in\n",
    "                      geojson_to_epm.set_index('Geojson')['EPM'].to_dict().items()}  # Reverse dictionary\n",
    "    geojson_to_divide = geojson_to_epm.loc[geojson_to_epm.region.notna()]\n",
    "    geojson_complete = geojson_to_epm.loc[~geojson_to_epm.region.notna()]\n",
    "    if selected_zones is None:\n",
    "        selected_zones_epm = geojson_to_epm['EPM'].unique()\n",
    "    else:\n",
    "        selected_zones_epm = selected_zones\n",
    "    selected_zones_to_divide = [e for e in selected_zones_epm if e in geojson_to_divide['EPM'].values]\n",
    "    selected_countries_geojson = [\n",
    "        epm_to_geojson[key] for key in selected_zones_epm if\n",
    "        ((key not in selected_zones_to_divide) and (key in epm_to_geojson))\n",
    "    ]\n",
    "\n",
    "    if zone_map is None:\n",
    "        zone_map = dict_specs['map_zones']  # getting json data on all countries\n",
    "    else:\n",
    "        zone_map = gpd.read_file(zone_map)\n",
    "\n",
    "    zone_map = zone_map[zone_map['ADMIN'].isin(selected_countries_geojson)]\n",
    "\n",
    "    if geo_add is not None:\n",
    "        zone_map_add = gpd.read_file(geo_add)\n",
    "        zone_map = pd.concat([zone_map, zone_map_add])\n",
    "\n",
    "    divided_parts = []\n",
    "    for (country, division), subset in geojson_to_divide.groupby(['country', 'division']):\n",
    "        # Apply division function\n",
    "        divided_parts.append(divide(dict_specs['map_zones'], country, division))\n",
    "\n",
    "    if divided_parts:\n",
    "        zone_map_divide = pd.concat(divided_parts)\n",
    "\n",
    "        zone_map_divide = \\\n",
    "        geojson_to_divide.rename(columns={'country': 'ADMIN'}).merge(zone_map_divide, on=['region', 'ADMIN'])[\n",
    "            ['Geojson', 'ISO_A3', 'ISO_A2', 'geometry']]\n",
    "        zone_map_divide = zone_map_divide.rename(columns={'Geojson': 'ADMIN'})\n",
    "        # Convert zone_map_divide back to a GeoDataFrame\n",
    "        zone_map_divide = gpd.GeoDataFrame(zone_map_divide, geometry='geometry', crs=zone_map.crs)\n",
    "\n",
    "        # Ensure final zone_map is in EPSG:4326\n",
    "        zone_map = pd.concat([zone_map, zone_map_divide]).to_crs(epsg=4326)\n",
    "    geojson_to_epm = geojson_to_epm.set_index('Geojson')['EPM'].to_dict()  # get only relevant info\n",
    "    return zone_map, geojson_to_epm\n",
    "\n",
    "\n",
    "def get_value(df, zone, year, scenario, attribute, column_to_select='attribute'):\n",
    "    \"\"\"Safely retrieves an energy value for a given zone, year, scenario, and attribute.\"\"\"\n",
    "    value = df.loc[\n",
    "        (df['zone'] == zone) & (df['year'] == year) & (df['scenario'] == scenario) & (df[column_to_select] == attribute),\n",
    "        'value'\n",
    "    ]\n",
    "    return value.values[0] if not value.empty else 0\n",
    "\n",
    "\n",
    "def divide(geodf, country, division):\n",
    "    \"\"\"\n",
    "    Divide a country's geometry into two subzones using North-South (NS) or East-West (EW) division.\n",
    "\n",
    "    This function overlays the country geometry with a dividing polygon and extracts\n",
    "    the two subregions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geodf : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing geometries of all countries.\n",
    "    country : str\n",
    "        Name of the country to divide.\n",
    "    division : str\n",
    "        Type of division:\n",
    "        - 'NS' (North-South) splits along the latitude midpoint.\n",
    "        - 'EW' (East-West) splits along the longitude midpoint.\n",
    "        - 'NSE' (North-South-East) splits into three quadrants.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.GeoDataFrame\n",
    "        GeoDataFrame containing the divided subregions with the correct CRS.\n",
    "    \"\"\"\n",
    "    # Get the country geometry\n",
    "    crs = geodf.crs\n",
    "    country_geometry = geodf.loc[geodf['ADMIN'] == country, 'geometry'].values[0]\n",
    "\n",
    "    # Get bounds\n",
    "    minx, miny, maxx, maxy = country_geometry.bounds\n",
    "\n",
    "    if division == 'NS':\n",
    "        median_latitude = (miny + maxy) / 2\n",
    "        south_polygon = Polygon([(minx, miny), (minx, median_latitude), (maxx, median_latitude), (maxx, miny)])\n",
    "        north_polygon = Polygon([(minx, median_latitude), (minx, maxy), (maxx, maxy), (maxx, median_latitude)])\n",
    "\n",
    "        # Convert to GeoDataFrame with the correct CRS\n",
    "        south_gdf = gpd.GeoDataFrame(geometry=[south_polygon], crs=crs)\n",
    "        north_gdf = gpd.GeoDataFrame(geometry=[north_polygon], crs=crs)\n",
    "\n",
    "        south_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], south_gdf, how='intersection')\n",
    "        north_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], north_gdf, how='intersection')\n",
    "        south_part = south_part.to_crs(crs)\n",
    "        north_part = north_part.to_crs(crs)\n",
    "        south_part['region'] = 'south'\n",
    "        north_part['region'] = 'north'\n",
    "\n",
    "        return pd.concat([south_part, north_part])\n",
    "    \n",
    "    elif division == 'EW':\n",
    "        median_longitude = (maxx-minx) / 2\n",
    "        median_limit = minx + median_longitude\n",
    "        \n",
    "        west_polygon = Polygon([(minx, miny), (minx, maxy), (median_limit, maxy), (median_limit, miny)])\n",
    "        east_polygon = Polygon([(median_limit, miny), (median_limit, maxy), (maxx, maxy), (maxx, miny)])\n",
    "\n",
    "        # Convert to GeoDataFrame with the correct CRS\n",
    "        west_gdf = gpd.GeoDataFrame(geometry=[west_polygon], crs=crs)\n",
    "        east_gdf = gpd.GeoDataFrame(geometry=[east_polygon], crs=crs)\n",
    "\n",
    "        west_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country],west_gdf, how='intersection')\n",
    "        east_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], east_gdf, how='intersection')\n",
    "        west_part['region'] = 'west'\n",
    "        east_part['region'] = 'east'\n",
    "        \n",
    "        return pd.concat([west_part, east_part])\n",
    "        \n",
    "    elif division == 'NCS':\n",
    "         \n",
    "        third_latitude = (maxy - miny) / 3\n",
    "        south_limit = miny + third_latitude\n",
    "        north_limit = maxy - third_latitude\n",
    "\n",
    "        south_polygon = Polygon([(minx, miny), (minx, south_limit), (maxx, south_limit), (maxx, miny)])\n",
    "        center_polygon = Polygon([(minx, south_limit), (minx, north_limit), (maxx, north_limit), (maxx, south_limit)])\n",
    "        north_polygon = Polygon([(minx, north_limit), (minx, maxy), (maxx, maxy), (maxx, north_limit)])\n",
    "\n",
    "        south_gdf = gpd.GeoDataFrame(geometry=[south_polygon], crs=crs)\n",
    "        center_gdf = gpd.GeoDataFrame(geometry=[center_polygon], crs=crs)\n",
    "        north_gdf = gpd.GeoDataFrame(geometry=[north_polygon], crs=crs)\n",
    "\n",
    "        south_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], south_gdf, how='intersection')\n",
    "        center_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], center_gdf, how='intersection')\n",
    "        north_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], north_gdf, how='intersection')\n",
    "\n",
    "        south_part['region'] = 'south'\n",
    "        center_part['region'] = 'center'\n",
    "        north_part['region'] = 'north'\n",
    "\n",
    "        return pd.concat([north_part, center_part, south_part])\n",
    "         \n",
    "        \n",
    "    elif division == 'NSE':\n",
    "        median_latitude = (miny + maxy) / 2\n",
    "        median_longitude = (minx + maxx) / 2\n",
    "        north_polygon = Polygon([(minx, median_latitude), (minx, maxy), (median_longitude, maxy), (median_longitude, median_latitude)])\n",
    "        south_polygon = Polygon([(minx, miny), (minx, median_latitude), (median_longitude, median_latitude), (median_longitude, miny)])\n",
    "        east_polygon = Polygon([(median_longitude, miny), (median_longitude, median_latitude), (maxx, median_latitude), (maxx, miny)])\n",
    "        west_polygon = Polygon([(minx, median_latitude), (minx, maxy), (median_longitude, maxy), (median_longitude, median_latitude)])\n",
    "        # Convert to GeoDataFrame with the correct CRS\n",
    "        north_gdf = gpd.GeoDataFrame(geometry=[north_polygon], crs=crs)\n",
    "        south_gdf = gpd.GeoDataFrame(geometry=[south_polygon], crs= crs)\n",
    "        east_gdf = gpd.GeoDataFrame(geometry=[east_polygon], crs= crs)\n",
    "        west_gdf = gpd.GeoDataFrame(geometry=[west_polygon], crs= crs)\n",
    "        north_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], north_gdf, how='intersection')\n",
    "        south_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], south_gdf, how='intersection')\n",
    "        east_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], east_gdf, how='intersection')\n",
    "        west_part = gpd.overlay(geodf.loc[geodf['ADMIN'] == country], west_gdf, how='intersection')\n",
    "        north_part['region'] = 'north'\n",
    "        south_part['region'] = 'south'\n",
    "        east_part['region'] = 'east'\n",
    "        west_part['region'] = 'west'\n",
    "\n",
    "        return pd.concat([west_part, east_part, north_part, south_part])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid division type. Use 'NS' (North-South) , 'NSE' (North-South-East') or 'EW' (East-West).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5615c82-7e1c-4244-aae0-23e32ab7520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_to_epm_path = \"C:\\\\Users\\\\wb590499\\\\Documents\\\\Projects\\\\EPM\\\\epm\\\\postprocessing\\\\static\\\\geojson_to_epm_bis.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cbfe958-5aaa-4f1e-b550-492da746a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_map= \"C:\\\\Users\\\\wb590499\\\\Documents\\\\Projects\\\\EPM\\\\epm\\\\postprocessing\\\\static\\\\zones.geojson\"\n",
    "dict_specs = {\n",
    "        'map_zones': gpd.read_file(zone_map),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "869c1bd3-a6a2-4ef8-82f0-1f8307e3fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_map, geojson_to_epm_dict = get_json_data(epm_results = None, dict_specs = dict_specs, geojson_to_epm = geojson_to_epm_path,\n",
    "                                                  zone_map=zone_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49fa1627-af06-44c1-b0cb-b591bcdeaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_map, centers = create_zonemap(zone_map, map_geojson_to_epm=geojson_to_epm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7619474d-d18f-49cb-bd43-9a2656062896",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = zone_map.total_bounds  # [minx, miny, maxx, maxy]\n",
    "region_center = [(bounds[1] + bounds[3]) / 2, (bounds[0] + bounds[2]) / 2]  # Center latitude, longitude\n",
    "energy_map = folium.Map(location=region_center, zoom_start=6, tiles='CartoDB positron')\n",
    "\n",
    "# Add country zones\n",
    "folium.GeoJson(zone_map, style_function=lambda feature: {\n",
    "        'fillColor': '#ffffff', 'color': '#000000', 'weight': 1, 'fillOpacity': 0.3\n",
    "}).add_to(energy_map)\n",
    "energy_map.save(\"footprint.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
